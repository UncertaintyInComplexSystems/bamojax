{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BaMoJAX","text":"<p>A JAX-powered toolbox for Bayesian modelling.</p> <ul> <li>Docs: API and examples</li> <li>Source: GitHub</li> </ul>"},{"location":"SUMMARY/","title":"Table of contents","text":"<ul> <li>Home</li> <li>API</li> </ul>"},{"location":"api/","title":"API reference","text":""},{"location":"api/#bamojax","title":"<code>bamojax</code>","text":""},{"location":"api/#bamojax.base","title":"<code>bamojax.base</code>","text":""},{"location":"api/#bamojax.base.Node","title":"<code>bamojax.base.Node</code>","text":"<p>The essential element of any Bayesian model is the variable, represented by a node in a DAG. </p> <p>Nodes can consist of stochastic or deterministic variables, and can be observed or latent.</p> <p>Hyperparameters of a model are implicitly observed, deterministic nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>class Node:\n    r\"\"\" The essential element of any Bayesian model is the variable, represented by a node in a DAG. \n\n    Nodes can consist of stochastic or deterministic variables, and can be observed or latent.\n\n    Hyperparameters of a model are implicitly observed, deterministic nodes.\n\n    \"\"\"\n\n    def __init__(self, name: str = 'root', \n                 observations: Array = None, \n                 distribution: Union[Distribution, Bijector] = None, \n                 parents = None, \n                 link_fn: Callable = None,\n                 shape: Union[Tuple, int] = None,\n                 bijector: Bijector = None):\n        self.name = name\n\n        if shape is None: \n            shape = ( )\n        self.shape = shape\n        if bijector is not None:\n            self.bijector = bijector\n        if observations is not None:\n            observations = jnp.asarray(observations) if jnp.isscalar(observations) else observations\n            self.observations = observations\n        if distribution is not None:\n            self.distribution = distribution\n            self.parents = {}\n            if parents is not None:\n                for param, parent in parents.items():\n                    self.add_parent(param, parent)            \n            if link_fn is None:\n                def identity(**kwargs):\n                    return kwargs\n                link_fn = identity\n            self.link_fn = link_fn        \n\n    #\n    def is_observed(self) -&gt; bool:\n        \"\"\" Check if a node is an observed variable.\n\n        \"\"\"\n        return hasattr(self, 'observations')\n\n    #\n    def is_stochastic(self) -&gt; bool:\n        \"\"\" Check whether a node is stochastic or deterministic.\n\n        \"\"\"\n        return hasattr(self, 'distribution')\n\n    #\n    def is_root(self) -&gt; bool:\n        \"\"\" Check whether a node is a root node.\n\n        \"\"\"\n        return not hasattr(self, 'parents') or len(self.parents) == 0\n\n    #\n    def add_parent(self, param, node):\n        \"\"\" Add a parent node.\n\n        \"\"\"\n        assert isinstance(node, Node)\n        self.parents[param] = node\n\n    #\n    def set_step_fn(self, step_fn):\n        self.step_fn = step_fn\n\n    #\n    def set_step_fn_parameter(self, step_fn_params):\n        self.step_fn_params = step_fn_params\n\n    #\n    def is_leaf(self):\n        \"\"\" Check whether a node is observed and has parents.\n\n        \"\"\"\n        return hasattr(self, 'parents') and self.is_observed()\n\n    #    \n    def get_distribution(self, state: dict = None, minibatch: dict = None) -&gt; Distribution:\n        r\"\"\" Derives the parametrized distribution p(node | Parents=x), where x is derived from the state object.\n\n        Args:\n            state: Current assignment of (parent) values.\n            minibatch: A additional set of assigned variables, useful for out-of-sample predictions.\n        Returns:\n            An instantiated distrax distribution object.\n\n        \"\"\"\n\n        # Root-level nodes can be defined as instantiated distrax distributions.\n        if isinstance(self.distribution, Distribution):\n            return self.distribution\n\n        if minibatch is None:\n            minibatch = {}\n\n        # Otherwise the distribution is instantiated from the state.\n        parent_values = {}\n        for parent_name, parent_node in self.parents.items():\n            if parent_node in state:\n                parent_values[parent_name] = state[parent_node]\n            else:\n                if parent_name in minibatch:\n                    parent_values[parent_name] = minibatch[parent_node]\n                else:\n                    parent_values[parent_name] = self.parents[parent_name].observations\n\n        transformed_parents = self.link_fn(**parent_values)\n        if hasattr(self, 'bijector'):\n            return dx.Transformed(self.distribution(**transformed_parents), self.bijector)\n        else:\n            return self.distribution(**transformed_parents)\n\n    #\n    def __repr__(self) -&gt; str:\n        return f'{self.name}'\n\n    #\n    def __hash__(self):\n        return hash((self.name))\n\n    #\n    def __eq__(self, other):\n        if isinstance(other, Node):\n            return self.name == other.name\n        return NotImplementedError\n</code></pre>"},{"location":"api/#bamojax.base.Node.is_observed","title":"<code>bamojax.base.Node.is_observed()</code>","text":"<p>Check if a node is an observed variable.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_observed(self) -&gt; bool:\n    \"\"\" Check if a node is an observed variable.\n\n    \"\"\"\n    return hasattr(self, 'observations')\n</code></pre>"},{"location":"api/#bamojax.base.Node.is_stochastic","title":"<code>bamojax.base.Node.is_stochastic()</code>","text":"<p>Check whether a node is stochastic or deterministic.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_stochastic(self) -&gt; bool:\n    \"\"\" Check whether a node is stochastic or deterministic.\n\n    \"\"\"\n    return hasattr(self, 'distribution')\n</code></pre>"},{"location":"api/#bamojax.base.Node.is_root","title":"<code>bamojax.base.Node.is_root()</code>","text":"<p>Check whether a node is a root node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_root(self) -&gt; bool:\n    \"\"\" Check whether a node is a root node.\n\n    \"\"\"\n    return not hasattr(self, 'parents') or len(self.parents) == 0\n</code></pre>"},{"location":"api/#bamojax.base.Node.add_parent","title":"<code>bamojax.base.Node.add_parent(param, node)</code>","text":"<p>Add a parent node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def add_parent(self, param, node):\n    \"\"\" Add a parent node.\n\n    \"\"\"\n    assert isinstance(node, Node)\n    self.parents[param] = node\n</code></pre>"},{"location":"api/#bamojax.base.Node.is_leaf","title":"<code>bamojax.base.Node.is_leaf()</code>","text":"<p>Check whether a node is observed and has parents.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_leaf(self):\n    \"\"\" Check whether a node is observed and has parents.\n\n    \"\"\"\n    return hasattr(self, 'parents') and self.is_observed()\n</code></pre>"},{"location":"api/#bamojax.base.Node.get_distribution","title":"<code>bamojax.base.Node.get_distribution(state=None, minibatch=None)</code>","text":"<p>Derives the parametrized distribution p(node | Parents=x), where x is derived from the state object.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Current assignment of (parent) values.</p> <code>None</code> <code>minibatch</code> <code>dict</code> <p>A additional set of assigned variables, useful for out-of-sample predictions.</p> <code>None</code> <p>Returns:     An instantiated distrax distribution object.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_distribution(self, state: dict = None, minibatch: dict = None) -&gt; Distribution:\n    r\"\"\" Derives the parametrized distribution p(node | Parents=x), where x is derived from the state object.\n\n    Args:\n        state: Current assignment of (parent) values.\n        minibatch: A additional set of assigned variables, useful for out-of-sample predictions.\n    Returns:\n        An instantiated distrax distribution object.\n\n    \"\"\"\n\n    # Root-level nodes can be defined as instantiated distrax distributions.\n    if isinstance(self.distribution, Distribution):\n        return self.distribution\n\n    if minibatch is None:\n        minibatch = {}\n\n    # Otherwise the distribution is instantiated from the state.\n    parent_values = {}\n    for parent_name, parent_node in self.parents.items():\n        if parent_node in state:\n            parent_values[parent_name] = state[parent_node]\n        else:\n            if parent_name in minibatch:\n                parent_values[parent_name] = minibatch[parent_node]\n            else:\n                parent_values[parent_name] = self.parents[parent_name].observations\n\n    transformed_parents = self.link_fn(**parent_values)\n    if hasattr(self, 'bijector'):\n        return dx.Transformed(self.distribution(**transformed_parents), self.bijector)\n    else:\n        return self.distribution(**transformed_parents)\n</code></pre>"},{"location":"api/#bamojax.base.Model","title":"<code>bamojax.base.Model</code>","text":"<p>A Bayesian model is represented as a directed acyclic graph, in which nodes are associated with random variables.</p> <p>Typical use:</p> <pre><code>model = Model('model name')\n_ = model.add_node('x', observations=...)\n</code></pre> Source code in <code>bamojax/base.py</code> <pre><code>class Model:\n    r\"\"\" A Bayesian model is represented as a directed acyclic graph, in which nodes are associated with random variables.\n\n    Typical use:\n\n        model = Model('model name')\n        _ = model.add_node('x', observations=...)\n\n    \"\"\"\n\n    def __init__(self, name='Bayesian model', verbose=False):\n        self.name = name\n        self.nodes = {}\n        self.root_nodes = list()\n        self.leaf_nodes = list()\n        self.children = dict()\n        self.parents = dict()\n        self.verbose = verbose\n\n    #\n    def add_node(self, \n                 name: str = 'root', \n                 distribution: Union[Distribution, Bijector] = None, \n                 observations: Array = None, \n                 parents: dict = None, \n                 link_fn: Callable = None,\n                 shape: Union[Tuple, int] = None,\n                 bijector: Bijector = None) -&gt; Node:\n        r\"\"\" Adds a node to the Bayesian model DAG\n\n        Args:\n          name: The name of the variable.\n          distribution: The distrax distribution of the variable given its (transformed) parents.\n          observations: If the node is observed; the actual observations.\n          parents: The nodes that this node depends on.\n          link_fn: A link function combining the inputs to form the input to the corresponding distrax distribution.\n          shape: The dimensions of the variable.\n          bijector: A bijector can be passed to transform variables.\n        Returns:\n          New node\n\n        \"\"\"\n        if parents is not None:\n            new_parents = {}\n            for parent_name, parent in parents.items():\n                if not isinstance(parent, Node):\n                    # Parent is numeric\n                    parent_node = self.add_node(name=f'{parent_name}_{name}', observations=parent)\n                    new_parents[parent_name] = parent_node\n                else:\n                    new_parents[parent_name] = parent\n            parents = new_parents\n        new_node = Node(name=name, distribution=distribution, observations=observations, parents=parents, link_fn=link_fn, shape=shape, bijector=bijector)\n        self.nodes[name] = new_node\n        if self.verbose: print(f'Adding node ({name})')\n        if parents is not None:\n            for parent in parents.values():\n                self.add_edge(parent, new_node)\n        if new_node.is_root():\n            self.root_nodes.append(new_node)\n        if new_node.is_leaf():\n            self.leaf_nodes.append(new_node)\n        return new_node\n\n    #    \n    def add_edge(self, from_node, to_node):\n        r\"\"\" Store the dependence between two nodes.\n\n        Args:\n            from_node: source node\n            to_node: target node\n\n        \"\"\"\n        if self.verbose: print(f'Add edge ({from_node}) -&gt; ({to_node})')\n        if not from_node in self.children:\n            self.children[from_node] = set()\n        self.children[from_node].add(to_node)\n        if not to_node in self.parents:\n            self.parents[to_node] = set()\n        self.parents[to_node].add(from_node)\n\n    #\n    def get_children(self, node):\n        \"\"\" Returns the children of a node.\n\n        \"\"\"\n        if node in self.children:\n            return self.children[node]\n        return []\n\n    #\n    def get_parents(self, node):\n        \"\"\" Returns the parents of a node.\n\n        \"\"\"\n        if node in self.parents:\n            return self.parents[node]\n        return []\n\n    #\n    def get_root_nodes(self):\n        \"\"\" Return all nodes that are roots.\n\n        \"\"\"\n        return self.root_nodes\n\n    #\n    def get_leaf_nodes(self):\n        \"\"\" Returns all nodes that are leaves.\n\n        \"\"\"\n        return {self.nodes[k]: self.nodes[k] for k in self.nodes.keys() - self.children.keys()}\n\n    #\n    def get_stochastic_nodes(self):\n        \"\"\" Returns all stochastic nodes.\n\n        \"\"\"\n        return {k: v for k, v in self.nodes.items() if v.is_stochastic()}\n\n    #\n    def get_latent_nodes(self):\n        \"\"\" Returns all latent nodes.\n\n        \"\"\"\n        return {k: v for k, v in self.nodes.items() if v.is_stochastic() and not v.is_observed()}\n\n    #\n    def logprior_fn(self) -&gt; Callable:\n        r\"\"\" Returns a callable function that provides the log prior of the model given the current state of assigned variables.\n\n        \"\"\"\n\n        def logprior_fn_(state) -&gt; float:\n            sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n            logprob = 0.0\n            for node in sorted_free_variables:\n                if node.is_root():\n                    logprob += jnp.sum(node.get_distribution().log_prob(state[node.name]))\n                else:\n                    logprob += jnp.sum(node.get_distribution(state).log_prob(state[node.name]))\n            return logprob\n\n        #\n        return logprior_fn_\n\n    #\n    def loglikelihood_fn(self) -&gt; Callable:\n        r\"\"\" Returns a callable function that provides the log likelihood of the model given the current state of assigned variables.\n\n        \"\"\"\n\n        def loglikelihood_fn_(state) -&gt; float:\n            logprob = 0.0\n            for node in self.get_leaf_nodes():\n                if '__mask' in state:\n                    element_wise_logp = node.get_distribution(state).log_prob(value=node.observations)\n                    logprob += jnp.sum(jnp.nan_to_num(element_wise_logp, nan=0.0)*state['__mask'])\n                else:\n                    logprob += jnp.sum(node.get_distribution(state).log_prob(value=node.observations))\n            return logprob\n\n        #\n        return loglikelihood_fn_\n\n    #\n    def batched_loglikelihood_fn(self) -&gt; Callable:\n        r\"\"\" Batched loglikelihood function for stochastic-gradient methods.\n\n        Assumes `minibatch` is a dictionary containing a subset of observations for each observed leaf node.\n\n        \"\"\"\n\n        def loglikelihood_fn_(state, minibatch) -&gt; float:\n            logprob = 0.0\n            for node in self.get_leaf_nodes():\n                logprob += jnp.sum(node.get_distribution(state, minibatch=minibatch).log_prob(value=minibatch[node]))\n            return logprob\n\n        #\n        return loglikelihood_fn_\n\n    #\n    def get_model_size(self) -&gt; int:\n        r\"\"\" Returns the total dimensions of the model. \n\n        As node.distribution can be abstract, we create a concrete instantiation by drawing a sample from the prior and deriving the shape from this sample.\n\n        \"\"\"\n\n        draw = self.sample_prior(key=jrnd.PRNGKey(0)) \n        size = jnp.sum(jnp.array([jnp.size(v) for v in draw.values()]))\n        return size\n\n    #  \n    def get_node_order(self):\n        r\"\"\" Returns the latent variables in topological order; child nodes are always listed after their parents.\n\n        \"\"\"\n        if not hasattr(self, 'node_order'):\n            self.node_order = self.__get_topological_order()\n        return self.node_order\n\n    #\n    def __get_topological_order(self) -&gt; list:\n        r\"\"\" Traverses the directed acyclic graph that defines the Bayesian model and returns its nodes in topological order\n\n        Returns:\n            A list of sorted nodes.\n\n        \"\"\"\n        def traverse_dag_backwards(node: Node, visited_: dict, order: list):\n            if node in visited_:\n                return\n            visited_[node] = 1\n\n            for parent_node in node.parents.values():\n                if parent_node.is_stochastic():\n                    traverse_dag_backwards(parent_node, visited_, order)\n\n            order.append(node)\n\n        #\n        order = []\n        visited = {}\n        leaves = self.get_leaf_nodes()\n        for leaf in leaves:\n            traverse_dag_backwards(leaf, visited, order)\n        return order\n\n    #\n    def sample_prior(self, key) -&gt; dict:\n        r\"\"\" Samples from the (hierarchical) prior distribution of the model.\n\n        Args:\n            key: Random seed\n        Returns:\n            A state dictionary with one random value for each node.\n\n        \"\"\"\n        state = dict()\n        sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n        for node in sorted_free_variables:\n            key, subkey = jrnd.split(key)\n            if node.is_root(): \n                dist = node.get_distribution()               \n            else:\n                dist = node.get_distribution(state)\n            state[node.name] = dist.sample(seed=subkey, sample_shape=node.shape)   \n        return state\n\n    #\n    def sample_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        r\"\"\" Sample stochastic observed nodes\n\n        Args:\n            key: PRNGKey\n            state: a draw from either $p(x)$ or $p(x \\mid \\cdot)$\n            input_variables: a dictionary with values for observed non-stochastic nodes\n\n        Returns:\n            A dictionary which is the same as 'state' but appended with sampled values.\n\n        \"\"\"\n\n        for node in self.get_leaf_nodes():\n            key, key_obs = jrnd.split(key)\n            state[node.name] = node.get_distribution(state, minibatch=input_variables).sample(seed=key_obs, sample_shape=node.shape)\n        return state\n\n\n    def sample_prior_predictive(self, key, prediction_options: dict = None) -&gt; dict:\n        r\"\"\" Sample from the (hierarchical) prior predictive distribution of the model.\n\n        Args:\n            key: Random seed\n            prediction_options: A dictionary of options which can include minibatched input variables\n\n        Returns:\n            A dictionary with a random value for all stochastic observed nodes.\n\n        \"\"\"\n        key, key_latent = jrnd.split(key)\n        state = self.sample_prior(key_latent)\n        return self.sample_predictive(key, state, prediction_options)\n\n    #\n    def sample_posterior_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        r\"\"\" Sample from the posterior predictive\n\n        Args:\n            key: Random key\n            state: A draw from the posterior\n            input_variables: Potential predictors and other non-stochastic observations\n\n        Returns:\n            A dictionary containing values for all stochastic observed nodes, conditioned on the observations.\n\n        \"\"\"\n\n        return self.sample_predictive(key, state, input_variables)\n\n    #\n    def print_gibbs(self):\n        r\"\"\" Print the structure of conditional distributions. \n\n\n        \"\"\"\n\n        print('Gibbs structure:')\n        sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n        for node in sorted_free_variables:\n            # get prior density        \n            if node.is_root():\n                prior = f'p({node})'\n            else:\n                parents = {p for p in self.get_parents(node)}\n                prior = f'p({node} | {\", \".join([p.name for p in parents])})'\n\n            # get conditional density\n            conditionals = []\n            children = [c for c in self.get_children(node)]        \n            for child in children:\n                co_parents = set()\n                for parent in self.get_parents(child):\n                    if not parent == node or parent in co_parents:\n                        co_parents.add(parent)        \n                co_parents.add(node)\n                conditional = f'p({child} | {\", \".join([str(p) for p in co_parents])})'\n                conditionals.append(conditional)\n\n            print(f'{str(node):20s}: {\" \".join(conditionals)} {prior}')\n</code></pre>"},{"location":"api/#bamojax.base.Model.add_node","title":"<code>bamojax.base.Model.add_node(name='root', distribution=None, observations=None, parents=None, link_fn=None, shape=None, bijector=None)</code>","text":"<p>Adds a node to the Bayesian model DAG</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable.</p> <code>'root'</code> <code>distribution</code> <code>Union[Distribution, Bijector]</code> <p>The distrax distribution of the variable given its (transformed) parents.</p> <code>None</code> <code>observations</code> <code>Array</code> <p>If the node is observed; the actual observations.</p> <code>None</code> <code>parents</code> <code>dict</code> <p>The nodes that this node depends on.</p> <code>None</code> <code>link_fn</code> <code>Callable</code> <p>A link function combining the inputs to form the input to the corresponding distrax distribution.</p> <code>None</code> <code>shape</code> <code>Union[Tuple, int]</code> <p>The dimensions of the variable.</p> <code>None</code> <code>bijector</code> <code>Bijector</code> <p>A bijector can be passed to transform variables.</p> <code>None</code> <p>Returns:   New node</p> Source code in <code>bamojax/base.py</code> <pre><code>def add_node(self, \n             name: str = 'root', \n             distribution: Union[Distribution, Bijector] = None, \n             observations: Array = None, \n             parents: dict = None, \n             link_fn: Callable = None,\n             shape: Union[Tuple, int] = None,\n             bijector: Bijector = None) -&gt; Node:\n    r\"\"\" Adds a node to the Bayesian model DAG\n\n    Args:\n      name: The name of the variable.\n      distribution: The distrax distribution of the variable given its (transformed) parents.\n      observations: If the node is observed; the actual observations.\n      parents: The nodes that this node depends on.\n      link_fn: A link function combining the inputs to form the input to the corresponding distrax distribution.\n      shape: The dimensions of the variable.\n      bijector: A bijector can be passed to transform variables.\n    Returns:\n      New node\n\n    \"\"\"\n    if parents is not None:\n        new_parents = {}\n        for parent_name, parent in parents.items():\n            if not isinstance(parent, Node):\n                # Parent is numeric\n                parent_node = self.add_node(name=f'{parent_name}_{name}', observations=parent)\n                new_parents[parent_name] = parent_node\n            else:\n                new_parents[parent_name] = parent\n        parents = new_parents\n    new_node = Node(name=name, distribution=distribution, observations=observations, parents=parents, link_fn=link_fn, shape=shape, bijector=bijector)\n    self.nodes[name] = new_node\n    if self.verbose: print(f'Adding node ({name})')\n    if parents is not None:\n        for parent in parents.values():\n            self.add_edge(parent, new_node)\n    if new_node.is_root():\n        self.root_nodes.append(new_node)\n    if new_node.is_leaf():\n        self.leaf_nodes.append(new_node)\n    return new_node\n</code></pre>"},{"location":"api/#bamojax.base.Model.add_edge","title":"<code>bamojax.base.Model.add_edge(from_node, to_node)</code>","text":"<p>Store the dependence between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>from_node</code> <p>source node</p> required <code>to_node</code> <p>target node</p> required Source code in <code>bamojax/base.py</code> <pre><code>def add_edge(self, from_node, to_node):\n    r\"\"\" Store the dependence between two nodes.\n\n    Args:\n        from_node: source node\n        to_node: target node\n\n    \"\"\"\n    if self.verbose: print(f'Add edge ({from_node}) -&gt; ({to_node})')\n    if not from_node in self.children:\n        self.children[from_node] = set()\n    self.children[from_node].add(to_node)\n    if not to_node in self.parents:\n        self.parents[to_node] = set()\n    self.parents[to_node].add(from_node)\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_children","title":"<code>bamojax.base.Model.get_children(node)</code>","text":"<p>Returns the children of a node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_children(self, node):\n    \"\"\" Returns the children of a node.\n\n    \"\"\"\n    if node in self.children:\n        return self.children[node]\n    return []\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_parents","title":"<code>bamojax.base.Model.get_parents(node)</code>","text":"<p>Returns the parents of a node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_parents(self, node):\n    \"\"\" Returns the parents of a node.\n\n    \"\"\"\n    if node in self.parents:\n        return self.parents[node]\n    return []\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_root_nodes","title":"<code>bamojax.base.Model.get_root_nodes()</code>","text":"<p>Return all nodes that are roots.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_root_nodes(self):\n    \"\"\" Return all nodes that are roots.\n\n    \"\"\"\n    return self.root_nodes\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_leaf_nodes","title":"<code>bamojax.base.Model.get_leaf_nodes()</code>","text":"<p>Returns all nodes that are leaves.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_leaf_nodes(self):\n    \"\"\" Returns all nodes that are leaves.\n\n    \"\"\"\n    return {self.nodes[k]: self.nodes[k] for k in self.nodes.keys() - self.children.keys()}\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_stochastic_nodes","title":"<code>bamojax.base.Model.get_stochastic_nodes()</code>","text":"<p>Returns all stochastic nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_stochastic_nodes(self):\n    \"\"\" Returns all stochastic nodes.\n\n    \"\"\"\n    return {k: v for k, v in self.nodes.items() if v.is_stochastic()}\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_latent_nodes","title":"<code>bamojax.base.Model.get_latent_nodes()</code>","text":"<p>Returns all latent nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_latent_nodes(self):\n    \"\"\" Returns all latent nodes.\n\n    \"\"\"\n    return {k: v for k, v in self.nodes.items() if v.is_stochastic() and not v.is_observed()}\n</code></pre>"},{"location":"api/#bamojax.base.Model.logprior_fn","title":"<code>bamojax.base.Model.logprior_fn()</code>","text":"<p>Returns a callable function that provides the log prior of the model given the current state of assigned variables.</p> Source code in <code>bamojax/base.py</code> <pre><code>def logprior_fn(self) -&gt; Callable:\n    r\"\"\" Returns a callable function that provides the log prior of the model given the current state of assigned variables.\n\n    \"\"\"\n\n    def logprior_fn_(state) -&gt; float:\n        sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n        logprob = 0.0\n        for node in sorted_free_variables:\n            if node.is_root():\n                logprob += jnp.sum(node.get_distribution().log_prob(state[node.name]))\n            else:\n                logprob += jnp.sum(node.get_distribution(state).log_prob(state[node.name]))\n        return logprob\n\n    #\n    return logprior_fn_\n</code></pre>"},{"location":"api/#bamojax.base.Model.loglikelihood_fn","title":"<code>bamojax.base.Model.loglikelihood_fn()</code>","text":"<p>Returns a callable function that provides the log likelihood of the model given the current state of assigned variables.</p> Source code in <code>bamojax/base.py</code> <pre><code>def loglikelihood_fn(self) -&gt; Callable:\n    r\"\"\" Returns a callable function that provides the log likelihood of the model given the current state of assigned variables.\n\n    \"\"\"\n\n    def loglikelihood_fn_(state) -&gt; float:\n        logprob = 0.0\n        for node in self.get_leaf_nodes():\n            if '__mask' in state:\n                element_wise_logp = node.get_distribution(state).log_prob(value=node.observations)\n                logprob += jnp.sum(jnp.nan_to_num(element_wise_logp, nan=0.0)*state['__mask'])\n            else:\n                logprob += jnp.sum(node.get_distribution(state).log_prob(value=node.observations))\n        return logprob\n\n    #\n    return loglikelihood_fn_\n</code></pre>"},{"location":"api/#bamojax.base.Model.batched_loglikelihood_fn","title":"<code>bamojax.base.Model.batched_loglikelihood_fn()</code>","text":"<p>Batched loglikelihood function for stochastic-gradient methods.</p> <p>Assumes <code>minibatch</code> is a dictionary containing a subset of observations for each observed leaf node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def batched_loglikelihood_fn(self) -&gt; Callable:\n    r\"\"\" Batched loglikelihood function for stochastic-gradient methods.\n\n    Assumes `minibatch` is a dictionary containing a subset of observations for each observed leaf node.\n\n    \"\"\"\n\n    def loglikelihood_fn_(state, minibatch) -&gt; float:\n        logprob = 0.0\n        for node in self.get_leaf_nodes():\n            logprob += jnp.sum(node.get_distribution(state, minibatch=minibatch).log_prob(value=minibatch[node]))\n        return logprob\n\n    #\n    return loglikelihood_fn_\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_model_size","title":"<code>bamojax.base.Model.get_model_size()</code>","text":"<p>Returns the total dimensions of the model. </p> <p>As node.distribution can be abstract, we create a concrete instantiation by drawing a sample from the prior and deriving the shape from this sample.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_model_size(self) -&gt; int:\n    r\"\"\" Returns the total dimensions of the model. \n\n    As node.distribution can be abstract, we create a concrete instantiation by drawing a sample from the prior and deriving the shape from this sample.\n\n    \"\"\"\n\n    draw = self.sample_prior(key=jrnd.PRNGKey(0)) \n    size = jnp.sum(jnp.array([jnp.size(v) for v in draw.values()]))\n    return size\n</code></pre>"},{"location":"api/#bamojax.base.Model.get_node_order","title":"<code>bamojax.base.Model.get_node_order()</code>","text":"<p>Returns the latent variables in topological order; child nodes are always listed after their parents.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_node_order(self):\n    r\"\"\" Returns the latent variables in topological order; child nodes are always listed after their parents.\n\n    \"\"\"\n    if not hasattr(self, 'node_order'):\n        self.node_order = self.__get_topological_order()\n    return self.node_order\n</code></pre>"},{"location":"api/#bamojax.base.Model.sample_prior","title":"<code>bamojax.base.Model.sample_prior(key)</code>","text":"<p>Samples from the (hierarchical) prior distribution of the model.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random seed</p> required <p>Returns:     A state dictionary with one random value for each node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_prior(self, key) -&gt; dict:\n    r\"\"\" Samples from the (hierarchical) prior distribution of the model.\n\n    Args:\n        key: Random seed\n    Returns:\n        A state dictionary with one random value for each node.\n\n    \"\"\"\n    state = dict()\n    sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n    for node in sorted_free_variables:\n        key, subkey = jrnd.split(key)\n        if node.is_root(): \n            dist = node.get_distribution()               \n        else:\n            dist = node.get_distribution(state)\n        state[node.name] = dist.sample(seed=subkey, sample_shape=node.shape)   \n    return state\n</code></pre>"},{"location":"api/#bamojax.base.Model.sample_predictive","title":"<code>bamojax.base.Model.sample_predictive(key, state, input_variables=None)</code>","text":"<p>Sample stochastic observed nodes</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>PRNGKey</p> required <code>state</code> <code>dict</code> <p>a draw from either \\(p(x)\\) or \\(p(x \\mid \\cdot)\\)</p> required <code>input_variables</code> <code>dict</code> <p>a dictionary with values for observed non-stochastic nodes</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary which is the same as 'state' but appended with sampled values.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n    r\"\"\" Sample stochastic observed nodes\n\n    Args:\n        key: PRNGKey\n        state: a draw from either $p(x)$ or $p(x \\mid \\cdot)$\n        input_variables: a dictionary with values for observed non-stochastic nodes\n\n    Returns:\n        A dictionary which is the same as 'state' but appended with sampled values.\n\n    \"\"\"\n\n    for node in self.get_leaf_nodes():\n        key, key_obs = jrnd.split(key)\n        state[node.name] = node.get_distribution(state, minibatch=input_variables).sample(seed=key_obs, sample_shape=node.shape)\n    return state\n</code></pre>"},{"location":"api/#bamojax.base.Model.sample_prior_predictive","title":"<code>bamojax.base.Model.sample_prior_predictive(key, prediction_options=None)</code>","text":"<p>Sample from the (hierarchical) prior predictive distribution of the model.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random seed</p> required <code>prediction_options</code> <code>dict</code> <p>A dictionary of options which can include minibatched input variables</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with a random value for all stochastic observed nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_prior_predictive(self, key, prediction_options: dict = None) -&gt; dict:\n    r\"\"\" Sample from the (hierarchical) prior predictive distribution of the model.\n\n    Args:\n        key: Random seed\n        prediction_options: A dictionary of options which can include minibatched input variables\n\n    Returns:\n        A dictionary with a random value for all stochastic observed nodes.\n\n    \"\"\"\n    key, key_latent = jrnd.split(key)\n    state = self.sample_prior(key_latent)\n    return self.sample_predictive(key, state, prediction_options)\n</code></pre>"},{"location":"api/#bamojax.base.Model.sample_posterior_predictive","title":"<code>bamojax.base.Model.sample_posterior_predictive(key, state, input_variables=None)</code>","text":"<p>Sample from the posterior predictive</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random key</p> required <code>state</code> <code>dict</code> <p>A draw from the posterior</p> required <code>input_variables</code> <code>dict</code> <p>Potential predictors and other non-stochastic observations</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing values for all stochastic observed nodes, conditioned on the observations.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_posterior_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n    r\"\"\" Sample from the posterior predictive\n\n    Args:\n        key: Random key\n        state: A draw from the posterior\n        input_variables: Potential predictors and other non-stochastic observations\n\n    Returns:\n        A dictionary containing values for all stochastic observed nodes, conditioned on the observations.\n\n    \"\"\"\n\n    return self.sample_predictive(key, state, input_variables)\n</code></pre>"},{"location":"api/#bamojax.base.Model.print_gibbs","title":"<code>bamojax.base.Model.print_gibbs()</code>","text":"<p>Print the structure of conditional distributions.</p> Source code in <code>bamojax/base.py</code> <pre><code>def print_gibbs(self):\n    r\"\"\" Print the structure of conditional distributions. \n\n\n    \"\"\"\n\n    print('Gibbs structure:')\n    sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n    for node in sorted_free_variables:\n        # get prior density        \n        if node.is_root():\n            prior = f'p({node})'\n        else:\n            parents = {p for p in self.get_parents(node)}\n            prior = f'p({node} | {\", \".join([p.name for p in parents])})'\n\n        # get conditional density\n        conditionals = []\n        children = [c for c in self.get_children(node)]        \n        for child in children:\n            co_parents = set()\n            for parent in self.get_parents(child):\n                if not parent == node or parent in co_parents:\n                    co_parents.add(parent)        \n            co_parents.add(node)\n            conditional = f'p({child} | {\", \".join([str(p) for p in co_parents])})'\n            conditionals.append(conditional)\n\n        print(f'{str(node):20s}: {\" \".join(conditionals)} {prior}')\n</code></pre>"},{"location":"api/#bamojax.base.MetaModel","title":"<code>bamojax.base.MetaModel</code>","text":"<p>A meta-model is a collection of Bayesian models, which can be used for reversible jump MCMC.</p> Source code in <code>bamojax/base.py</code> <pre><code>class MetaModel():\n    r\"\"\" A meta-model is a collection of Bayesian models, which can be used for reversible jump MCMC.\n    \"\"\"\n\n    def __init__(self, \n                 model_list):\n        self.model_list = model_list\n        self.M = len(model_list)\n        self.model_sizes = [model.get_model_size() for model in model_list]        \n        self.indiv_latent_nodes = [set(model.get_latent_nodes().keys()) for model in model_list]\n\n        # find the number of auxiliary variables needed, as the largest difference in latent node sets between any two models\n        self.num_auxiliary = 0\n        for i in range(self.M):\n            for j in range(i+1, self.M):\n                diff_left = set.difference(self.indiv_latent_nodes[i], self.indiv_latent_nodes[j])\n                diff_right = set.difference(self.indiv_latent_nodes[j], self.indiv_latent_nodes[i])\n                max_diff = jnp.max(jnp.array([len(diff_left), len(diff_right)]))\n                if max_diff &gt; self.num_auxiliary:\n                    self.num_auxiliary = max_diff\n\n        self.latent_variables = set.union(*self.indiv_latent_nodes)\n        self.auxiliary_variables = [f'u_{i}' for i in range(self.num_auxiliary)]\n        self.meta_state = self.latent_variables.union(set(self.auxiliary_variables))\n\n        def make_model_sample_prior_fn(model_index):\n            \"\"\"Creates a function that samples from the prior of the specified model index.\"\"\"\n            def fn(key):\n                sample = self.model_list[model_index].sample_prior(key)\n                all_latents = {k: jnp.nan for k in self.latent_variables}\n                return {**all_latents, **sample}  \n\n            #   \n            return fn\n        #\n        self.model_sample_prior_fns = [make_model_sample_prior_fn(i) for i in range(self.M)]\n\n        def make_model_sample_predictive_fn(model_index):\n            \"\"\"Creates a function that samples from the predictive distribution of the specified model index.\"\"\"\n            def fn(input):\n                key, state, input_variables = input\n                return self.model_list[model_index].sample_predictive(key, state, input_variables)\n\n            #   \n            return fn\n\n        #\n        self.model_sample_predictive_fns = [make_model_sample_predictive_fn(i) for i in range(self.M)]\n\n        def make_logprior_fn(model_index):\n            \"\"\"Creates a function that computes the log prior of the specified model index.\"\"\"\n            def fn(state):\n                return self.model_list[model_index].logprior_fn()(state)\n\n            #   \n            return fn\n\n        #\n        self.model_logprior_fns = [make_logprior_fn(i) for i in range(self.M)]\n\n        def make_loglikelihood_fn(model_index):\n            \"\"\"Creates a function that computes the log likelihood of the specified model index.\"\"\"\n            def fn(state):\n                return self.model_list[model_index].loglikelihood_fn()(state)\n\n            #   \n            return fn\n\n        #\n        self.model_loglikelihood_fns = [make_loglikelihood_fn(i) for i in range(self.M)]\n\n    #\n    def sample_prior(self, key) -&gt; dict:\n        key_model, key_sample = jrnd.split(key)\n        model_index = jrnd.randint(key_model, shape=(), minval=0, maxval=self.M)\n        sample = jax.lax.switch(model_index, self.model_sample_prior_fns, operand=key_sample)\n        auxiliary_values = {f'u_{i}': jnp.nan for i in range(self.num_auxiliary)}  # to keep the same pytree structure across models in reversible jump MCMC\n        return {'model_index': model_index, **sample, **auxiliary_values}\n\n    #\n    def sample_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        return jax.lax.switch(state['model_index'], self.model_sample_predictive_fns, \n                              operand=[key, state, input_variables])\n\n    #\n    def sample_prior_predictive(self, key, **input_variables) -&gt; dict:\n        key_prior, key_predictive = jrnd.split(key)\n        prior_sample = self.sample_prior(key_prior)\n        return self.sample_predictive(key_predictive, prior_sample, input_variables=input_variables)\n\n    #\n    def sample_posterior_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        return self.sample_predictive(key, state, input_variables=input_variables)\n\n    #\n    def logprior_fn(self) -&gt; Callable:\n\n        def fn(state: dict) -&gt; float:\n            \"\"\"Computes the log prior of the state.\"\"\"\n            model_index = state['model_index']\n            return jax.lax.switch(model_index, self.model_logprior_fns, operand=state)\n\n        #\n        return fn\n\n    #\n    def loglikelihood_fn(self) -&gt; Callable:\n\n        def fn(state: dict) -&gt; float:   \n            \"\"\"Computes the log likelihood of the state.\"\"\"\n            model_index = state['model_index']\n            return jax.lax.switch(model_index, self.model_loglikelihood_fns, operand=state)\n        #\n        return fn   \n</code></pre>"},{"location":"api/#bamojax.inference","title":"<code>bamojax.inference</code>","text":""},{"location":"api/#bamojax.inference.InferenceEngine","title":"<code>bamojax.inference.InferenceEngine</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class for inference engines</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the Bayesian model for which to run the approximate inference</p> <code>num_chains</code> <p>the number of parallel, independent chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>class InferenceEngine(ABC):\n    r\"\"\" The abstract class for inference engines\n\n    Attributes:\n        model: the Bayesian model for which to run the approximate inference\n        num_chains: the number of parallel, independent chains\n\n    \"\"\"\n\n    def __init__(self, model: Model, num_chains: int = 1):\n        self.model = model\n        self.num_chains = num_chains\n\n    #\n    @abstractmethod\n    def run_single_chain(self, key: PRNGKey):\n        pass\n\n    #\n    def run(self, key):\n        r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n        \"\"\"\n        if self.num_chains &gt; 1:\n            keys = jrnd.split(key, self.num_chains)\n            return jax.vmap(self.run_single_chain)(keys)\n        return self.run_single_chain(key)\n</code></pre>"},{"location":"api/#bamojax.inference.InferenceEngine.run","title":"<code>bamojax.inference.InferenceEngine.run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/#bamojax.inference.MCMCInference","title":"<code>bamojax.inference.MCMCInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The MCMC approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class MCMCInference(InferenceEngine):\n    r\"\"\" The MCMC approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model, \n                 num_chains: int = 1, \n                 mcmc_kernel: SamplingAlgorithm = None, \n                 num_samples: int = 10_000, \n                 num_burn: int = 10_000, \n                 num_warmup: int = 0,\n                 num_thin: int = 1, \n                 return_diagnostics: bool = True):\n        super().__init__(model, num_chains)\n        self.mcmc_kernel = mcmc_kernel\n        self.num_samples = num_samples\n        self.num_burn = num_burn\n        self.num_warmup = num_warmup\n        self.num_thin = num_thin\n        self.return_diagnostics = return_diagnostics\n\n    #    \n    def dense_step(self, key, state):\n        \"\"\" Take a MCMC step.\n\n        Can be a composite step to reduce autocorrelation and memory consumption (thinning). Rather than\n        post-hoc removing samples, a dense step can take N steps, then return only the final state, so \n        that we effective thin by a factor N.\n\n        Args:\n            key: Random seed\n            state: The current state\n        Returns:\n            A new state\n            (optional) Diagnostic information, depending on the `return_diagnostics` attribute\n\n        \"\"\"\n        @jax.jit\n        def one_step_fn(state, key):\n            state, info = self.mcmc_kernel.step(key, state)\n            return state, info\n\n        #\n        @jax.jit\n        def one_step_state_only_fn(state, key):\n            state, _ = self.mcmc_kernel.step(key, state)\n            return state, None\n\n        #\n        conditional_step = one_step_fn if self.return_diagnostics else one_step_state_only_fn\n\n        if self.num_thin &gt; 1:\n            keys = jrnd.split(key, self.num_thin)\n            state, infos = jax.lax.scan(conditional_step, state, keys)\n            info = tree_map(lambda x: x[-1, ...], infos)\n            return state, info if self.return_diagnostics else (state, None)\n\n        return conditional_step(state, key)\n\n    #\n    def run_single_chain(self, key: PRNGKey) -&gt; dict:\n        r\"\"\" Run one MCMC chain\n\n        Depending on different preferences, this \n            - optimizes the MCMC kernel hyperparameters\n            - runs `num_burn` burn-in samples, that are discarded\n            - performs the actual sampling for `num_samples` / `num_thin` dense steps\n\n        Args:\n            key: Random seed\n\n        Returns:\n            A dictionary containing the resulting collection of states, and optional diagnostic information\n\n        \"\"\"\n        def mcmc_body_fn(state, key):\n            state, info = self.dense_step(key, state)\n            return state, (state, info) \n\n        #\n        key, key_init = jrnd.split(key)        \n\n        if self.num_warmup &gt; 0:\n            print('Adapting NUTS HMC parameters...', end=\" \")\n            warm_state, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup) \n            print('done.')\n            adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n            self.mcmc_kernel = adapted_kernel\n            initial_state = self.mcmc_kernel.init(warm_state.position)\n        else:\n            initial_state = self.mcmc_kernel.init(self.model.sample_prior(key_init))        \n\n        if self.num_burn &gt; 0:\n            num_dense_burn_steps = int(self.num_burn / self.num_thin)\n            key, key_burn = jrnd.split(key)\n            keys = jrnd.split(key_burn, num_dense_burn_steps)\n            initial_state, _ = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n        num_dense_steps = int(self.num_samples / self.num_thin)\n        key, key_inference = jrnd.split(key)\n        keys = jrnd.split(key_inference, num_dense_steps)\n        _, (states, info) = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n        return dict(states=states.position, info=info) if self.return_diagnostics else dict(states=states.position)\n</code></pre>"},{"location":"api/#bamojax.inference.MCMCInference.dense_step","title":"<code>bamojax.inference.MCMCInference.dense_step(key, state)</code>","text":"<p>Take a MCMC step.</p> <p>Can be a composite step to reduce autocorrelation and memory consumption (thinning). Rather than post-hoc removing samples, a dense step can take N steps, then return only the final state, so  that we effective thin by a factor N.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random seed</p> required <code>state</code> <p>The current state</p> required <p>Returns:     A new state     (optional) Diagnostic information, depending on the <code>return_diagnostics</code> attribute</p> Source code in <code>bamojax/inference.py</code> <pre><code>def dense_step(self, key, state):\n    \"\"\" Take a MCMC step.\n\n    Can be a composite step to reduce autocorrelation and memory consumption (thinning). Rather than\n    post-hoc removing samples, a dense step can take N steps, then return only the final state, so \n    that we effective thin by a factor N.\n\n    Args:\n        key: Random seed\n        state: The current state\n    Returns:\n        A new state\n        (optional) Diagnostic information, depending on the `return_diagnostics` attribute\n\n    \"\"\"\n    @jax.jit\n    def one_step_fn(state, key):\n        state, info = self.mcmc_kernel.step(key, state)\n        return state, info\n\n    #\n    @jax.jit\n    def one_step_state_only_fn(state, key):\n        state, _ = self.mcmc_kernel.step(key, state)\n        return state, None\n\n    #\n    conditional_step = one_step_fn if self.return_diagnostics else one_step_state_only_fn\n\n    if self.num_thin &gt; 1:\n        keys = jrnd.split(key, self.num_thin)\n        state, infos = jax.lax.scan(conditional_step, state, keys)\n        info = tree_map(lambda x: x[-1, ...], infos)\n        return state, info if self.return_diagnostics else (state, None)\n\n    return conditional_step(state, key)\n</code></pre>"},{"location":"api/#bamojax.inference.MCMCInference.run_single_chain","title":"<code>bamojax.inference.MCMCInference.run_single_chain(key)</code>","text":"<p>Run one MCMC chain</p> <p>Depending on different preferences, this      - optimizes the MCMC kernel hyperparameters     - runs <code>num_burn</code> burn-in samples, that are discarded     - performs the actual sampling for <code>num_samples</code> / <code>num_thin</code> dense steps</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>Random seed</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the resulting collection of states, and optional diagnostic information</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey) -&gt; dict:\n    r\"\"\" Run one MCMC chain\n\n    Depending on different preferences, this \n        - optimizes the MCMC kernel hyperparameters\n        - runs `num_burn` burn-in samples, that are discarded\n        - performs the actual sampling for `num_samples` / `num_thin` dense steps\n\n    Args:\n        key: Random seed\n\n    Returns:\n        A dictionary containing the resulting collection of states, and optional diagnostic information\n\n    \"\"\"\n    def mcmc_body_fn(state, key):\n        state, info = self.dense_step(key, state)\n        return state, (state, info) \n\n    #\n    key, key_init = jrnd.split(key)        \n\n    if self.num_warmup &gt; 0:\n        print('Adapting NUTS HMC parameters...', end=\" \")\n        warm_state, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup) \n        print('done.')\n        adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n        self.mcmc_kernel = adapted_kernel\n        initial_state = self.mcmc_kernel.init(warm_state.position)\n    else:\n        initial_state = self.mcmc_kernel.init(self.model.sample_prior(key_init))        \n\n    if self.num_burn &gt; 0:\n        num_dense_burn_steps = int(self.num_burn / self.num_thin)\n        key, key_burn = jrnd.split(key)\n        keys = jrnd.split(key_burn, num_dense_burn_steps)\n        initial_state, _ = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n    num_dense_steps = int(self.num_samples / self.num_thin)\n    key, key_inference = jrnd.split(key)\n    keys = jrnd.split(key_inference, num_dense_steps)\n    _, (states, info) = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n    return dict(states=states.position, info=info) if self.return_diagnostics else dict(states=states.position)\n</code></pre>"},{"location":"api/#bamojax.inference.MCMCInference.run","title":"<code>bamojax.inference.MCMCInference.run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/#bamojax.inference.SGMCMCInference","title":"<code>bamojax.inference.SGMCMCInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The Stochastic Gradient MCMC approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class SGMCMCInference(InferenceEngine):\n    r\"\"\" The Stochastic Gradient MCMC approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,                   \n                 sgmcmc_kernel: SamplingAlgorithm, \n                 data_size: int, \n                 batch_size: int, \n                 stepsize: float, \n                 batch_nodes: list, \n                 num_chains: int = 1,\n                 num_samples: int = 10_000, \n                 num_burn: int = 10_000, \n                 num_thin: int = 1,  \n                 sgmcmc_params: dict = None):\n        assert batch_size &lt; data_size, f'Batch size must be smaller than data set size, but found batch size: {batch_size} and data size: {data_size}.'\n        super().__init__(model, num_chains)\n\n        if sgmcmc_params is None:\n            sgmcmc_params = {}\n\n        self.num_samples = num_samples\n        self.num_burn = num_burn\n        self.num_thin = num_thin\n\n        self.sgmcmc_params = sgmcmc_params\n        self.data_size = data_size\n        self.batch_size = batch_size\n        self.batch_nodes = batch_nodes\n        self.stepsize = stepsize\n        self.grad_fn = self.grad_estimator()\n        self.sgmcmc_kernel = sgmcmc_kernel(self.grad_fn, **sgmcmc_params)\n\n    #\n    def grad_estimator(self) -&gt; Callable:\n        \"\"\" The stochastic gradient estimator\n\n        Build a simple estimator for the gradient of the log-density, assuming data is mini-batched.\n\n        Returns:\n            The gradient of the batched log-density        \n\n        \"\"\"\n\n        logprior_fn = self.model.logprior_fn()\n        loglikelihood_fn = self.model.batched_loglikelihood_fn()\n\n        def logdensity_estimator_fn(position, minibatch):\n            return logprior_fn(position) + self.data_size / self.batch_size * loglikelihood_fn(position, minibatch)\n\n        #\n        return jax.grad(logdensity_estimator_fn)\n\n    #\n    def get_minibatch(self, data, indices):\n        r\"\"\" Slice a minibatch of data\n\n        Args:\n            data: the data array\n            indices: a set of indices into `data` \n\n        Returns:\n            Dynamic slicing of data[indices, ...]\n\n        \"\"\"\n        if jnp.ndim(data) == 1:\n            data = data[:, jnp.newaxis]    \n        slice_size = (1,) + data.shape[1:]  # For (N, p), this will be (1, p)    \n        return jnp.squeeze(jax.vmap(lambda i: jax.lax.dynamic_slice(data, (i,) + (0,) * (data.ndim - 1), slice_size))(indices))\n\n    #\n    def one_step(self, state, key):\n        r\"\"\" Take one step of stochastic gradient MCMC\n\n        Args:\n            state: Current state\n            key: Random seed\n\n        Returns:\n            The updated state\n\n        \"\"\"\n        key_sgmcmc, key_batch = jrnd.split(key)\n        idx = jrnd.choice(key_batch, self.data_size, shape=(self.batch_size, ), replace=False)\n        minibatch = {node.name: self.get_minibatch(node.observations, idx) for node in self.batch_nodes}\n        state = self.sgmcmc_kernel.step(key_sgmcmc, state, minibatch, self.stepsize)   \n        return state, state\n\n    #\n    def run_single_chain(self, key: PRNGKey):\n        r\"\"\" Run Stochastic Gradient MCMC\n\n        Args: \n            key: PRNGKey\n        Returns:\n            A dictionary of samples for each variable in the Bayesian model.\n\n        \"\"\"\n\n        key, key_init = jrnd.split(key)\n        initial_state = self.sgmcmc_kernel.init(self.model.sample_prior(key_init))\n        step_fn = jax.jit(self.one_step)\n\n        if self.num_burn &gt; 0:\n            key, key_burn = jrnd.split(key)\n            keys = jrnd.split(key_burn, self.num_burn)\n            initial_state, _ = jax.lax.scan(step_fn, initial_state, keys)\n\n        keys = jrnd.split(key, self.num_samples)\n        _, states = jax.lax.scan(step_fn, initial_state, keys)\n\n        return dict(states=states)\n</code></pre>"},{"location":"api/#bamojax.inference.SGMCMCInference.grad_estimator","title":"<code>bamojax.inference.SGMCMCInference.grad_estimator()</code>","text":"<p>The stochastic gradient estimator</p> <p>Build a simple estimator for the gradient of the log-density, assuming data is mini-batched.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>The gradient of the batched log-density</p> Source code in <code>bamojax/inference.py</code> <pre><code>def grad_estimator(self) -&gt; Callable:\n    \"\"\" The stochastic gradient estimator\n\n    Build a simple estimator for the gradient of the log-density, assuming data is mini-batched.\n\n    Returns:\n        The gradient of the batched log-density        \n\n    \"\"\"\n\n    logprior_fn = self.model.logprior_fn()\n    loglikelihood_fn = self.model.batched_loglikelihood_fn()\n\n    def logdensity_estimator_fn(position, minibatch):\n        return logprior_fn(position) + self.data_size / self.batch_size * loglikelihood_fn(position, minibatch)\n\n    #\n    return jax.grad(logdensity_estimator_fn)\n</code></pre>"},{"location":"api/#bamojax.inference.SGMCMCInference.get_minibatch","title":"<code>bamojax.inference.SGMCMCInference.get_minibatch(data, indices)</code>","text":"<p>Slice a minibatch of data</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>the data array</p> required <code>indices</code> <p>a set of indices into <code>data</code> </p> required <p>Returns:</p> Type Description <p>Dynamic slicing of data[indices, ...]</p> Source code in <code>bamojax/inference.py</code> <pre><code>def get_minibatch(self, data, indices):\n    r\"\"\" Slice a minibatch of data\n\n    Args:\n        data: the data array\n        indices: a set of indices into `data` \n\n    Returns:\n        Dynamic slicing of data[indices, ...]\n\n    \"\"\"\n    if jnp.ndim(data) == 1:\n        data = data[:, jnp.newaxis]    \n    slice_size = (1,) + data.shape[1:]  # For (N, p), this will be (1, p)    \n    return jnp.squeeze(jax.vmap(lambda i: jax.lax.dynamic_slice(data, (i,) + (0,) * (data.ndim - 1), slice_size))(indices))\n</code></pre>"},{"location":"api/#bamojax.inference.SGMCMCInference.one_step","title":"<code>bamojax.inference.SGMCMCInference.one_step(state, key)</code>","text":"<p>Take one step of stochastic gradient MCMC</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <p>Current state</p> required <code>key</code> <p>Random seed</p> required <p>Returns:</p> Type Description <p>The updated state</p> Source code in <code>bamojax/inference.py</code> <pre><code>def one_step(self, state, key):\n    r\"\"\" Take one step of stochastic gradient MCMC\n\n    Args:\n        state: Current state\n        key: Random seed\n\n    Returns:\n        The updated state\n\n    \"\"\"\n    key_sgmcmc, key_batch = jrnd.split(key)\n    idx = jrnd.choice(key_batch, self.data_size, shape=(self.batch_size, ), replace=False)\n    minibatch = {node.name: self.get_minibatch(node.observations, idx) for node in self.batch_nodes}\n    state = self.sgmcmc_kernel.step(key_sgmcmc, state, minibatch, self.stepsize)   \n    return state, state\n</code></pre>"},{"location":"api/#bamojax.inference.SGMCMCInference.run_single_chain","title":"<code>bamojax.inference.SGMCMCInference.run_single_chain(key)</code>","text":"<p>Run Stochastic Gradient MCMC</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <p>Returns:     A dictionary of samples for each variable in the Bayesian model.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey):\n    r\"\"\" Run Stochastic Gradient MCMC\n\n    Args: \n        key: PRNGKey\n    Returns:\n        A dictionary of samples for each variable in the Bayesian model.\n\n    \"\"\"\n\n    key, key_init = jrnd.split(key)\n    initial_state = self.sgmcmc_kernel.init(self.model.sample_prior(key_init))\n    step_fn = jax.jit(self.one_step)\n\n    if self.num_burn &gt; 0:\n        key, key_burn = jrnd.split(key)\n        keys = jrnd.split(key_burn, self.num_burn)\n        initial_state, _ = jax.lax.scan(step_fn, initial_state, keys)\n\n    keys = jrnd.split(key, self.num_samples)\n    _, states = jax.lax.scan(step_fn, initial_state, keys)\n\n    return dict(states=states)\n</code></pre>"},{"location":"api/#bamojax.inference.SGMCMCInference.run","title":"<code>bamojax.inference.SGMCMCInference.run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/#bamojax.inference.SMCInference","title":"<code>bamojax.inference.SMCInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The SMC approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class SMCInference(InferenceEngine):\n    r\"\"\" The SMC approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,                   \n                 mcmc_kernel: SamplingAlgorithm, \n                 num_particles: int, \n                 num_mutations: int, \n                 num_chains: int = 1,\n                 mcmc_parameters: dict = None,\n                 resampling_fn = systematic, \n                 target_ess: float = 0.5, \n                 return_trace: bool = False, \n                 return_diagnostics: bool = True,\n                 num_warmup: int = 0,\n                 max_iter: int = 40):\n        super().__init__(model, num_chains)\n        self.mcmc_kernel = mcmc_kernel\n        if mcmc_parameters is None:\n            mcmc_parameters = {}\n        self.mcmc_parameters = mcmc_parameters\n        self.num_particles = num_particles\n        self.num_mutations = num_mutations\n        self.resampling_fn = resampling_fn\n        self.target_ess = target_ess\n        self.return_trace = return_trace\n        self.return_diagnostics = return_diagnostics\n        self.num_warmup = num_warmup\n        self.max_iter = max_iter\n        assert not (return_diagnostics and return_trace), 'Returning both the trace and diagnostics is not supported.'\n\n    #\n    def create_smc_kernel(self) -&gt; SamplingAlgorithm:\n        r\"\"\" Creates an SMC kernel\n\n        Currently a lightweight wrapper around the blackjax adaptive-tempered SMC object.\n\n        Returns:\n            A blackjax SMC `SamplingAlgorithm`\n\n        \"\"\"\n        return adaptive_tempered_smc(logprior_fn=self.model.logprior_fn(),\n                                     loglikelihood_fn=self.model.loglikelihood_fn(),\n                                     mcmc_step_fn=self.mcmc_kernel.step,\n                                     mcmc_init_fn=self.mcmc_kernel.init,\n                                     mcmc_parameters=self.mcmc_parameters,\n                                     resampling_fn=self.resampling_fn,\n                                     target_ess=self.target_ess,\n                                     num_mcmc_steps=self.num_mutations)\n\n    #    \n    def tempering_condition(self):\n        r\"\"\" Checks whether the SMC procedure terminates.\n\n        Returns:\n            A boolean deciding on whether the SMC tempering procedure is finished\n\n        \"\"\"\n        def cond(carry):\n            i, state, *_ = carry\n            if self.return_trace:\n                return jnp.logical_and(state.lmbda &lt; 1, i &lt; self.max_iter)\n            return state.lmbda &lt; 1\n\n        #\n        return cond\n\n    #\n    def smc_cycle(self, smc_kernel) -&gt; Callable:\n        r\"\"\" One iteration of the adaptive-tempered SMC algorithm.\n\n        Args:\n            smc_kernel: A Blackjax SamplingAlgorithm containing the SMC logic.\n        Returns:\n            A Callable step function that performs one iteration.\n\n        \"\"\"\n        @jax.jit\n        def one_step(carry): \n            if self.return_trace:\n                i, state, k, curr_log_likelihood, state_hist = carry\n            else:               \n                if self.return_diagnostics:\n                    i, state, k, curr_log_likelihood, _ = carry \n                else:\n                    i, state, k, curr_log_likelihood = carry \n            k, subk = jrnd.split(k)\n            state, info = smc_kernel.step(subk, state)    \n            base_return_tuple = (i + 1, state, k, curr_log_likelihood + info.log_likelihood_increment)\n            if self.return_trace:\n                state_hist = tree_map(lambda arr, val: arr.at[i].set(val), state_hist, state)\n                return base_return_tuple + (state_hist, )            \n\n            return base_return_tuple + (info, ) if self.return_diagnostics else base_return_tuple\n\n        #\n        return one_step\n\n    #\n\n    def run_single_chain(self, key: PRNGKey) -&gt; dict:\n        r\"\"\" Run one chain of Sequential Monte Carlo.\n\n        This returns SMC particles in different ways, depending on user provided settings.\n\n        Args:\n            key: PRNGKey\n        Returns:\n            A dictionary with the final SMC state, the number of iterations, the log marginal likelihood\n            (Optional) diagnostics and the trace of particles over each tempering step\n\n        \"\"\"\n\n        if self.num_warmup &gt; 0:\n            key, key_init = jrnd.split(key)\n            print('Adapting NUTS HMC parameters...', end=\" \")\n            _, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup)\n            print('done.')\n            adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n            self.mcmc_kernel = adapted_kernel\n\n        smc_kernel = self.create_smc_kernel()\n        smc_cycle = self.smc_cycle(smc_kernel)\n        cond = self.tempering_condition()\n\n        key, key_init = jrnd.split(key)        \n        keys = jrnd.split(key_init, self.num_particles)\n        initial_particles = smc_kernel.init(jax.vmap(self.model.sample_prior)(keys))\n\n        if self.return_trace or self.return_diagnostics:\n            # Call SMC once to determine PyTree structures\n            key_smc, key_init = jrnd.split(key)\n            initial_particles, sample_info = smc_kernel.step(key_init, initial_particles)\n            initial_info = tree_map(lambda x: jax.numpy.zeros_like(x), sample_info)\n            initial_log_likelihood = sample_info.log_likelihood_increment\n\n        if self.return_trace:\n             # Preallocate arrays for state and info history\n            trace = tree_map(lambda x: jnp.zeros((self.max_iter,) + x.shape, dtype=x.dtype), initial_particles)\n            trace = jax.tree_util.tree_map(lambda arr, val: arr.at[0].set(val), trace, initial_particles)\n            n_iter, final_state, _, lml, trace = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, trace))\n            trace = tree_map(lambda x: x[:n_iter], trace)\n            return dict(\n                n_iter=n_iter, \n                final_state=final_state, \n                lml=lml, \n                trace=trace\n            )\n        if self.return_diagnostics:            \n            n_iter, final_state, _, lml, final_info = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, initial_info))\n            return dict(\n                n_iter=n_iter, \n                final_state=final_state, \n                lml=lml, \n                final_info=final_info\n            )            \n        else:\n            n_iter, final_state, _, lml, = jax.lax.while_loop(cond, smc_cycle, (0, initial_particles, key, 0))\n            return dict(\n                n_iter=n_iter, \n                final_state=final_state, \n                lml=lml\n            )  \n</code></pre>"},{"location":"api/#bamojax.inference.SMCInference.create_smc_kernel","title":"<code>bamojax.inference.SMCInference.create_smc_kernel()</code>","text":"<p>Creates an SMC kernel</p> <p>Currently a lightweight wrapper around the blackjax adaptive-tempered SMC object.</p> <p>Returns:</p> Type Description <code>SamplingAlgorithm</code> <p>A blackjax SMC <code>SamplingAlgorithm</code></p> Source code in <code>bamojax/inference.py</code> <pre><code>def create_smc_kernel(self) -&gt; SamplingAlgorithm:\n    r\"\"\" Creates an SMC kernel\n\n    Currently a lightweight wrapper around the blackjax adaptive-tempered SMC object.\n\n    Returns:\n        A blackjax SMC `SamplingAlgorithm`\n\n    \"\"\"\n    return adaptive_tempered_smc(logprior_fn=self.model.logprior_fn(),\n                                 loglikelihood_fn=self.model.loglikelihood_fn(),\n                                 mcmc_step_fn=self.mcmc_kernel.step,\n                                 mcmc_init_fn=self.mcmc_kernel.init,\n                                 mcmc_parameters=self.mcmc_parameters,\n                                 resampling_fn=self.resampling_fn,\n                                 target_ess=self.target_ess,\n                                 num_mcmc_steps=self.num_mutations)\n</code></pre>"},{"location":"api/#bamojax.inference.SMCInference.tempering_condition","title":"<code>bamojax.inference.SMCInference.tempering_condition()</code>","text":"<p>Checks whether the SMC procedure terminates.</p> <p>Returns:</p> Type Description <p>A boolean deciding on whether the SMC tempering procedure is finished</p> Source code in <code>bamojax/inference.py</code> <pre><code>def tempering_condition(self):\n    r\"\"\" Checks whether the SMC procedure terminates.\n\n    Returns:\n        A boolean deciding on whether the SMC tempering procedure is finished\n\n    \"\"\"\n    def cond(carry):\n        i, state, *_ = carry\n        if self.return_trace:\n            return jnp.logical_and(state.lmbda &lt; 1, i &lt; self.max_iter)\n        return state.lmbda &lt; 1\n\n    #\n    return cond\n</code></pre>"},{"location":"api/#bamojax.inference.SMCInference.smc_cycle","title":"<code>bamojax.inference.SMCInference.smc_cycle(smc_kernel)</code>","text":"<p>One iteration of the adaptive-tempered SMC algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>smc_kernel</code> <p>A Blackjax SamplingAlgorithm containing the SMC logic.</p> required <p>Returns:     A Callable step function that performs one iteration.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def smc_cycle(self, smc_kernel) -&gt; Callable:\n    r\"\"\" One iteration of the adaptive-tempered SMC algorithm.\n\n    Args:\n        smc_kernel: A Blackjax SamplingAlgorithm containing the SMC logic.\n    Returns:\n        A Callable step function that performs one iteration.\n\n    \"\"\"\n    @jax.jit\n    def one_step(carry): \n        if self.return_trace:\n            i, state, k, curr_log_likelihood, state_hist = carry\n        else:               \n            if self.return_diagnostics:\n                i, state, k, curr_log_likelihood, _ = carry \n            else:\n                i, state, k, curr_log_likelihood = carry \n        k, subk = jrnd.split(k)\n        state, info = smc_kernel.step(subk, state)    \n        base_return_tuple = (i + 1, state, k, curr_log_likelihood + info.log_likelihood_increment)\n        if self.return_trace:\n            state_hist = tree_map(lambda arr, val: arr.at[i].set(val), state_hist, state)\n            return base_return_tuple + (state_hist, )            \n\n        return base_return_tuple + (info, ) if self.return_diagnostics else base_return_tuple\n\n    #\n    return one_step\n</code></pre>"},{"location":"api/#bamojax.inference.SMCInference.run_single_chain","title":"<code>bamojax.inference.SMCInference.run_single_chain(key)</code>","text":"<p>Run one chain of Sequential Monte Carlo.</p> <p>This returns SMC particles in different ways, depending on user provided settings.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <p>Returns:     A dictionary with the final SMC state, the number of iterations, the log marginal likelihood     (Optional) diagnostics and the trace of particles over each tempering step</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey) -&gt; dict:\n    r\"\"\" Run one chain of Sequential Monte Carlo.\n\n    This returns SMC particles in different ways, depending on user provided settings.\n\n    Args:\n        key: PRNGKey\n    Returns:\n        A dictionary with the final SMC state, the number of iterations, the log marginal likelihood\n        (Optional) diagnostics and the trace of particles over each tempering step\n\n    \"\"\"\n\n    if self.num_warmup &gt; 0:\n        key, key_init = jrnd.split(key)\n        print('Adapting NUTS HMC parameters...', end=\" \")\n        _, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup)\n        print('done.')\n        adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n        self.mcmc_kernel = adapted_kernel\n\n    smc_kernel = self.create_smc_kernel()\n    smc_cycle = self.smc_cycle(smc_kernel)\n    cond = self.tempering_condition()\n\n    key, key_init = jrnd.split(key)        \n    keys = jrnd.split(key_init, self.num_particles)\n    initial_particles = smc_kernel.init(jax.vmap(self.model.sample_prior)(keys))\n\n    if self.return_trace or self.return_diagnostics:\n        # Call SMC once to determine PyTree structures\n        key_smc, key_init = jrnd.split(key)\n        initial_particles, sample_info = smc_kernel.step(key_init, initial_particles)\n        initial_info = tree_map(lambda x: jax.numpy.zeros_like(x), sample_info)\n        initial_log_likelihood = sample_info.log_likelihood_increment\n\n    if self.return_trace:\n         # Preallocate arrays for state and info history\n        trace = tree_map(lambda x: jnp.zeros((self.max_iter,) + x.shape, dtype=x.dtype), initial_particles)\n        trace = jax.tree_util.tree_map(lambda arr, val: arr.at[0].set(val), trace, initial_particles)\n        n_iter, final_state, _, lml, trace = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, trace))\n        trace = tree_map(lambda x: x[:n_iter], trace)\n        return dict(\n            n_iter=n_iter, \n            final_state=final_state, \n            lml=lml, \n            trace=trace\n        )\n    if self.return_diagnostics:            \n        n_iter, final_state, _, lml, final_info = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, initial_info))\n        return dict(\n            n_iter=n_iter, \n            final_state=final_state, \n            lml=lml, \n            final_info=final_info\n        )            \n    else:\n        n_iter, final_state, _, lml, = jax.lax.while_loop(cond, smc_cycle, (0, initial_particles, key, 0))\n        return dict(\n            n_iter=n_iter, \n            final_state=final_state, \n            lml=lml\n        )  \n</code></pre>"},{"location":"api/#bamojax.inference.SMCInference.run","title":"<code>bamojax.inference.SMCInference.run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/#bamojax.inference.VIInference","title":"<code>bamojax.inference.VIInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The VI approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class VIInference(InferenceEngine):\n    r\"\"\" The VI approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,                  \n                 num_steps: int,\n                 num_chains: int = 1,\n                 num_gradient_samples: int = 10,\n                 optimizer: Callable = optax.sgd,\n                 optimizer_chain_args: list = None):\n        super().__init__(model, num_chains)\n        self.num_steps = num_steps\n        self.num_gradient_samples = num_gradient_samples\n        if optimizer_chain_args is None:\n            self.optimizer = optimizer\n        else:\n            if not isinstance(optimizer_chain_args, list):\n                optimizer_chain_args = [optimizer_chain_args]\n            self.optimizer = optax.chain(*optimizer_chain_args, optimizer)\n        self.bijectors = get_model_bijectors(self.model)\n        self.is_leaf_fn = lambda x: hasattr(x, 'forward') and hasattr(x, 'inverse')\n        self.forward_bijectors = lambda x: jax.tree.map(lambda b, v: b.forward(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n        self.backward_bijectors = lambda x: jax.tree.map(lambda b, v: b.inverse(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n\n        def logdensity_fn(z):\n            z = self.forward_bijectors(z)\n            return model.loglikelihood_fn()(z) + model.logprior_fn()(z)\n\n        #\n        self.logdensity_fn = logdensity_fn\n\n    #   \n    def sample_from_variational(self, key: PRNGKey, vi_result: dict, num_draws: int) -&gt; dict:\n        r\"\"\" Draw samples x ~ q(x | mu, rho)\n\n        Args:\n            key: PRNGKey\n            vi_result: a dictionary containing the variational approximation\n            num_draws: the number of samples to draw from the variational distribution.\n\n        Returns:\n            A dictionary with samples from the variational distribution, for each variable in the model.        \n\n        \"\"\"\n\n        if self.num_chains &gt; 1:\n            final_state = tree_map(lambda x: x[:, -1, ...], vi_result['states'])\n        else:\n            final_state = tree_map(lambda x: x[-1, ...], vi_result['states'])\n\n        def sample_fn(key, loc, scale):\n            return loc + scale*jrnd.normal(key, shape=(num_draws, ) + loc.shape)\n\n        #\n        vi_mu = final_state.mu\n        vi_rho = tree_map(lambda x: jnp.exp(x), final_state.rho)\n\n        flat_pytree, treedef = tree_flatten(vi_mu)\n        num_leaves = len(flat_pytree)\n        keys = jrnd.split(key, num_leaves)\n        keys_pytree = tree_unflatten(treedef, keys)\n\n        vi_samples = tree_map(sample_fn, keys_pytree, vi_mu, vi_rho)\n        if self.num_chains &gt; 1:\n            vi_samples = tree_map(lambda x: jnp.swapaxes(x, 0, 1), vi_samples)\n\n        # apply bijectors\n        vi_samples = self.forward_bijectors(vi_samples)\n        return vi_samples\n\n    #\n    def run_single_chain(self, key: PRNGKey) -&gt; dict:\n        r\"\"\" Run variational inference.\n\n        Args: \n            key: PRNGKey\n        Returns:\n            A dictionary with the variational parameters across iterations, and logging results such as the ELBO.\n\n        \"\"\"\n        mfvi = meanfield_vi(self.logdensity_fn, self.optimizer, self.num_gradient_samples)\n        initial_position = self.model.sample_prior(key=jrnd.PRNGKey(0))  # these are overriden by Blackjax\n        initial_state = mfvi.init(initial_position)\n\n        @jax.jit\n        def one_step(state, rng_key):\n            state, info = mfvi.step(rng_key, state)\n            return state, (state, info)\n\n        #\n        keys = jrnd.split(key, self.num_steps)\n        _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n        return dict(\n            states=states, \n            info=infos\n        )\n</code></pre>"},{"location":"api/#bamojax.inference.VIInference.sample_from_variational","title":"<code>bamojax.inference.VIInference.sample_from_variational(key, vi_result, num_draws)</code>","text":"<p>Draw samples x ~ q(x | mu, rho)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <code>vi_result</code> <code>dict</code> <p>a dictionary containing the variational approximation</p> required <code>num_draws</code> <code>int</code> <p>the number of samples to draw from the variational distribution.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with samples from the variational distribution, for each variable in the model.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def sample_from_variational(self, key: PRNGKey, vi_result: dict, num_draws: int) -&gt; dict:\n    r\"\"\" Draw samples x ~ q(x | mu, rho)\n\n    Args:\n        key: PRNGKey\n        vi_result: a dictionary containing the variational approximation\n        num_draws: the number of samples to draw from the variational distribution.\n\n    Returns:\n        A dictionary with samples from the variational distribution, for each variable in the model.        \n\n    \"\"\"\n\n    if self.num_chains &gt; 1:\n        final_state = tree_map(lambda x: x[:, -1, ...], vi_result['states'])\n    else:\n        final_state = tree_map(lambda x: x[-1, ...], vi_result['states'])\n\n    def sample_fn(key, loc, scale):\n        return loc + scale*jrnd.normal(key, shape=(num_draws, ) + loc.shape)\n\n    #\n    vi_mu = final_state.mu\n    vi_rho = tree_map(lambda x: jnp.exp(x), final_state.rho)\n\n    flat_pytree, treedef = tree_flatten(vi_mu)\n    num_leaves = len(flat_pytree)\n    keys = jrnd.split(key, num_leaves)\n    keys_pytree = tree_unflatten(treedef, keys)\n\n    vi_samples = tree_map(sample_fn, keys_pytree, vi_mu, vi_rho)\n    if self.num_chains &gt; 1:\n        vi_samples = tree_map(lambda x: jnp.swapaxes(x, 0, 1), vi_samples)\n\n    # apply bijectors\n    vi_samples = self.forward_bijectors(vi_samples)\n    return vi_samples\n</code></pre>"},{"location":"api/#bamojax.inference.VIInference.run_single_chain","title":"<code>bamojax.inference.VIInference.run_single_chain(key)</code>","text":"<p>Run variational inference.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <p>Returns:     A dictionary with the variational parameters across iterations, and logging results such as the ELBO.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey) -&gt; dict:\n    r\"\"\" Run variational inference.\n\n    Args: \n        key: PRNGKey\n    Returns:\n        A dictionary with the variational parameters across iterations, and logging results such as the ELBO.\n\n    \"\"\"\n    mfvi = meanfield_vi(self.logdensity_fn, self.optimizer, self.num_gradient_samples)\n    initial_position = self.model.sample_prior(key=jrnd.PRNGKey(0))  # these are overriden by Blackjax\n    initial_state = mfvi.init(initial_position)\n\n    @jax.jit\n    def one_step(state, rng_key):\n        state, info = mfvi.step(rng_key, state)\n        return state, (state, info)\n\n    #\n    keys = jrnd.split(key, self.num_steps)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n    return dict(\n        states=states, \n        info=infos\n    )\n</code></pre>"},{"location":"api/#bamojax.inference.VIInference.run","title":"<code>bamojax.inference.VIInference.run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/#bamojax.inference.LaplaceInference","title":"<code>bamojax.inference.LaplaceInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The Laplace approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class LaplaceInference(InferenceEngine):\n    r\"\"\" The Laplace approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,   \n                 num_chains: int = 1,\n                 optimizer: Callable = jaxopt.ScipyMinimize,\n                 optimizer_args: dict = None,\n                 bounds: dict = None):\n        super().__init__(model, num_chains)\n        if optimizer_args is None:\n            optimizer_args = {}\n        self.bounds = bounds\n\n        self.bijectors = get_model_bijectors(model)\n        self.is_leaf_fn = lambda x: hasattr(x, 'forward') and hasattr(x, 'inverse')\n        self.forward_bijectors = lambda x: jax.tree.map(lambda b, v: b.forward(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n        self.backward_bijectors = lambda x: jax.tree.map(lambda b, v: b.inverse(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n\n        @jax.jit\n        def logdensity_fn(z):\n            z = self.forward_bijectors(z)\n            return -1.0 * (model.loglikelihood_fn()(z) + model.logprior_fn()(z))\n\n        #\n        self.obj_fun = logdensity_fn\n\n        if self.bounds is not None:\n            optimizer = jaxopt.ScipyBoundedMinimize(fun=self.obj_fun, **optimizer_args)\n        else:\n            optimizer = optimizer(fun=self.obj_fun, **optimizer_args)\n        self.optimizer = optimizer\n        self.optimizer_args = optimizer_args if optimizer_args is not None else {}        \n        self.D = model.get_model_size()\n\n    #\n    def run_single_chain(self, key):\n        def get_unconstrained_init(model, key):\n            constrained = tree_map(jnp.asarray, model.sample_prior(key))\n            unconstrained = self.backward_bijectors(constrained)\n            return unconstrained\n\n        #\n        init_params = get_unconstrained_init(self.model, key)\n\n        if self.bounds is not None:\n            sol = self.optimizer.run(init_params, bounds=self.bounds)   \n        else:\n            sol = self.optimizer.run(init_params)   \n\n        # We fit a Gaussian(\\hat{\\theta}, \\Sigma) with \n        # \\hat{\\theta} = \\argmax_\\theta p(\\theta \\mid y)\n        # \\Sigma^-1 is the Hessian of -\\log p(\\theta \\mid y) at \\theta=\\hat{\\theta}\n\n        mode = self.forward_bijectors(sol.params)\n        H = jax.hessian(self.obj_fun)(mode)\n        theta_hat_flat, unravel_fn = ravel_pytree(mode)\n\n        def flat_obj_fn(flat_params):\n            params = unravel_fn(flat_params)\n            return self.obj_fun(params)\n\n        H = jax.hessian(flat_obj_fn)(theta_hat_flat)\n\n        Sigma = jnp.linalg.inv(H)\n\n        if theta_hat_flat.shape == () or theta_hat_flat.shape == (1,):  # Univariate case\n            dist = dx.Normal(loc=theta_hat_flat, scale=jnp.sqrt(Sigma))\n        else:\n            dist = dx.MultivariateNormalFullCovariance(loc=theta_hat_flat, covariance_matrix=Sigma)\n\n        _, logdet = jnp.linalg.slogdet(Sigma)\n\n        log_posterior = -1.0 * self.obj_fun(mode)\n        lml = log_posterior + 1/2*logdet + self.D/2 * jnp.log(2*jnp.pi)\n\n        return dict(\n            distribution=dist,\n            mode=mode,\n            flat_mode=theta_hat_flat,\n            covariance=Sigma,\n            hessian=H,\n            unravel_fn=unravel_fn,\n            lml=lml\n        )\n</code></pre>"},{"location":"api/#bamojax.inference.LaplaceInference.run","title":"<code>bamojax.inference.LaplaceInference.run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/#bamojax.inference.run_window_adaptation","title":"<code>bamojax.inference.run_window_adaptation(model, key, num_warmup_steps)</code>","text":"<p>Find optimized parameters for HMC-based inference.</p> <p>A wrapper for blackjax HMC window adaptation.</p> <p>Parameters:</p> Name Type Description Default <code>Model</code> <p>A bamojax model (DAG)</p> required <code>key</code> <code>PRNGKey</code> <p>Random seed</p> required <code>num_warmum_steps</code> <p>int, number of warmup steps to take</p> required <p>Returns:</p> Type Description <ul> <li>A dictionary of the state after performing the warmup steps</li> </ul> <ul> <li>A set of HMC kernel parameters that result in optimal acceptance rates</li> </ul> Source code in <code>bamojax/inference.py</code> <pre><code>def run_window_adaptation(model, key: PRNGKey, num_warmup_steps):\n    r\"\"\" Find optimized parameters for HMC-based inference.\n\n    A wrapper for blackjax HMC window adaptation.\n\n    Args:\n        Model: A bamojax model (DAG)\n        key: Random seed\n        num_warmum_steps: int, number of warmup steps to take\n\n    Returns:\n        - A dictionary of the state after performing the warmup steps\n        - A set of HMC kernel parameters that result in optimal acceptance rates\n\n    \"\"\"\n    logdensity_fn = lambda state: model.loglikelihood_fn()(state) + model.logprior_fn()(state)\n    warmup = window_adaptation(nuts, logdensity_fn)\n    key_init, key_warmup = jrnd.split(key)\n    initial_state = model.sample_prior(key_init)\n    (warm_state, warm_parameters), _ = warmup.run(key_warmup, initial_state, num_steps=num_warmup_steps)  \n    return warm_state, warm_parameters\n</code></pre>"},{"location":"api/#bamojax.inference.get_model_bijectors","title":"<code>bamojax.inference.get_model_bijectors(model)</code>","text":"<p>Meanfield VI imposes a Gaussian variational distribution. </p> <p>In order to use the correct parameter constraints this function determines all relevant bijectors.</p> <p>It's a bijector detector! :-)</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with bijectors for the variables that use it, and an identity bijector otherwise.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def get_model_bijectors(model) -&gt; dict:\n    r\"\"\" Meanfield VI imposes a Gaussian variational distribution. \n\n    In order to use the correct parameter constraints this function determines all relevant bijectors.\n\n    It's a bijector detector! :-)\n\n    Returns:\n        A dictionary with bijectors for the variables that use it, and an identity bijector otherwise.\n\n    \"\"\"\n    latent_nodes = model.get_latent_nodes()\n    bijectors = {}\n    for node_name, node in latent_nodes.items():\n        if hasattr(node.distribution, '_bijector'):\n            bij = node.distribution._bijector\n            transform = bij\n        else:\n            transform = tfb.Identity()  # Use TensorFlow Probability's Identity bijector\n        bijectors[node_name] = transform\n    return bijectors\n</code></pre>"},{"location":"api/#bamojax.marginal_likelihoods","title":"<code>bamojax.marginal_likelihoods</code>","text":""},{"location":"api/#bamojax.marginal_likelihoods.iid_likelihood","title":"<code>bamojax.marginal_likelihoods.iid_likelihood(L)</code>","text":"<p>We typically have multiple observations and assume the likelihood factorizes  as: </p> \\[         \\log p\\left(Y \\mid \\theta\\right) = \\sum_{i=1}^N \\log p\\left(y_i \\mid \\theta\\right) \\enspace. \\] Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def iid_likelihood(L: Callable):\n    r\"\"\"\n\n    We typically have multiple observations and assume the likelihood factorizes \n    as: \n\n    $$    \n        \\log p\\left(Y \\mid \\theta\\right) = \\sum_{i=1}^N \\log p\\left(y_i \\mid \\theta\\right) \\enspace.\n    $$\n\n    \"\"\"\n    return lambda x: jnp.sum(L()(x))\n</code></pre>"},{"location":"api/#bamojax.marginal_likelihoods.naive_monte_carlo","title":"<code>bamojax.marginal_likelihoods.naive_monte_carlo(key, model, num_prior_draws=1000, num_chunks=5, iid_obs=True, pb=True)</code>","text":"<p>The Naive Monte Carlo (NMC) estimator</p> <p>The marginal likelihood is defined as  $$     p(D) = \\int_\\Theta p\\left(D \\mid \\theta\\right) p(\\theta) \\,\\text{d}\\theta \\enspace . $$</p> <p>In NMC we draw samples from the prior and approximate the ML as $$     p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right), with \\theta_i \\sim p(\\theta) \\enspace. $$ In nontrivial models, we need a large \\(N\\) for this approximation to be  reasonable.</p> Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def naive_monte_carlo(key, \n                      model: Model, \n                      num_prior_draws: int = 1_000, \n                      num_chunks: int = 5,\n                      iid_obs: bool = True,\n                      pb = True) -&gt; Float:   \n    r\"\"\"The Naive Monte Carlo (NMC) estimator\n\n    The marginal likelihood is defined as \n    $$\n        p(D) = \\int_\\Theta p\\left(D \\mid \\theta\\right) p(\\theta) \\,\\text{d}\\theta \\enspace .\n    $$\n\n    In NMC we draw samples from the prior and approximate the ML as\n    $$\n        p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right), with \\theta_i \\sim p(\\theta) \\enspace.\n    $$\n    In nontrivial models, we need a *large* $N$ for this approximation to be \n    reasonable.\n\n    \"\"\"\n\n    if iid_obs:\n        loglikelihood_fn = iid_likelihood(model.loglikelihood_fn)\n    else:\n        loglikelihood_fn = model.loglikelihood_fn\n\n    loglikelihoods = jnp.zeros((num_prior_draws, \n                                num_chunks))\n\n    # We don't want to vmap this loop, as the reason for the loop is to avoid\n    # running out of memory!\n    for i in tqdm(range(num_chunks), disable=not pb):\n        key, subkey = jrnd.split(key)\n        keys = jrnd.split(subkey, num_prior_draws)\n        prior_draws = jax.vmap(model.sample_prior)(keys)\n        loglikelihoods = loglikelihoods.at[:, i].set(jax.vmap(loglikelihood_fn)(prior_draws))\n    return logsumexp(loglikelihoods.flatten()) - jnp.log(num_prior_draws*num_chunks)\n</code></pre>"},{"location":"api/#bamojax.marginal_likelihoods.importance_sampling","title":"<code>bamojax.marginal_likelihoods.importance_sampling(key, model, g_IS, num_samples=1000, iid_obs=True)</code>","text":"<p>Importance sampling routine for a given BayesianModel.</p> <p>Importance sampling is based around the following approximation to the log marginal likelihood [Gronau et al., 2017]:</p> <p>$$ p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right) \\frac{p(\\theta_i)}{g_IS(\\theta_i)}\\enspace, $$ with \\(\\theta_i \\sim g_IS(\\theta)\\)</p> <p>Here, g_IS is the importance density, which should meet these criteria:</p> <ol> <li>It is easy to evaluate.</li> <li>It has the same domain as the posterior p(\\theta \\mid D).</li> <li>It matches the posterior as closely as possible.</li> <li>It has fatter tails than the posterior.</li> </ol> <p>There is no one-size-fits-all importance density; this needs to be crafted carefully for each specific problem.</p> <p>Note that the importance density can also be a mixture distribution, which  can make it easier to introduce heavy tails.</p> <p>References:</p> <ul> <li>Gronau, Q. F., Sarafoglou, A., Matzke, D., Ly, A., Boehm, U., Marsman, M., Leslie, D. S., Forster, J. J., Wagenmakers, E.-J., &amp; Steingroever, H. (2017). A tutorial on bridge sampling. Journal of Mathematical Psychology, 81, 80-97. https://doi.org/10.1016/j.jmp.2017.09.005</li> </ul> Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def importance_sampling(key, \n                        model: Model, \n                        g_IS: Distribution,\n                        num_samples: int = 1_000,\n                        iid_obs: bool = True) -&gt; Float:\n\n    r\"\"\"Importance sampling routine for a given BayesianModel.\n\n    Importance sampling is based around the following approximation to the log\n    marginal likelihood [Gronau et al., 2017]:\n\n    $$\n    p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right) \\frac{p(\\theta_i)}{g_IS(\\theta_i)}\\enspace,\n    $$\n    with $\\theta_i \\sim g_IS(\\theta)$\n\n    Here, g_IS is the importance density, which should meet these criteria:\n\n    1. It is easy to evaluate.\n    2. It has the same domain as the posterior p(\\theta \\mid D).\n    3. It matches the posterior as closely as possible.\n    4. It has fatter tails than the posterior.\n\n    There is no one-size-fits-all importance density; this needs to be crafted\n    carefully for each specific problem.\n\n    Note that the importance density can also be a mixture distribution, which \n    can make it easier to introduce heavy tails.\n\n    References:\n\n    - Gronau, Q. F., Sarafoglou, A., Matzke, D., Ly, A., Boehm, U., Marsman, M., Leslie, D. S., Forster, J. J., Wagenmakers, E.-J., &amp; Steingroever, H. (2017). A tutorial on bridge sampling. Journal of Mathematical Psychology, 81, 80-97. https://doi.org/10.1016/j.jmp.2017.09.005\n\n\n    \"\"\"\n\n    def g_eval(state):\n        logprob = 0        \n        values_flat, _ = tree_flatten(state)\n        for value, dist in zip(values_flat, g_flat):\n            logprob += jnp.sum(dist.log_prob(value))\n        return logprob\n\n    # \n    def adjusted_likelihood(state):\n        return loglikelihood_fn(state) + logprior_fn(state) - g_eval(state)\n\n    #\n\n    if iid_obs:\n        loglikelihood_fn = iid_likelihood(model.loglikelihood_fn)\n    else:\n        loglikelihood_fn = model.loglikelihood_fn\n\n    logprior_fn = model.logprior_fn()\n\n    g_flat, g_treedef = tree_flatten(g_IS, \n                                     lambda l: isinstance(l, (Distribution, Bijector)))\n\n    samples = list()\n    for g in g_flat:\n        key, subkey = jrnd.split(key)\n        samples.append(g.sample(seed=subkey, sample_shape=(num_samples, )))\n\n    importance_samples = tree_unflatten(g_treedef, samples)\n    adjusted_likelihoods = jax.vmap(adjusted_likelihood)(importance_samples)\n    return logsumexp(adjusted_likelihoods) - jnp.log(num_samples)\n</code></pre>"},{"location":"api/#bamojax.marginal_likelihoods.laplace_approximation","title":"<code>bamojax.marginal_likelihoods.laplace_approximation(key, model, iid_obs=True, **opt_args)</code>","text":"<p>Compute the Laplace approximation of the log marginal likelihood of model</p> <p>The Laplace approximation approximates the posterior density of the model  with a Gaussian, centered at the mode of the density and with its curvature determined by the Hessian matrix of the negative log posterior density.</p> <p>The marginal likelihood of this proxy distribution is known in closed-form, and is used to approximate the actual marginal likelihood.</p> <p>See https://en.wikipedia.org/wiki/Laplace%27s_approximation</p> Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def laplace_approximation(key,\n                          model: Model,\n                          iid_obs: bool= True,\n                          **opt_args):\n\n    r\"\"\"Compute the Laplace approximation of the log marginal likelihood of model\n\n    The Laplace approximation approximates the posterior density of the model \n    with a Gaussian, centered at the mode of the density and with its curvature\n    determined by the Hessian matrix of the negative log posterior density.\n\n    The marginal likelihood of this proxy distribution is known in closed-form,\n    and is used to approximate the actual marginal likelihood.\n\n    See https://en.wikipedia.org/wiki/Laplace%27s_approximation\n\n    \"\"\"\n\n    # The objective function is the unnormalized posterior\n    @jax.jit\n    def fun(x):\n        return -1.0 * (loglikelihood_fn(x) + logprior_fn(x))\n\n    #\n    if iid_obs:\n        loglikelihood_fn = iid_likelihood(model.loglikelihood_fn)\n    else:\n        loglikelihood_fn = model.loglikelihood_fn\n    logprior_fn = model.logprior_fn()\n\n    # Get initial values in the same PyTree structure as the model expects\n    init_params = tree_map(jnp.asarray, \n                           model.sample_prior(key))\n\n    # For some models, the parameters are bounded\n    if 'bounds' in opt_args:\n        print(opt_args['bounds'])\n        solver = jaxopt.ScipyBoundedMinimize(fun=fun)\n    else:\n        solver = jaxopt.ScipyMinimize(fun=fun)\n\n\n    # Derive the number of parameters\n    D = 0\n    vars_flattened, _ = tree_flatten(init_params)\n    for varval in vars_flattened:\n        D += varval.shape[0] if varval.shape else 1\n\n    # Compute MAP\n    sol = solver.run(init_params, **opt_args)   \n\n    # We fit a Gaussian(\\hat{\\theta}, \\Sigma) with \n    # \\hat{\\theta} = \\argmax_\\theta p(\\theta \\mid y)\n    # \\Sigma^-1 is the Hessian of -\\log p(\\theta \\mid y) at \\theta=\\hat{\\theta}\n\n    mode = sol.params\n    H = jax.hessian(fun)(mode)\n    h, _ = tree_flatten(H)\n    if D &gt; 1:\n        S = jnp.squeeze(jnp.linalg.inv(jnp.reshape(jnp.asarray(h), \n                                                   newshape=(D, D))))\n        _, logdet = jnp.linalg.slogdet(S)\n    else: \n        S = 1.0 / jnp.squeeze(jnp.asarray(h))\n        logdet = jnp.log(S)\n\n    log_posterior = -1.0 * sol.state.fun_val\n    lml = log_posterior + 1/2*logdet + D/2 * jnp.log(2*jnp.pi)\n    return lml\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax","title":"<code>bamojax.modified_blackjax</code>","text":""},{"location":"api/#bamojax.modified_blackjax.modified_adaptive_tempered","title":"<code>bamojax.modified_blackjax.modified_adaptive_tempered</code>","text":""},{"location":"api/#bamojax.modified_blackjax.modified_adaptive_tempered.build_kernel","title":"<code>bamojax.modified_blackjax.modified_adaptive_tempered.build_kernel(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, resampling_fn, target_ess, root_solver=solver.dichotomy, **extra_parameters)</code>","text":"<p>Build a Tempered SMC step using an adaptive schedule.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_adaptive_tempered.build_kernel--parameters","title":"Parameters","text":"<p>logprior_fn: Callable     A function that computes the log-prior density. loglikelihood_fn: Callable     A function that returns the log-likelihood density. mcmc_kernel_factory: Callable     A callable function that creates a mcmc kernel from a log-probability     density function. make_mcmc_state: Callable     A function that creates a new mcmc state from a position and a     log-probability density function. resampling_fn: Callable     A random function that resamples generated particles based of weights target_ess: float     The target ESS for the adaptive MCMC tempering root_solver: Callable, optional     A solver utility to find delta matching the target ESS. Signature is     <code>root_solver(fun, delta_0, min_delta, max_delta)</code>, default is a dichotomy solver use_log_ess: bool, optional     Use ESS in log space to solve for delta, default is <code>True</code>.     This is usually more stable when using gradient based solvers.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_adaptive_tempered.build_kernel--returns","title":"Returns","text":"<p>A callable that takes a rng_key and a TemperedSMCState that contains the current state of the chain and that returns a new state of the chain along with information about the transition.</p> Source code in <code>bamojax/modified_blackjax/modified_adaptive_tempered.py</code> <pre><code>def build_kernel(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    resampling_fn: Callable,\n    target_ess: float,\n    root_solver: Callable = solver.dichotomy,\n    **extra_parameters,\n) -&gt; Callable:\n    r\"\"\"Build a Tempered SMC step using an adaptive schedule.\n\n    Parameters\n    ----------\n    logprior_fn: Callable\n        A function that computes the log-prior density.\n    loglikelihood_fn: Callable\n        A function that returns the log-likelihood density.\n    mcmc_kernel_factory: Callable\n        A callable function that creates a mcmc kernel from a log-probability\n        density function.\n    make_mcmc_state: Callable\n        A function that creates a new mcmc state from a position and a\n        log-probability density function.\n    resampling_fn: Callable\n        A random function that resamples generated particles based of weights\n    target_ess: float\n        The target ESS for the adaptive MCMC tempering\n    root_solver: Callable, optional\n        A solver utility to find delta matching the target ESS. Signature is\n        `root_solver(fun, delta_0, min_delta, max_delta)`, default is a dichotomy solver\n    use_log_ess: bool, optional\n        Use ESS in log space to solve for delta, default is `True`.\n        This is usually more stable when using gradient based solvers.\n\n    Returns\n    -------\n    A callable that takes a rng_key and a TemperedSMCState that contains the current state\n    of the chain and that returns a new state of the chain along with\n    information about the transition.\n\n    \"\"\"\n\n    def compute_delta(state: tempered.TemperedSMCState) -&gt; float:\n        lmbda = state.lmbda\n        max_delta = 1 - lmbda\n        delta = ess.ess_solver(\n            jax.vmap(loglikelihood_fn),\n            state.particles,\n            target_ess,\n            max_delta,\n            root_solver,\n        )\n        delta = jnp.clip(delta, 0.0, max_delta)\n\n        return delta\n\n    tempered_kernel = tempered.build_kernel(\n        logprior_fn,\n        loglikelihood_fn,\n        mcmc_step_fn,\n        mcmc_init_fn,\n        resampling_fn,\n        **extra_parameters,\n    )\n\n    def kernel(\n        rng_key: PRNGKey,\n        state: tempered.TemperedSMCState,\n        num_mcmc_steps: int,\n        mcmc_parameters: dict,\n    ) -&gt; tuple[tempered.TemperedSMCState, base.SMCInfo]:\n        delta = compute_delta(state)\n        lmbda = delta + state.lmbda\n        return tempered_kernel(rng_key, state, num_mcmc_steps, lmbda, mcmc_parameters)\n\n    return kernel\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_adaptive_tempered.as_top_level_api","title":"<code>bamojax.modified_blackjax.modified_adaptive_tempered.as_top_level_api(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, mcmc_parameters, resampling_fn, target_ess, root_solver=solver.dichotomy, num_mcmc_steps=10, **extra_parameters)</code>","text":"<p>Implements the (basic) user interface for the Adaptive Tempered SMC kernel.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_adaptive_tempered.as_top_level_api--parameters","title":"Parameters","text":"<p>logprior_fn     The log-prior function of the model we wish to draw samples from. loglikelihood_fn     The log-likelihood function of the model we wish to draw samples from. mcmc_step_fn     The MCMC step function used to update the particles. mcmc_init_fn     The MCMC init function used to build a MCMC state from a particle position. mcmc_parameters     The parameters of the MCMC step function.  Parameters with leading dimension     length of 1 are shared amongst the particles. resampling_fn     The function used to resample the particles. target_ess     The number of effective sample size to aim for at each step. root_solver     The solver used to adaptively compute the temperature given a target number     of effective samples. num_mcmc_steps     The number of times the MCMC kernel is applied to the particles per step.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_adaptive_tempered.as_top_level_api--returns","title":"Returns","text":"<p>A <code>SamplingAlgorithm</code>.</p> Source code in <code>bamojax/modified_blackjax/modified_adaptive_tempered.py</code> <pre><code>def as_top_level_api(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    mcmc_parameters: dict,\n    resampling_fn: Callable,\n    target_ess: float,\n    root_solver: Callable = solver.dichotomy,\n    num_mcmc_steps: int = 10,\n    **extra_parameters,\n) -&gt; SamplingAlgorithm:\n    \"\"\"Implements the (basic) user interface for the Adaptive Tempered SMC kernel.\n\n    Parameters\n    ----------\n    logprior_fn\n        The log-prior function of the model we wish to draw samples from.\n    loglikelihood_fn\n        The log-likelihood function of the model we wish to draw samples from.\n    mcmc_step_fn\n        The MCMC step function used to update the particles.\n    mcmc_init_fn\n        The MCMC init function used to build a MCMC state from a particle position.\n    mcmc_parameters\n        The parameters of the MCMC step function.  Parameters with leading dimension\n        length of 1 are shared amongst the particles.\n    resampling_fn\n        The function used to resample the particles.\n    target_ess\n        The number of effective sample size to aim for at each step.\n    root_solver\n        The solver used to adaptively compute the temperature given a target number\n        of effective samples.\n    num_mcmc_steps\n        The number of times the MCMC kernel is applied to the particles per step.\n\n    Returns\n    -------\n    A ``SamplingAlgorithm``.\n\n    \"\"\"\n    kernel = build_kernel(\n        logprior_fn,\n        loglikelihood_fn,\n        mcmc_step_fn,\n        mcmc_init_fn,\n        resampling_fn,\n        target_ess,\n        root_solver,\n        **extra_parameters,\n    )\n\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return init(position)\n\n    def step_fn(rng_key: PRNGKey, state):\n        return kernel(\n            rng_key,\n            state,\n            num_mcmc_steps,\n            mcmc_parameters,\n        )\n\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd","title":"<code>bamojax.modified_blackjax.modified_elliptical_slice_nd</code>","text":"<p>Public API for the Elliptical Slice sampling Kernel</p>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.EllipSliceState","title":"<code>bamojax.modified_blackjax.modified_elliptical_slice_nd.EllipSliceState</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>State of the Elliptical Slice sampling algorithm.</p> <p>position     Current position of the chain. logdensity     Current value of the logdensity (evaluated at current position).</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>class EllipSliceState(NamedTuple):\n    \"\"\"State of the Elliptical Slice sampling algorithm.\n\n    position\n        Current position of the chain.\n    logdensity\n        Current value of the logdensity (evaluated at current position).\n\n    \"\"\"\n\n    position: ArrayTree\n    logdensity: ArrayTree\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.EllipSliceInfo","title":"<code>bamojax.modified_blackjax.modified_elliptical_slice_nd.EllipSliceInfo</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Additional information on the Elliptical Slice sampling chain.</p> <p>This additional information can be used for debugging or computing diagnostics.</p> <p>momentum     The latent momentum variable returned at the end of the transition. theta     A value between [-2\\pi, 2\\pi] identifying points in the ellipsis drawn     from the positon and mommentum variables. This value indicates the theta     value of the accepted proposal. subiter     Number of sub iterations needed to accept a proposal. The more subiterations     needed the less efficient the algorithm will be, and the more dependent the     new value is likely to be to the previous value.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>class EllipSliceInfo(NamedTuple):\n    r\"\"\"Additional information on the Elliptical Slice sampling chain.\n\n    This additional information can be used for debugging or computing\n    diagnostics.\n\n    momentum\n        The latent momentum variable returned at the end of the transition.\n    theta\n        A value between [-2\\pi, 2\\pi] identifying points in the ellipsis drawn\n        from the positon and mommentum variables. This value indicates the theta\n        value of the accepted proposal.\n    subiter\n        Number of sub iterations needed to accept a proposal. The more subiterations\n        needed the less efficient the algorithm will be, and the more dependent the\n        new value is likely to be to the previous value.\n\n    \"\"\"\n\n    momentum: ArrayTree\n    theta: float\n    subiter: int\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.build_kernel","title":"<code>bamojax.modified_blackjax.modified_elliptical_slice_nd.build_kernel(cov_matrix, mean, nd=None)</code>","text":"<p>Build an Elliptical Slice sampling kernel :cite:p:<code>murray2010elliptical</code>.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.build_kernel--parameters","title":"Parameters","text":"<p>cov_matrix     The value of the covariance matrix of the gaussian prior distribution from     the posterior we wish to sample.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.build_kernel--returns","title":"Returns","text":"<p>A kernel that takes a rng_key and a Pytree that contains the current state of the chain and that returns a new state of the chain along with information about the transition.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>def build_kernel(cov_matrix: Array, mean: Array, nd: tuple = None):\n    \"\"\"Build an Elliptical Slice sampling kernel :cite:p:`murray2010elliptical`.\n\n    Parameters\n    ----------\n    cov_matrix\n        The value of the covariance matrix of the gaussian prior distribution from\n        the posterior we wish to sample.\n\n    Returns\n    -------\n    A kernel that takes a rng_key and a Pytree that contains the current state\n    of the chain and that returns a new state of the chain along with\n    information about the transition.\n\n    \"\"\"\n    ndim = jnp.ndim(cov_matrix)  # type: ignore[arg-type]\n    n = cov_matrix.shape[0]\n\n    if len(nd) == 2:\n        d, nu = nd        \n        flat_shape = (d*nu*n, )\n    elif len(nd) == 1:\n        d = 1\n        nu = nd[0]\n        flat_shape = (nu*n, )\n    else:\n        raise NotImplementedError(f'Elliptical slice sampling is not implemented for nd = {nd}')\n\n    if ndim == 1:  # diagonal covariance matrix\n        cov_matrix_sqrt = jnp.sqrt(cov_matrix)\n\n    elif ndim == 2:\n        cov_matrix_sqrt = jax.lax.linalg.cholesky(cov_matrix)\n\n    else:\n        raise ValueError(\n            \"The mass matrix has the wrong number of dimensions:\"\n            f\" expected 1 or 2, got {jnp.ndim(cov_matrix)}.\"  # type: ignore[arg-type]\n        )\n\n    def to_flat(u):\n        return jax.tree_util.tree_map(lambda l: jnp.reshape(l, shape=flat_shape), u)\n\n    def to_nd(u):\n        return jax.tree_util.tree_map(lambda l: jnp.reshape(l, shape=nd + (n, )), u)\n\n    def momentum_generator(rng_key, position):\n        generate_noise_fn = lambda k, p: generate_gaussian_noise(k, p, mean, cov_matrix_sqrt)\n        # [MODIFICATIONS] vmap the noise generation over the dimensions nd, then reshape into flattened array\n\n        u = to_nd(position)\n\n        if d==1 and nu==1:\n            z = generate_noise_fn(rng_key, position)\n        elif d&gt;1 and nu==1:\n            keys_d = jax.random.split(rng_key, d)\n            z = jax.vmap(generate_noise_fn, in_axes=(0, 0))(keys_d, u)\n        elif d==1 and nu &gt; 1:\n            keys_nu = jax.random.split(rng_key, nu)\n            z = jax.vmap(generate_noise_fn, in_axes=(0, 0))(keys_nu, u)\n        elif d&gt;1 and nu&gt;1:\n            keys_d = jax.random.split(rng_key, d)\n            keys_dnu = jax.vmap(lambda k: jax.random.split(k, nu))(keys_d)\n            z = jax.vmap(jax.vmap(generate_noise_fn, in_axes=(0, 0)), in_axes=(0, 0))(keys_dnu, u)\n        else:\n            raise NotImplementedError\n        z_reshaped = to_flat(z)\n        return z_reshaped\n\n    def elliptical_proposal(\n        logdensity_fn: Callable,\n        momentum_generator: Callable,\n        mean: Array,\n    ) -&gt; Callable:\n        \"\"\"Build an Ellitpical slice sampling kernel.\n\n        The algorithm samples a latent parameter, traces an ellipse connecting the\n        initial position and the latent parameter and does slice sampling on this\n        ellipse to output a new sample from the posterior distribution.\n\n        Parameters\n        ----------\n        logdensity_fn\n            A function that returns the log-likelihood at a given position.\n        momentum_generator\n            A function that generates a new latent momentum variable.\n\n        Returns\n        -------\n        A kernel that takes a rng_key and a Pytree that contains the current state\n        of the chain and that returns a new state of the chain along with\n        information about the transition.\n\n        \"\"\"\n        num_el = d*nu\n        mean = jnp.tile(mean, reps=num_el)\n\n        def generate(\n            rng_key: PRNGKey, state: EllipSliceState\n        ) -&gt; tuple[EllipSliceState, EllipSliceInfo]:\n            position, logdensity = state\n            position = to_flat(position)\n            key_slice, key_momentum, key_uniform, key_theta = jax.random.split(rng_key, 4)\n            # step 1: sample momentum\n            momentum = momentum_generator(key_momentum, position) \n            # step 2: get slice (y)\n            logy = logdensity + jnp.log(jax.random.uniform(key_uniform))\n            # step 3: get theta (ellipsis move), set inital interval\n            theta = 2 * jnp.pi * jax.random.uniform(key_theta)\n            theta_min = theta - 2 * jnp.pi\n            theta_max = theta\n            # step 4: proposal\n            p, m = ellipsis(position, momentum, theta, mean)\n            # step 5: acceptance\n            logdensity = logdensity_fn(p)\n\n            def slice_fn(vals):\n                \"\"\"Perform slice sampling around the ellipsis.\n\n                Checks if the proposed position's likelihood is larger than the slice\n                variable. Returns the position if True, shrinks the bracket for sampling\n                `theta` and samples a new proposal if False.\n\n                As the bracket `[theta_min, theta_max]` shrinks, the proposal gets closer\n                to the original position, which has likelihood larger than the slice variable.\n                It is guaranteed to stop in a finite number of iterations as long as the\n                likelihood is continuous with respect to the parameter being sampled.\n\n                \"\"\"\n                _, subiter, theta, theta_min, theta_max, *_ = vals\n                thetak = jax.random.fold_in(key_slice, subiter)\n                theta = jax.random.uniform(thetak, minval=theta_min, maxval=theta_max)\n                p, m = ellipsis(position, momentum, theta, mean)\n                logdensity = logdensity_fn(p)\n                theta_min = jnp.where(theta &lt; 0, theta, theta_min)\n                theta_max = jnp.where(theta &gt; 0, theta, theta_max)\n                subiter += 1\n                return logdensity, subiter, theta, theta_min, theta_max, p, m\n\n            logdensity, subiter, theta, *_, position, momentum = jax.lax.while_loop(\n                lambda vals: vals[0] &lt;= logy,\n                slice_fn,\n                (logdensity, 1, theta, theta_min, theta_max, p, m),\n            )\n            position = to_nd(position)\n            return (\n                EllipSliceState(position, logdensity),\n                EllipSliceInfo(momentum, theta, subiter),\n            )\n\n        return generate\n\n\n    def kernel(\n        rng_key: PRNGKey,\n        state: EllipSliceState,\n        logdensity_fn: Callable,\n    ) -&gt; tuple[EllipSliceState, EllipSliceInfo]:\n        proposal_generator = elliptical_proposal(\n            logdensity_fn, momentum_generator, mean, \n        )       \n        return proposal_generator(rng_key, state)\n\n    return kernel\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api","title":"<code>bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api(loglikelihood_fn, *, mean, cov, nd)</code>","text":"<p>Implements the (basic) user interface for the Elliptical Slice sampling kernel.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api--examples","title":"Examples","text":"<p>A new Elliptical Slice sampling kernel can be initialized and used with the following code:</p> <p>.. code::</p> <pre><code>ellip_slice = blackjax.elliptical_slice(loglikelihood_fn, cov_matrix)\nstate = ellip_slice.init(position)\nnew_state, info = ellip_slice.step(rng_key, state)\n</code></pre> <p>We can JIT-compile the step function for better performance</p> <p>.. code::</p> <pre><code>step = jax.jit(ellip_slice.step)\nnew_state, info = step(rng_key, state)\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api--parameters","title":"Parameters","text":"<p>loglikelihood_fn     Only the log likelihood function from the posterior distributon we wish to sample. cov_matrix     The value of the covariance matrix of the gaussian prior distribution from the posterior we wish to sample.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api--returns","title":"Returns","text":"<p>A <code>SamplingAlgorithm</code>.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>def as_top_level_api(\n    loglikelihood_fn: Callable,\n    *,\n    mean: Array,\n    cov: Array,\n    nd: tuple,\n) -&gt; SamplingAlgorithm:\n    \"\"\"Implements the (basic) user interface for the Elliptical Slice sampling kernel.\n\n    Examples\n    --------\n\n    A new Elliptical Slice sampling kernel can be initialized and used with the following code:\n\n    .. code::\n\n        ellip_slice = blackjax.elliptical_slice(loglikelihood_fn, cov_matrix)\n        state = ellip_slice.init(position)\n        new_state, info = ellip_slice.step(rng_key, state)\n\n    We can JIT-compile the step function for better performance\n\n    .. code::\n\n        step = jax.jit(ellip_slice.step)\n        new_state, info = step(rng_key, state)\n\n    Parameters\n    ----------\n    loglikelihood_fn\n        Only the log likelihood function from the posterior distributon we wish to sample.\n    cov_matrix\n        The value of the covariance matrix of the gaussian prior distribution from the posterior we wish to sample.\n\n    Returns\n    -------\n    A ``SamplingAlgorithm``.\n    \"\"\"\n    kernel = build_kernel(cov, mean, nd)\n\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return init(position, loglikelihood_fn)\n\n    def step_fn(rng_key: PRNGKey, state):\n        return kernel(\n            rng_key,\n            state,\n            loglikelihood_fn,\n        )\n\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_elliptical_slice_nd.ellipsis","title":"<code>bamojax.modified_blackjax.modified_elliptical_slice_nd.ellipsis(position, momentum, theta, mean)</code>","text":"<p>Generate proposal from the ellipsis.</p> <p>Given a scalar theta indicating a point on the circumference of the ellipsis and the shared mean vector for both position and momentum variables, generate proposed position and momentum to later accept or reject depending on the slice variable.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>def ellipsis(position, momentum, theta, mean):\n    \"\"\"Generate proposal from the ellipsis.\n\n    Given a scalar theta indicating a point on the circumference of the ellipsis\n    and the shared mean vector for both position and momentum variables,\n    generate proposed position and momentum to later accept or reject\n    depending on the slice variable.\n\n    \"\"\"\n    position, unravel_fn = jax.flatten_util.ravel_pytree(position)\n    momentum, _ = jax.flatten_util.ravel_pytree(momentum)\n    position_centered = position - mean\n    momentum_centered = momentum - mean\n    return (\n        unravel_fn(\n            position_centered * jnp.cos(theta)\n            + momentum_centered * jnp.sin(theta)\n            + mean\n        ),\n        unravel_fn(\n            momentum_centered * jnp.cos(theta)\n            - position_centered * jnp.sin(theta)\n            + mean\n        ),\n    )\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered","title":"<code>bamojax.modified_blackjax.modified_tempered</code>","text":""},{"location":"api/#bamojax.modified_blackjax.modified_tempered.TemperedSMCState","title":"<code>bamojax.modified_blackjax.modified_tempered.TemperedSMCState</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Current state for the tempered SMC algorithm.</p> PyTree <p>The particles' positions.</p> <p>lmbda: float     Current value of the tempering parameter.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>class TemperedSMCState(NamedTuple):\n    \"\"\"Current state for the tempered SMC algorithm.\n\n    particles: PyTree\n        The particles' positions.\n    lmbda: float\n        Current value of the tempering parameter.\n\n    \"\"\"\n\n    particles: ArrayTree\n    weights: Array\n    lmbda: float\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered.update_and_take_last","title":"<code>bamojax.modified_blackjax.modified_tempered.update_and_take_last(mcmc_init_fn, tempered_logposterior_fn, shared_mcmc_step_fn, num_mcmc_steps, n_particles)</code>","text":"<p>Given N particles, runs num_mcmc_steps of a kernel starting at each particle, and returns the last values, waisting the previous num_mcmc_steps-1 samples per chain.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>def update_and_take_last(\n    mcmc_init_fn,\n    tempered_logposterior_fn,\n    shared_mcmc_step_fn,\n    num_mcmc_steps,\n    n_particles,\n):\n    \"\"\"\n    Given N particles, runs num_mcmc_steps of a kernel starting at each particle, and\n    returns the last values, waisting the previous num_mcmc_steps-1\n    samples per chain.\n    \"\"\"\n\n    def mcmc_kernel(rng_key, position, step_parameters):\n        state = mcmc_init_fn(position, tempered_logposterior_fn)\n\n        def body_fn(state, rng_key):\n            new_state, info = shared_mcmc_step_fn(\n                rng_key, state, tempered_logposterior_fn, **step_parameters\n            )\n            return new_state, info\n\n        keys = jax.random.split(rng_key, num_mcmc_steps)\n        last_state, info = jax.lax.scan(body_fn, state, keys)\n        return last_state.position, info\n\n    return jax.vmap(mcmc_kernel), n_particles\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered.build_kernel","title":"<code>bamojax.modified_blackjax.modified_tempered.build_kernel(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, resampling_fn, update_strategy=update_and_take_last)</code>","text":"<p>Build the base Tempered SMC kernel.</p> <p>Tempered SMC uses tempering to sample from a distribution given by</p> <p>.. math::     p(x) \\propto p_0(x) \\exp(-V(x)) \\mathrm{d}x</p> <p>where :math:<code>p_0</code> is the prior distribution, typically easy to sample from and for which the density is easy to compute, and :math:<code>\\exp(-V(x))</code> is an unnormalized likelihood term for which :math:<code>V(x)</code> is easy to compute pointwise.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered.build_kernel--parameters","title":"Parameters","text":"<p>logprior_fn     A function that computes the log density of the prior distribution loglikelihood_fn     A function that returns the probability at a given     position. mcmc_step_fn     A function that creates a mcmc kernel from a log-probability density function. mcmc_init_fn: Callable     A function that creates a new mcmc state from a position and a     log-probability density function. resampling_fn     A random function that resamples generated particles based of weights num_mcmc_iterations     Number of iterations in the MCMC chain.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered.build_kernel--returns","title":"Returns","text":"<p>A callable that takes a rng_key and a TemperedSMCState that contains the current state of the chain and that returns a new state of the chain along with information about the transition.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>def build_kernel(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    resampling_fn: Callable,\n    update_strategy: Callable = update_and_take_last,\n) -&gt; Callable:\n    \"\"\"Build the base Tempered SMC kernel.\n\n    Tempered SMC uses tempering to sample from a distribution given by\n\n    .. math::\n        p(x) \\\\propto p_0(x) \\\\exp(-V(x)) \\\\mathrm{d}x\n\n    where :math:`p_0` is the prior distribution, typically easy to sample from\n    and for which the density is easy to compute, and :math:`\\\\exp(-V(x))` is an\n    unnormalized likelihood term for which :math:`V(x)` is easy to compute\n    pointwise.\n\n    Parameters\n    ----------\n    logprior_fn\n        A function that computes the log density of the prior distribution\n    loglikelihood_fn\n        A function that returns the probability at a given\n        position.\n    mcmc_step_fn\n        A function that creates a mcmc kernel from a log-probability density function.\n    mcmc_init_fn: Callable\n        A function that creates a new mcmc state from a position and a\n        log-probability density function.\n    resampling_fn\n        A random function that resamples generated particles based of weights\n    num_mcmc_iterations\n        Number of iterations in the MCMC chain.\n\n    Returns\n    -------\n    A callable that takes a rng_key and a TemperedSMCState that contains the current state\n    of the chain and that returns a new state of the chain along with\n    information about the transition.\n\n    \"\"\"\n\n    def kernel(\n        rng_key: PRNGKey,\n        state: TemperedSMCState,\n        num_mcmc_steps: int,\n        lmbda: float,\n        mcmc_parameters: dict,\n    ) -&gt; tuple[TemperedSMCState, smc.base.SMCInfo]:\n        \"\"\"Move the particles one step using the Tempered SMC algorithm.\n\n        Parameters\n        ----------\n        rng_key\n            JAX PRNGKey for randomness\n        state\n            Current state of the tempered SMC algorithm\n        lmbda\n            Current value of the tempering parameter\n        mcmc_parameters\n            The parameters of the MCMC step function.  Parameters with leading dimension\n            length of 1 are shared amongst the particles.\n\n        Returns\n        -------\n        state\n            The new state of the tempered SMC algorithm\n        info\n            Additional information on the SMC step\n\n        \"\"\"\n        delta = lmbda - state.lmbda\n\n        # [MODIFICATION]\n        mcmc_parameters['temperature'] = state.lmbda*jnp.eye(1)\n        # [MODIFICATION]\n\n        shared_mcmc_parameters = {}\n        unshared_mcmc_parameters = {}\n        for k, v in mcmc_parameters.items():\n            if v.shape[0] == 1:\n                shared_mcmc_parameters[k] = v[0, ...]\n            else:\n                unshared_mcmc_parameters[k] = v\n\n        def log_weights_fn(position: ArrayLikeTree) -&gt; float:\n            return delta * loglikelihood_fn(position)\n\n        def tempered_logposterior_fn(position: ArrayLikeTree) -&gt; float:\n            logprior = logprior_fn(position)\n            tempered_loglikelihood = state.lmbda * loglikelihood_fn(position)\n            return logprior + tempered_loglikelihood\n\n        shared_mcmc_step_fn = partial(mcmc_step_fn, **shared_mcmc_parameters)\n\n        update_fn, num_resampled = update_strategy(\n            mcmc_init_fn,\n            tempered_logposterior_fn,\n            shared_mcmc_step_fn,\n            n_particles=state.weights.shape[0],\n            num_mcmc_steps=num_mcmc_steps,\n        )\n\n        smc_state, info = smc.base.step(\n            rng_key,\n            SMCState(state.particles, state.weights, unshared_mcmc_parameters),\n            update_fn,\n            jax.vmap(log_weights_fn),\n            resampling_fn,\n            num_resampled,\n        )\n\n        tempered_state = TemperedSMCState(\n            smc_state.particles, smc_state.weights, state.lmbda + delta\n        )\n\n        return tempered_state, info\n\n    return kernel\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered.as_top_level_api","title":"<code>bamojax.modified_blackjax.modified_tempered.as_top_level_api(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, mcmc_parameters, resampling_fn, num_mcmc_steps=10, update_strategy=update_and_take_last)</code>","text":"<p>Implements the (basic) user interface for the Adaptive Tempered SMC kernel.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered.as_top_level_api--parameters","title":"Parameters","text":"<p>logprior_fn     The log-prior function of the model we wish to draw samples from. loglikelihood_fn     The log-likelihood function of the model we wish to draw samples from. mcmc_step_fn     The MCMC step function used to update the particles. mcmc_init_fn     The MCMC init function used to build a MCMC state from a particle position. mcmc_parameters     The parameters of the MCMC step function.  Parameters with leading dimension     length of 1 are shared amongst the particles. resampling_fn     The function used to resample the particles. num_mcmc_steps     The number of times the MCMC kernel is applied to the particles per step.</p>"},{"location":"api/#bamojax.modified_blackjax.modified_tempered.as_top_level_api--returns","title":"Returns","text":"<p>A <code>SamplingAlgorithm</code>.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>def as_top_level_api(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    mcmc_parameters: dict,\n    resampling_fn: Callable,\n    num_mcmc_steps: Optional[int] = 10,\n    update_strategy=update_and_take_last,\n) -&gt; SamplingAlgorithm:\n    \"\"\"Implements the (basic) user interface for the Adaptive Tempered SMC kernel.\n\n    Parameters\n    ----------\n    logprior_fn\n        The log-prior function of the model we wish to draw samples from.\n    loglikelihood_fn\n        The log-likelihood function of the model we wish to draw samples from.\n    mcmc_step_fn\n        The MCMC step function used to update the particles.\n    mcmc_init_fn\n        The MCMC init function used to build a MCMC state from a particle position.\n    mcmc_parameters\n        The parameters of the MCMC step function.  Parameters with leading dimension\n        length of 1 are shared amongst the particles.\n    resampling_fn\n        The function used to resample the particles.\n    num_mcmc_steps\n        The number of times the MCMC kernel is applied to the particles per step.\n\n    Returns\n    -------\n    A ``SamplingAlgorithm``.\n\n    \"\"\"\n\n    kernel = build_kernel(\n        logprior_fn,\n        loglikelihood_fn,\n        mcmc_step_fn,\n        mcmc_init_fn,\n        resampling_fn,\n        update_strategy,\n    )\n\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return init(position)\n\n    def step_fn(rng_key: PRNGKey, state, lmbda):\n        return kernel(\n            rng_key,\n            state,\n            num_mcmc_steps,\n            lmbda,\n            mcmc_parameters,\n        )\n\n    return SamplingAlgorithm(init_fn, step_fn)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.more_proposals","title":"<code>bamojax.modified_blackjax.more_proposals</code>","text":""},{"location":"api/#bamojax.modified_blackjax.more_proposals.generate_bernoulli_noise","title":"<code>bamojax.modified_blackjax.more_proposals.generate_bernoulli_noise(rng_key, position, theta)</code>","text":"<p>Given a position (pytree) and a probability theta, generate a new position by flipping each bit with probability theta.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def generate_bernoulli_noise(rng_key: PRNGKey, position, theta):\n    r\"\"\" Given a position (pytree) and a probability theta, generate a new position by flipping each bit with probability theta.\n    \"\"\"\n    p, unravel_fn = jax.flatten_util.ravel_pytree(position)\n    sample = jrnd.bernoulli(rng_key, shape=p.shape, p=theta)\n    return unravel_fn(sample)\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.more_proposals.bernoulli","title":"<code>bamojax.modified_blackjax.more_proposals.bernoulli(theta)</code>","text":"<p>Create a proposal function that flips each Bernoulli random variable of the input position with probability theta.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def bernoulli(theta: Array) -&gt; Callable:\n    r\"\"\" Create a proposal function that flips each Bernoulli random variable of the input position with probability theta.\n    \"\"\"\n    def propose(rng_key: PRNGKey, position) -&gt; ArrayTree:\n        return generate_bernoulli_noise(rng_key, position, theta=theta)\n\n    #\n    return propose\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.more_proposals.build_xor_step","title":"<code>bamojax.modified_blackjax.more_proposals.build_xor_step()</code>","text":"<p>Build a kernel that uses the xor operation to flip bits in a binary vector.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def build_xor_step():\n    r\"\"\" Build a kernel that uses the xor operation to flip bits in a binary vector.\n    \"\"\"\n    def kernel(\n        rng_key: PRNGKey, state, logdensity_fn: Callable, random_step: Callable\n    ):\n        def proposal_generator(key_proposal, position):\n            move_proposal = jax.tree_util.tree_map(lambda x: x.astype(int), random_step(key_proposal, position)) \n            new_position = jax.tree_util.tree_map(jnp.bitwise_xor, position, move_proposal)\n            return new_position\n\n        inner_kernel = blackjax.mcmc.random_walk.build_rmh()\n        return inner_kernel(rng_key, state, logdensity_fn, proposal_generator)\n\n    return kernel\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.more_proposals.xor_step_random_walk","title":"<code>bamojax.modified_blackjax.more_proposals.xor_step_random_walk(logdensity_fn, random_step)</code>","text":"<p>Create a random walk MCMC algorithm that uses the xor operation to flip bits in a binary vector.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def xor_step_random_walk(logdensity_fn: Callable, random_step: Callable) -&gt; SamplingAlgorithm:\n    r\"\"\" Create a random walk MCMC algorithm that uses the xor operation to flip bits in a binary vector.\n    \"\"\"\n\n    kernel = build_xor_step()\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return blackjax.mcmc.random_walk.init(position, logdensity_fn)\n\n    def step_fn(rng_key: PRNGKey, state):\n        return kernel(rng_key, state, logdensity_fn, random_step)\n\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/#bamojax.modified_blackjax.more_proposals.bernoulli_random_walk","title":"<code>bamojax.modified_blackjax.more_proposals.bernoulli_random_walk(logdensity_fn, theta)</code>","text":"<p>Create a random walk MCMC algorithm that moves across the space of binary vectors.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def bernoulli_random_walk(logdensity_fn: Callable, theta):\n    r\"\"\" Create a random walk MCMC algorithm that moves across the space of binary vectors.\n    \"\"\"\n    return xor_step_random_walk(logdensity_fn, bernoulli(theta))\n</code></pre>"},{"location":"api/#bamojax.more_distributions","title":"<code>bamojax.more_distributions</code>","text":""},{"location":"api/#bamojax.more_distributions.Wishart","title":"<code>bamojax.more_distributions.Wishart</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>Wishart distribution with parameters <code>dof</code> and <code>scale</code>.</p> <p>TODO: make batchable</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>class Wishart(Distribution):\n    \"\"\" Wishart distribution with parameters `dof` and `scale`.\n\n    TODO: make batchable\n\n    \"\"\"\n\n\n    def __init__(self, dof: int, scale: Optional[Array]):\n        \"\"\" Initializes a Wishart distribution.\n\n        Args:\n          dof: degrees of freedom\n          scale: scale matrix        \n        \"\"\"\n        super().__init__()\n        p = scale.shape[0]\n        assert dof &gt; p - 1, f'DoF must be &gt; p - 1, found DoF = {dof}, and p = {p}.'\n        self._dof = dof\n        self._scale = scale\n        self._p = p\n\n    #\n    @property\n    def event_shape(self) -&gt; Tuple[int, ...]:\n        \"\"\" Shape of event of distribution samples.\n\n        \"\"\"\n        return (self._p, self._p)\n\n    #\n    @property\n    def batch_shape(self) -&gt; Tuple[int, ...]:\n        \"\"\" Shape of batch of distribution samples.\n\n        \"\"\"\n        return jax.lax.broadcast_shapes(self._dof.shape, self._scale.shape)\n\n    #\n\n    def _sample_n(self, key: Array, n: int) -&gt; Array:\n        \"\"\" See `Distribution._sample_n`.\n\n        \"\"\"\n        X = jrnd.multivariate_normal(key, mean=jnp.zeros((self._p, )), cov=self._scale, shape=(n, self._dof))\n        wishart_matrices = jnp.einsum('ndp,ndq-&gt;npq', X, X)\n        return wishart_matrices\n\n    #\n    def log_prob(self, value: Array) -&gt; Array:\n        \"\"\" Computes the log probability of the Wishart distribution.\n\n        \"\"\"\n        _, logdetV = jnp.linalg.slogdet(self._scale)\n        _, logdetK = jnp.linalg.slogdet(value)\n\n        logZ = 0.5*self._dof*self._p*jnp.log(2) + 0.5*self._dof*logdetV + multigammaln(0.5*self._dof, self._p)\n        return 0.5*(self._dof - self._p - 1)*logdetK - 0.5*jnp.sum(jnp.diag(jnp.linalg.solve(self._scale, value))) - logZ\n\n    #\n    def mean(self) -&gt; Array:\n        \"\"\"Calculates the mean.\"\"\"\n\n        return self._dof*self._scale\n\n    #\n    def mode(self) -&gt; Array:\n        \"\"\"Calculates the mode.\"\"\"\n\n        assert self._dof &gt; self._p + 2, f'The mode is only defined for DoF &gt; p + 2, found DoF = {self._dof} and p = {self._p}.'\n        return (self._dof - self._p - 1)*self._scale\n\n    #\n    def variance(self) -&gt; Array:\n        \"\"\"Calculates the variance.\"\"\"\n\n        V_ij_squared = jnp.square(self._scale)  \n        V_ii = jnp.diag(self._scale)  \n        V_ii_V_jj = jnp.outer(V_ii, V_ii) \n        return self._dof * (V_ij_squared + V_ii_V_jj)\n</code></pre>"},{"location":"api/#bamojax.more_distributions.Wishart.event_shape","title":"<code>bamojax.more_distributions.Wishart.event_shape</code>  <code>property</code>","text":"<p>Shape of event of distribution samples.</p>"},{"location":"api/#bamojax.more_distributions.Wishart.batch_shape","title":"<code>bamojax.more_distributions.Wishart.batch_shape</code>  <code>property</code>","text":"<p>Shape of batch of distribution samples.</p>"},{"location":"api/#bamojax.more_distributions.Wishart.log_prob","title":"<code>bamojax.more_distributions.Wishart.log_prob(value)</code>","text":"<p>Computes the log probability of the Wishart distribution.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def log_prob(self, value: Array) -&gt; Array:\n    \"\"\" Computes the log probability of the Wishart distribution.\n\n    \"\"\"\n    _, logdetV = jnp.linalg.slogdet(self._scale)\n    _, logdetK = jnp.linalg.slogdet(value)\n\n    logZ = 0.5*self._dof*self._p*jnp.log(2) + 0.5*self._dof*logdetV + multigammaln(0.5*self._dof, self._p)\n    return 0.5*(self._dof - self._p - 1)*logdetK - 0.5*jnp.sum(jnp.diag(jnp.linalg.solve(self._scale, value))) - logZ\n</code></pre>"},{"location":"api/#bamojax.more_distributions.Wishart.mean","title":"<code>bamojax.more_distributions.Wishart.mean()</code>","text":"<p>Calculates the mean.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def mean(self) -&gt; Array:\n    \"\"\"Calculates the mean.\"\"\"\n\n    return self._dof*self._scale\n</code></pre>"},{"location":"api/#bamojax.more_distributions.Wishart.mode","title":"<code>bamojax.more_distributions.Wishart.mode()</code>","text":"<p>Calculates the mode.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def mode(self) -&gt; Array:\n    \"\"\"Calculates the mode.\"\"\"\n\n    assert self._dof &gt; self._p + 2, f'The mode is only defined for DoF &gt; p + 2, found DoF = {self._dof} and p = {self._p}.'\n    return (self._dof - self._p - 1)*self._scale\n</code></pre>"},{"location":"api/#bamojax.more_distributions.Wishart.variance","title":"<code>bamojax.more_distributions.Wishart.variance()</code>","text":"<p>Calculates the variance.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def variance(self) -&gt; Array:\n    \"\"\"Calculates the variance.\"\"\"\n\n    V_ij_squared = jnp.square(self._scale)  \n    V_ii = jnp.diag(self._scale)  \n    V_ii_V_jj = jnp.outer(V_ii, V_ii) \n    return self._dof * (V_ij_squared + V_ii_V_jj)\n</code></pre>"},{"location":"api/#bamojax.more_distributions.GaussianProcessFactory","title":"<code>bamojax.more_distributions.GaussianProcessFactory(cov_fn, mean_fn=Zero(), nd=None, jitter=1e-06)</code>","text":"<p>Returns an instantiated Gaussian process distribution object. </p> <p>This is essentially a dx.MultivariateNormalFullCovariance object, with its mean and covariance determined by the mean and covariance functions of the GP.</p> <p>Parameters:</p> Name Type Description Default <code>cov_fn</code> <code>Callable</code> <p>The GP covariance function. It assumes a signature of cov_fn(parameters: dict, x: Array, y: Array).      This is provided by the <code>jaxkern</code> library, but others can be used as well.</p> required <code>mean_fn</code> <code>Callable</code> <p>The GP mean function.</p> <code>Zero()</code> <code>nd</code> <code>Tuple[int, ...]</code> <p>A tuple of integers indicating optional additional output dimensions (for multi-task GPs).</p> <code>None</code> <code>jitter</code> <code>float</code> <p>A small value for numerical stability.</p> <code>1e-06</code> <p>Returns:     A GaussianProcessInstance distrax Distribution object.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def GaussianProcessFactory(cov_fn: Callable, mean_fn: Callable = Zero(),  nd: Tuple[int, ...] = None, jitter: float = 1e-6):\n    r\"\"\" Returns an instantiated Gaussian process distribution object. \n\n    This is essentially a dx.MultivariateNormalFullCovariance object, with its mean and covariance determined by the mean and covariance functions of the GP.\n\n    Args: \n        cov_fn: The GP covariance function. It assumes a signature of cov_fn(parameters: dict, x: Array, y: Array). \n                This is provided by the `jaxkern` library, but others can be used as well.\n        mean_fn: The GP mean function.\n        nd: A tuple of integers indicating optional additional output dimensions (for multi-task GPs).\n        jitter: A small value for numerical stability.\n    Returns:\n        A GaussianProcessInstance distrax Distribution object.\n\n    \"\"\"\n\n    class GaussianProcessInstance(Distribution):\n        \"\"\" An instantiated Gaussian process distribution object, i.e. a multivariate Gaussian.\n\n        \"\"\"\n\n        def __init__(self, input: Node, **params):\n            self.input = input\n\n            # In case of composite covariance functions:\n            if 'params' in params:\n                self.params = params['params']\n            else:\n                self.params = params\n\n        #\n        def _sample_n(self, key, n):\n            r\"\"\" Sample from the instantiated Gaussian process (i.e. multivariate Gaussian)\n\n            \"\"\"\n            x = self.input\n            m = x.shape[0]\n            output_shape = (m, )\n            if nd is not None:\n                output_shape = nd + output_shape\n\n            if n &gt; 1:\n                output_shape = (n, ) + output_shape\n\n            mu = self._get_mean()\n            cov = self._get_cov()\n            L = jnp.linalg.cholesky(cov)\n            z = jrnd.normal(key, shape=output_shape).T\n            V = jnp.tensordot(L, z, axes=(1, 0))\n            f = jnp.add(mu, jnp.moveaxis(V, 0, -1))\n            if jnp.ndim(f) == 1:\n                f = f[jnp.newaxis, :]\n            return f\n\n        #\n        def log_prob(self, value):\n            mu = self._get_mean()\n            cov = self._get_cov()\n            return dx.MultivariateNormalFullCovariance(loc=mu, covariance_matrix=cov).log_prob(value=value)\n\n        #\n        def sample_predictive_batched(self, key: Array, x_pred: Array, f: Array, num_batches:int = 20):\n            r\"\"\" Samples from the posterior predictve of the latent f, but in batches to converve memory.\n\n            Args:\n                key: PRNGkey\n                x_pred: Array\n                    The test locations\n                f: Array\n                    The trained GP to condition on\n                num_batches: int\n                    The number of batches to predict over.\n\n            Returns:\n                Returns samples from the posterior predictive distribution:\n\n                $$\n                    \\mathbf{f}* \\sim p(\\mathbf{f}* \\mid \\mathbf{f}, X, y x^*) = \\int p(\\mathbf{f}* \\mid x^*, \\mathbf{f}) p(\\mathbf{f} \\mid X, y) \\,\\text{d} \\mathbf{f}\n                ##\n\n\n            \"\"\"\n            if jnp.ndim(x_pred) == 1:\n                x_pred = x_pred[:, jnp.newaxis]\n\n            n_pred = x_pred.shape[0]\n            data_per_batch = int(n_pred / num_batches)\n            fpreds = list()\n            for batch in range(num_batches):\n                key, subkey = jrnd.split(key)\n                lb = data_per_batch*batch\n                ub = data_per_batch*(batch + 1)\n                fpred_batch = self.sample_predictive(subkey, x_pred[lb:ub, :], f)\n                fpreds.append(fpred_batch)\n\n            fpred = jnp.hstack(fpreds)\n            return fpred\n\n        #\n        def sample_predictive(self, key: Array, x_pred: Array, f: Array):\n            r\"\"\"Sample latent f for new points x_pred given one posterior sample.\n\n            See Rasmussen &amp; Williams. We are sampling from the posterior predictive for\n            the latent GP f, at this point not concerned with an observation model yet.\n\n            We have $[\\mathbf{f}, \\mathbf{f}^*]^T ~ \\mathcal{N}(0, KK)$, where $KK$ can be partitioned as:\n\n            $$\n                KK = \\begin{bmatrix} K(x,x) &amp; K(x,x^*) \\\\ K(x,x^*)\\top &amp; K(x^*,x^*)\\end{bmatrix}\n            $$\n\n            This results in the conditional\n            $$\n            \\mathbf{f}^* | x, x^*, \\mathbf{f} ~ \\mathcal{N}(\\mu, \\Sigma) \\enspace,\n             $$ where\n\n            $$\n            \\begin{align*}\n                \\mu &amp;= K(x^*, x)K(x,x)^-1 f \\enspace,\n                \\Sigma &amp;= K(x^*, x^*) - K(x^*, x) K(x, x)^-1 K(x, x^*) \\enspace.\n            \\end{align*}                \n            $$\n\n            Args:\n                key: The jrnd.PRNGKey object\n                x_pred: The prediction locations $x^*$\n                state_variables: A sample from the posterior\n\n            Returns:\n                A single posterior predictive sample $\\mathbf{f}^*$\n\n            \"\"\"\n            x = self.input\n            n = x.shape[0]\n            z = x_pred\n            if 'obs_noise' in self.params:\n                obs_noise = self.params['obs_noise']\n                if jnp.isscalar(obs_noise) or jnp.ndim(obs_noise) == 0:\n                    diagonal_noise = obs_noise**2 * jnp.eye(n, )\n                else:\n                    diagonal_noise = jnp.diagflat(obs_noise)**2\n            else:\n                diagonal_noise = 0\n\n            mean = mean_fn.mean(params=self.params, x=z)\n            Kxx = self.get_cov()\n            Kzx = cov_fn(params=self.params, x=z, y=x)\n            Kzz = cov_fn(params=self.params, x=z, y=z)\n\n            Kxx += jitter * jnp.eye(*Kxx.shape)\n            Kzx += jitter * jnp.eye(*Kzx.shape)\n            Kzz += jitter * jnp.eye(*Kzz.shape)\n\n            L = jnp.linalg.cholesky(Kxx + diagonal_noise)\n            v = jnp.linalg.solve(L, Kzx.T)\n\n            predictive_var = Kzz - jnp.dot(v.T, v)\n            predictive_var += jitter * jnp.eye(*Kzz.shape)\n            C = jnp.linalg.cholesky(predictive_var)\n\n            def get_sample(u_, target_):\n                alpha = jnp.linalg.solve(L.T, jnp.linalg.solve(L, target_))\n                predictive_mean = mean + jnp.dot(Kzx, alpha)\n                return predictive_mean + jnp.dot(C, u_)\n\n            #\n            if jnp.ndim(f) == 3:            \n                _, nu, d = f.shape\n                u = jrnd.normal(key, shape=(len(z), nu, d))\n                samples = jax.vmap(jax.vmap(get_sample, in_axes=1), in_axes=1)(u, f)\n                return samples.transpose([2, 0, 1])\n            elif jnp.ndim(f) == 1:\n                u = jrnd.normal(key, shape=(len(z),))\n                return get_sample(u, f)\n            else:\n                raise NotImplementedError(f'Shape of target must be (n,) or (n, nu, d)',\n                f'but {f.shape} was provided.')\n\n        #\n        def _get_mean(self):\n            \"\"\" Returns the mean of the GP at the input locations.\n\n            \"\"\"\n            return mean_fn.mean(params=self.params, x=self.input)\n\n        #\n        def get_mean(self):\n            return self._get_mean()\n\n        #\n        def _get_cov(self):\n            \"\"\" Returns the covariance of the GP at the input locations.\n\n            \"\"\"\n            x = self.input\n            m = x.shape[0]\n            return cov_fn(params=self.params, x=x, y=x) + jitter * jnp.eye(m)\n\n        #\n        def get_cov(self):\n            return self._get_cov()\n\n        #\n        @property\n        def event_shape(self):\n            r\"\"\" Event shape in this case is the shape of a single draw of $F = (f(x_1), ..., f(x_n))$\n\n            \"\"\"\n\n            output_shape = (self.input.shape[0], )\n            if nd is not None:\n                output_shape = nd + output_shape\n            return output_shape\n\n        #\n        @property\n        def batch_shape(self):\n            return ()\n\n        #\n\n    #\n    return GaussianProcessInstance\n</code></pre>"},{"location":"api/#bamojax.more_distributions.AutoRegressionFactory","title":"<code>bamojax.more_distributions.AutoRegressionFactory(ar_fn)</code>","text":"<p>Generates an autoregressive distribution with Gaussian emissions.</p> <p>This is a generator function that constructs a distrax Distribution object, which can then be queried for its log probability for inference.</p> <p>Parameters:</p> Name Type Description Default <code>ar_fn</code> <code>Callable</code> <p>A Callable function that takes innovations \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), and the previous instances \\(x(t-1), ..., x(t-p)\\), and performs whatever computation the user requires.</p> required Source code in <code>bamojax/more_distributions.py</code> <pre><code>def AutoRegressionFactory(ar_fn: Callable):\n    r\"\"\" Generates an autoregressive distribution with Gaussian emissions.\n\n    This is a generator function that constructs a distrax Distribution object, which can then be queried for its log probability for inference.\n\n    Args:\n        ar_fn: A Callable function that takes innovations $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, and the previous instances $x(t-1), ..., x(t-p)$, and performs whatever computation the user requires.\n\n    \"\"\"\n\n    class ARInstance(Distribution):\n        \"\"\" An instantiated autoregressive distribution object.\n\n        \"\"\"\n\n        def __init__(self, **kwargs):\n            self.parameters = kwargs\n\n        #\n        def _construct_lag_matrix(self, y, y_init):\n            r\"\"\" Construct $y$, and up to order shifts of it.\n\n            \"\"\"\n            order = 1 if jnp.isscalar(y_init) else y_init.shape[0]\n\n            @jax.jit\n            def update_fn(carry, i):\n                y_shifted = jnp.roll(carry, shift=1)  \n                y_shifted = y_shifted.at[0].set(y_init[i])  \n                return y_shifted, y_shifted  \n\n            #\n            _, columns = jax.lax.scan(update_fn, y, jnp.arange(order))\n            return columns\n\n        #\n        def log_prob(self, value):\n            r\"\"\" Returns the log-density of the complete AR distribution\n\n            \"\"\"\n            y_lagged = self._construct_lag_matrix(y=value, y_init=self.parameters['y0'])   \n            mu = ar_fn(y_prev=y_lagged, **self.parameters) \n            return dx.Normal(loc=mu, scale=self.parameters['scale']).log_prob(value)\n\n        #\n        def _sample_n(self, key, n):\n            r\"\"\" Sample from the AR distribution\n\n            \"\"\"\n            keys = jrnd.split(key, n)\n            samples = jax.vmap(self._sample_predictive)(keys)  \n            return samples\n\n        #        \n        def _sample_predictive(self, key):\n            r\"\"\" Sample from the AR(p) model.\n\n            Let:\n\n            $$\n            \\begin{align*}\n                \\epsilon_t &amp;\\sim \\mathcal{N}(0, \\sigma_y)\n                y_t &amp;= f(y_t-1, \\theta) + \\epsilon_t\n            \\end{align*}\n            $$ for $t = M+1, \\ldots, T$.\n\n            \"\"\"\n            @jax.jit\n            def ar_step(carry, epsilon_t):\n                y_t = ar_fn(y_prev=carry, **self.parameters) + epsilon_t\n                new_carry = jnp.concatenate([carry[1:], jnp.array([y_t])])\n                return new_carry, y_t\n\n            # \n            y_init = self.parameters['y0']\n            order = 1 if jnp.isscalar(y_init) else y_init.shape[0]\n            innovations = self.parameters['scale'] * jrnd.normal(key, shape=(self.parameters['T'] - order, ))\n            _, ys = jax.lax.scan(ar_step, y_init, innovations)\n            y = jnp.concatenate([y_init, ys])\n            return y\n\n        #                        \n        @property\n        def batch_shape(self):\n            return ( )\n\n        #\n        @property\n        def event_shape(self):\n            return (self.T, )\n\n        #\n\n    #\n    return ARInstance\n</code></pre>"},{"location":"api/#bamojax.more_distributions.AscendingDistribution","title":"<code>bamojax.more_distributions.AscendingDistribution(min_u, max_u, num_el)</code>","text":"<p>Creates a distribution of a sorted array of continuous values in [min_u, max_u].</p> <p>To ensure gradient-based methods can work on the model, all transformations must be bijectors. A generic sort() does not meet this condition, as it is not invertible. By using the tfb  bijector Ascending() in combination with scaling and deriving the expected maximum value of  dx.Transformed(Uniform, Ascending()), we can construct, in expectation, the desired random  variable. Note that individual draws main contain values that exceed max_u. </p> <p>Parameters:</p> Name Type Description Default <code>min_u, max_u</code> <p>The desired range.</p> required <code>num_el</code> <p>The length of the desired variate.</p> required <p>Returns:     A distribution over arrays of length <code>num_el</code>, with values in ascending order.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def AscendingDistribution(min_u, max_u, num_el):\n    r\"\"\" Creates a distribution of a sorted array of continuous values in [min_u, max_u].\n\n    To ensure gradient-based methods can work on the model, all transformations must be bijectors.\n    A generic sort() does not meet this condition, as it is not invertible. By using the tfb \n    bijector Ascending() in combination with scaling and deriving the expected maximum value of \n    dx.Transformed(Uniform, Ascending()), we can construct, in expectation, the desired random \n    variable. Note that individual draws main contain values that exceed max_u. \n\n    Args:\n        min_u, max_u: The desired range.\n        num_el: The length of the desired variate.\n    Returns:\n        A distribution over arrays of length `num_el`, with values in ascending order.\n\n\n    \"\"\"\n\n    R = 0.5 + (num_el-1)*(jnp.exp(1) - 1)\n    base_distribution = dx.Independent(dx.Uniform(low=jnp.zeros(num_el), high=jnp.ones(num_el)), reinterpreted_batch_ndims=1)\n\n    bijector = tfb.Chain([\n        tfb.Scale(scale=(max_u - min_u) / R),  \n        tfb.Shift(shift=jnp.array(min_u, dtype=jnp.float64)),  \n        tfb.Ascending()               \n    ])\n\n    return dx.Transformed(base_distribution, bijector)\n</code></pre>"},{"location":"api/#bamojax.samplers","title":"<code>bamojax.samplers</code>","text":""},{"location":"api/#bamojax.samplers.gibbs_sampler","title":"<code>bamojax.samplers.gibbs_sampler(model, step_fns=None, step_fn_params=None)</code>","text":"<p>Constructs a Gibbs sampler as a Blackjax SamplingAlgorithm.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The bamojax definition of a Bayesian model.</p> required <code>step_fns</code> <code>dict</code> <p>(optional) a set of step functions to use for updating each variable in turn.</p> <code>None</code> <code>step_fn_params</code> <code>dict</code> <p>(optional) parameters of the step functions</p> <code>None</code> <p>Returns:</p> Type Description <code>SamplingAlgorithm</code> <p>A SamplingAlgorithm object. This can be used to call the respective .init and .step functions in the inference routines.</p> Source code in <code>bamojax/samplers.py</code> <pre><code>def gibbs_sampler(model: Model, \n                  step_fns: dict = None, \n                  step_fn_params: dict = None) -&gt; SamplingAlgorithm:\n    r\"\"\" Constructs a Gibbs sampler as a Blackjax SamplingAlgorithm.\n\n    Args:\n        model: The bamojax definition of a Bayesian model.\n        step_fns: (optional) a set of step functions to use for updating each variable in turn.\n        step_fn_params: (optional) parameters of the step functions\n\n    Returns:\n        A SamplingAlgorithm object. This can be used to call the respective .init and .step functions in the inference routines.\n\n    \"\"\"\n\n    def set_step_fns_defaults(step_fns: dict = None, step_fn_params: dict = None):\n        r\"\"\" Set the step function of each node if not specified. Defaults to a Gaussian random walk with a stepsize of 0.01.\n\n        \"\"\"\n        if step_fns is None:\n            step_fns = {}\n            print('No step functions found; setting defaults.')\n        if step_fn_params is None:\n            step_fn_params = {}\n\n        sorted_free_variables = [node for node in model.get_node_order() if node.is_stochastic() and not node.is_observed() and not node in step_fns]\n        for node in sorted_free_variables:\n            step_fns[node] = normal_random_walk\n            num_elem = 1 if node.shape == () else jnp.prod(jnp.asarray(node.shape))\n            step_fn_params[node] = dict(sigma=0.01*jnp.eye(num_elem))\n\n        return step_fns, step_fn_params\n\n    #\n    step_fns, step_fn_params = set_step_fns_defaults(step_fns=step_fns, step_fn_params=step_fn_params)\n\n    def get_nonstandard_gibbs_step(node, \n                                   position, \n                                   loglikelihood_fn, \n                                   step_fns, \n                                   step_fn_params):\n        r\"\"\" The Blackjax SamplingAlgorithm is not parametrized in entirely the same way for different algorithms. To not clutter the gibbs_fn, exception cases are handled here.\n\n        \"\"\"\n        mean = node.get_distribution(position).get_mean()\n        cov = node.get_distribution(position).get_cov()\n        if step_fn_params[node]['name'] == 'elliptical_slice':                    \n            step_kernel = step_fns[node](loglikelihood_fn, mean=mean, cov=cov)\n            step_substate = step_kernel.init({node.name: position[node]})  \n        elif step_fn_params[node]['name'] == 'elliptical_slice_nd':                     \n            nd = step_fn_params[node]['nd']\n            step_kernel = step_fns[node](loglikelihood_fn, mean=mean, cov=cov, nd=nd)\n            step_substate = step_kernel.init({node.name: position[node]})  \n        elif step_fn_params[node]['name'] == 'mgrad_gaussian':\n            # see issue https://github.com/blackjax-devs/blackjax/issues/237,mgrad does not seem robust yet\n            loglikelihood_fn_mgrad = lambda state: loglikelihood_fn(state[node])\n            step_kernel = step_fns[node](logdensity_fn=loglikelihood_fn_mgrad, mean=mean, covariance=cov, **step_fn_params[node]['params'])\n            step_substate = step_kernel.init({node.name: position[node.name]}) \n        else:\n            raise NotImplementedError\n        return step_kernel, step_substate\n\n    #\n    def gibbs_fn(model: Model, \n                 key, \n                 state: dict, \n                 *args, \n                 **kwargs) -&gt; dict:\n        r\"\"\" Updates each latent variable given the current assignment of all other latent variables, according to the assigned step functions.\n\n        The signature of the Gibbs function is (key, state, temperature) -&gt; (state, info)\n\n        The Gibbs densities are defined as follows. Let $\\text{Pa}(x)$ give the parents of the set of variables $x$, and let $\\text{Ch}(x)$ give the set of children. Then the density is given by:\n\n        $$\n            p\\left(x \\mid \\text{Pa}(x)\\right) \\propto p\\left(\\text{Ch}(x) \\mid \\text{Pa}(\\text{Ch}(x))\\right) p\\left(x \\mid \\text{Pa}(x)\\right)\n        $$\n\n        Args:\n            key: PRNGKey\n            state: The current assignment of all latent variables.\n\n        Returns:\n            state: The updated assignment of all latent variables.\n            info: Additional information regarding the updates for every latent variable, such as acceptance rates.\n\n        \"\"\"\n\n        # In case we apply likelihood tempering\n        temperature = kwargs.get('temperature', 1.0)\n        position = state.position.copy()\n\n        info = {}\n        sorted_free_variables = [node for node in model.get_node_order() if node.is_stochastic() and not node.is_observed()]\n\n        for node in sorted_free_variables:\n            # Get conditional densities\n            conditionals = []\n            children = [c for c in model.get_children(node)]        \n            for child in children:\n                # Co-parents are all parents of the child, except node\n                co_parents = {parent for parent in model.get_parents(child) if parent != node}\n\n                # Values for co-parents are either taken from the position (if latent), or from their respective observations (if observed)\n                co_parent_arguments = {k: (position[k] if k in position else k.observations) for k in co_parents}\n\n                def loglikelihood_fn_(substate):\n                    dynamic_state = {**co_parent_arguments, node.name: substate[node]}\n                    child_value = child.observations if child.is_leaf() else position[child]\n                    return child.get_distribution(dynamic_state).log_prob(value=child_value)\n\n                #            \n                co_parents.add(node)\n                conditionals.append(loglikelihood_fn_)\n\n            loglikelihood_fn = lambda val: jnp.sum(jnp.asarray([temperature*ll_fn(val).sum() for ll_fn in conditionals]))\n\n            if 'implied_mvn_prior' in step_fn_params[node]:\n                # Some Blackjax step functions are tailored to multivariate Gaussian priors.\n                step_kernel, step_substate = get_nonstandard_gibbs_step(node, position, loglikelihood_fn, step_fns, step_fn_params)\n            else:\n                logprior_fn = lambda substate: node.get_distribution(position).log_prob(value=substate[node]).sum() \n                logdensity_fn = lambda substate_: loglikelihood_fn(substate_) + logprior_fn(substate_)\n                step_kernel = step_fns[node](logdensity_fn, **step_fn_params[node])   \n                step_substate = step_kernel.init({node.name: position[node]})   \n\n            key, subkey = jrnd.split(key)     \n            # [TODO]: add functionality to sample specific variables for different numbers of steps\n            step_substate, step_info = step_kernel.step(subkey, step_substate)\n            info[node.name] = step_info\n\n            position = {**position, **step_substate.position}\n\n            del step_kernel\n            del step_substate\n            del conditionals\n            del children\n\n        del state\n        return GibbsState(position=position), info\n\n    #\n    def init_fn(position, rng_key=None):\n        del rng_key\n        return GibbsState(position=position)\n\n    #\n    def step_fn(key: PRNGKey, state, *args, **kwargs):\n        state, info = gibbs_fn(model, key, state, *args, **kwargs)\n        return state, info\n\n    #\n    step_fn.__name__ = 'gibbs_step_fn'\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/#bamojax.samplers.mcmc_sampler","title":"<code>bamojax.samplers.mcmc_sampler(model, mcmc_kernel, mcmc_parameters=None)</code>","text":"<p>Constructs an MCMC sampler from a given Blackjax algorithm.</p> <p>This lightweight wrapper ensures the (optional) tempering parameter 'temperature', as part of the keyword-arguments of step_fn(..., **kwargs), is passed correctly.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>A bamojax model definition.</p> required <code>mcmc_kernel</code> <p>A Blackjax MCMC algorithm.</p> required <code>mcmc_parameters</code> <code>dict</code> <p>Optional Blackjax MCMC parameters, such as step sizes.</p> <code>None</code> <p>Returns:     A Blackjax SamplingAlgorithm object with methods <code>init_fn</code> and <code>step_fn</code>.</p> Source code in <code>bamojax/samplers.py</code> <pre><code>def mcmc_sampler(model: Model, \n                 mcmc_kernel, \n                 mcmc_parameters: dict = None):\n    \"\"\" Constructs an MCMC sampler from a given Blackjax algorithm.\n\n    This lightweight wrapper ensures the (optional) tempering parameter 'temperature',\n    as part of the keyword-arguments of step_fn(..., **kwargs), is passed correctly.\n\n    Args:\n        model: A bamojax model definition.\n        mcmc_kernel: A Blackjax MCMC algorithm.\n        mcmc_parameters: Optional Blackjax MCMC parameters, such as step sizes.\n    Returns:\n        A Blackjax SamplingAlgorithm object with methods `init_fn` and `step_fn`.\n\n    \"\"\"\n\n    def mcmc_fn(model: Model, \n                key, \n                state: dict, \n                *args, \n                **kwargs) -&gt; dict:\n\n        def apply_mcmc_kernel(key_, logdensity_fn, pos):\n            kernel_instance = mcmc_kernel(logdensity_fn=logdensity_fn, **mcmc_parameters)\n            state_ = kernel_instance.init(pos)\n            state_, info_ = kernel_instance.step(key_, state_)\n            return state_.position, info_\n\n        #\n        temperature = kwargs.get('temperature', 1.0)\n        position = state.position.copy()\n\n        loglikelihood_fn_ = model.loglikelihood_fn()\n        logprior_fn_ = model.logprior_fn()\n        tempered_logdensity_fn = lambda state: jnp.squeeze(temperature * loglikelihood_fn_(state) + logprior_fn_(state))\n        new_position, mcmc_info = apply_mcmc_kernel(key, tempered_logdensity_fn, position)\n        return MCMCState(position=new_position), mcmc_info\n\n    #\n    def init_fn(position, rng_key=None):\n        del rng_key\n        return MCMCState(position=position)\n\n    #\n    def step_fn(key: PRNGKey, state, *args, **kwargs):\n        state, info = mcmc_fn(model, key, state, *args, **kwargs)\n        return state, info\n\n    #\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/#bamojax.samplers.reversible_jump_mcmc","title":"<code>bamojax.samplers.reversible_jump_mcmc(models, auxiliary_proposal_dist, jump_functions, jacobians, projections, within_model_kernels, model_move_prob=0.5)</code>","text":"<p>Constructs a reversible jump MCMC algorithm for the given models.</p> <p>Implementation follows the reversible jump MCMC algorithm described by Hastie &amp; Green (2012).</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[Model]</code> <p>List of models to sample from.</p> required <code>auxiliary_proposal_dist</code> <code>Distribution</code> <p>Distribution to sample the auxiliary variable from.</p> required <code>jump_functions</code> <code>list[Callable]</code> <p>List of functions to transform the position when jumping between models.</p> required <code>jacobians</code> <code>list[Callable]</code> <p>List of Jacobian determinant functions for the jump functions</p> required <code>within_model_kernel</code> <p>List of within-model sampling algorithms, one for each model.</p> required <p>Returns:</p> Name Type Description <code>SamplingAlgorithm</code> <code>SamplingAlgorithm</code> <p>A sampling algorithm that implements the reversible jump MCMC.</p> References <p>Hastie, T., &amp; Green, P. J. (2012). Model choice using reversible jump Markov chain Monte Carlo</p> Source code in <code>bamojax/samplers.py</code> <pre><code>def reversible_jump_mcmc(models: list[Model], \n                         auxiliary_proposal_dist: Distribution,\n                         jump_functions: list[Callable],\n                         jacobians: list[Callable],\n                         projections: list[Callable],\n                         within_model_kernels: list[dict],\n                         model_move_prob: float = 0.5) -&gt; SamplingAlgorithm:\n    \"\"\"Constructs a reversible jump MCMC algorithm for the given models.\n\n    Implementation follows the reversible jump MCMC algorithm described by Hastie &amp; Green (2012).\n\n    Args:\n        models: List of models to sample from.\n        auxiliary_proposal_dist: Distribution to sample the auxiliary variable from.\n        jump_functions: List of functions to transform the position when jumping between models.\n        jacobians: List of Jacobian determinant functions for the jump functions\n        within_model_kernel: List of within-model sampling algorithms, one for each model.\n\n    Returns:\n        SamplingAlgorithm: A sampling algorithm that implements the reversible jump MCMC.\n\n    References:\n        Hastie, T., &amp; Green, P. J. (2012). Model choice using reversible jump Markov chain Monte Carlo\n\n    \"\"\"\n\n    assert len(models) == 2, 'Reversible jump MCMC currently only supports two models.'\n    assert len(jump_functions) == 2, 'Reversible jump MCMC requires two jump functions for the two models.'\n    assert len(jacobians) == 2, 'Reversible jump MCMC requires two Jacobian determinant functions for the two models.'\n    assert len(within_model_kernels) == 2, 'Reversible jump MCMC requires within-model sampling algorithms for each models.'\n\n    def make_within_model_kernel(model_index) -&gt; SamplingAlgorithm:\n        \"\"\"Creates a within-model kernel for the specified model index.\n\n        Args:\n            model_index: Index of the model for which to create the within-model kernel.\n\n        Returns:\n            A SamplingAlgorithm that performs within-model sampling for the specified model.\n\n        \"\"\"\n        return mcmc_sampler(models[model_index], \n                            mcmc_kernel=within_model_kernels[model_index]['mcmc_kernel'], \n                            mcmc_parameters=within_model_kernels[model_index]['mcmc_parameters'])\n\n    #    \n    def make_within_model_fn(model_index) -&gt; tuple[RJState, dict]:\n        \"\"\"Creates a function that performs a within-model move for the specified model index.\n\n        Args:\n            model_index: Index of the model for which to create the within-model move function.\n\n        Returns:\n            A function that takes a key, position, and optional arguments, and returns a new RJState and info dictionary.\n            The info dictionary imputes nan values for log probabilities and Jacobian determinants, as they are not used in within-model moves.\n\n        \"\"\"\n        def fn( key, position, *args):\n            temperature = args[0] if len(args) &gt; 0 else 1.0\n            kernel = mcmc_samplers[model_index]\n            initial_position = {k: position[k] for k in models[model_index].get_latent_nodes()}            \n            within_move_initial_state = kernel.init(position=initial_position)\n            new_state, info = kernel.step(key, within_move_initial_state, temperature=temperature)\n            new_position = {**position, **new_state.position, 'model_index': model_index}  \n            return RJState(position=new_position), {'within_model_move': 1, \n                                                    'is_accepted': info.is_accepted,\n                                                    'log_accept_ratio': jnp.log(info.acceptance_rate),\n                                                    'step_info': {'log_p_current': jnp.nan,\n                                                                  'log_p_proposed': jnp.nan,\n                                                                  'logq': jnp.nan,\n                                                                  'jacobian_det': jnp.nan}}\n        return fn\n\n    #\n    def make_model_logprob(model):\n        \"\"\"Creates a function that computes the log probability of the model given a position.\"\"\"\n\n        latent_keys = model.get_latent_nodes()\n\n        def fn(position):\n            \"\"\"Position might contain auxiliary variables and variables from other models, so we extract only the correct latent variables.\"\"\"\n            model_variables = {k: position[k] for k in latent_keys}\n            return model.loglikelihood_fn()(model_variables) + model.logprior_fn()(model_variables)\n\n        return fn\n\n    #\n    def reversible_jump_fn(key: PRNGKey, state: RJState, *args, **kwargs):\n        \"\"\"Performs a reversible jump MCMC step.\n\n        Args:\n            key: Random key for sampling.\n            state: Current state of the reversible jump MCMC, containing the model index and position.\n            *args: Additional arguments to pass to the within-model sampling functions. NOT USED\n            **kwargs: Additional keyword arguments to pass to the within-model sampling functions. NOT USED\n\n        Returns:\n            A tuple containing the next state and an info dictionary with details about the move.\n\n        \"\"\"\n\n        position = state.position\n        model_index = position['model_index']\n        key, subkey = jrnd.split(key)\n        move_type = jrnd.bernoulli(subkey, p=model_move_prob)\n        jacobian_det_up, jacobian_det_down = jacobians\n\n        def do_within_model(_):\n            \"\"\"Perform a standard bamojax within-model move.\"\"\"\n\n            temperature = kwargs.get('temperature', 1.0)\n            return jax.lax.switch(model_index, within_model_fns, key, position, temperature)\n\n        #\n        def do_between_model(_):\n            \"\"\"Perform a reversible jump between models.\n\n            Currently, there is only support for RJMCMC between two models.\n\n            \"\"\"\n            new_model_index = 1 - model_index\n            key_aux, key_accept = jrnd.split(key)            \n\n            def up_branch(_):\n                u = auxiliary_proposal_dist.sample(seed=key_aux)\n                new_position = jump_functions[0](position, u)  # make kappa from the auxiliary variable u\n                jac_det = jacobian_det_up(u)\n                logq = -1.0*auxiliary_proposal_dist.log_prob(u)  # Note the negative sign! To check: is this robust for other proposal distributions?\n                return new_position, jac_det, logq\n\n            #\n            def down_branch(_):\n                new_position = jump_functions[1](position) # discard auxiliary variable and kappa\n                jac_det = jacobian_det_down(new_position['kappa'])\n                logq = auxiliary_proposal_dist.log_prob(projections[1](new_position)) # log(kappa / mu) where mu is the mean of the auxiliary proposal\n                return new_position, jac_det, logq\n\n            #\n            new_position, jac_det, logq = jax.lax.cond(model_index == 0, up_branch, down_branch, operand=None)\n            new_position['model_index'] = new_model_index  # update model index in the new position\n            log_p_current = jax.lax.switch(model_index, model_logprobs, position)\n            log_p_proposed = jax.lax.switch(new_model_index, model_logprobs, new_position)\n            log_accept_ratio = log_p_proposed - log_p_current + logq + jnp.log(jac_det) \n\n            accept = jnp.log(jrnd.uniform(key_accept)) &lt; log_accept_ratio\n            next_state = jax.lax.cond(accept, lambda _: RJState(new_position), lambda _: state, operand=None)       \n\n            return next_state, {'within_model_move': 0, \n                                'is_accepted': accept, \n                                'log_accept_ratio': log_accept_ratio,\n                                'step_info': {'log_p_current': log_p_current,\n                                              'log_p_proposed': log_p_proposed,\n                                              'logq': logq,\n                                              'jacobian_det': jac_det}}\n\n        #\n        return jax.lax.cond(move_type, do_within_model, do_between_model, operand=None)\n\n    #\n    def init_fn(position: ArrayTree, rng_key=None):\n        del rng_key\n        return RJState(position=position)\n\n    #\n    def step_fn(key: PRNGKey, state, *args, **kwargs):\n        state, info = reversible_jump_fn(key, state, *args, **kwargs)\n        return state, info\n\n    #\n\n    within_model_fns = [make_within_model_fn(i) for i in range(len(models))]\n    model_logprobs = [make_model_logprob(model) for model in models]\n    mcmc_samplers = [make_within_model_kernel(i) for i in range(len(models))]\n\n    step_fn.__name__ = 'reversible_jump_fn'\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/","title":"API reference","text":"<ul> <li><code>bamojax._version</code></li> <li><code>bamojax.base</code></li> <li><code>bamojax.inference</code></li> <li><code>bamojax.marginal_likelihoods</code></li> <li><code>bamojax.modified_blackjax.modified_adaptive_tempered</code></li> <li><code>bamojax.modified_blackjax.modified_elliptical_slice_nd</code></li> <li><code>bamojax.modified_blackjax.modified_tempered</code></li> <li><code>bamojax.modified_blackjax.more_proposals</code></li> <li><code>bamojax.more_distributions</code></li> <li><code>bamojax.samplers</code></li> </ul>"},{"location":"api/bamojax._version/","title":"<code>bamojax._version</code>","text":"<p>Git implementation of _version.py.</p>"},{"location":"api/bamojax._version/#bamojax._version.VersioneerConfig","title":"<code>VersioneerConfig</code>","text":"<p>Container for Versioneer configuration parameters.</p> Source code in <code>bamojax/_version.py</code> <pre><code>class VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n    VCS: str\n    style: str\n    tag_prefix: str\n    parentdir_prefix: str\n    versionfile_source: str\n    verbose: bool\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.NotThisMethod","title":"<code>NotThisMethod</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised if a method is not valid for the current scenario.</p> Source code in <code>bamojax/_version.py</code> <pre><code>class NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.get_keywords","title":"<code>get_keywords()</code>","text":"<p>Get the keywords needed to look up the version information.</p> Source code in <code>bamojax/_version.py</code> <pre><code>def get_keywords() -&gt; Dict[str, str]:\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"$Format:%d$\"\n    git_full = \"$Format:%H$\"\n    git_date = \"$Format:%ci$\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.get_config","title":"<code>get_config()</code>","text":"<p>Create, populate and return the VersioneerConfig() object.</p> Source code in <code>bamojax/_version.py</code> <pre><code>def get_config() -&gt; VersioneerConfig:\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"v\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"bamojax/_version.py\"\n    cfg.verbose = False\n    return cfg\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.register_vcs_handler","title":"<code>register_vcs_handler(vcs, method)</code>","text":"<p>Create decorator to mark a method as the handler of a VCS.</p> Source code in <code>bamojax/_version.py</code> <pre><code>def register_vcs_handler(vcs: str, method: str) -&gt; Callable:  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f: Callable) -&gt; Callable:\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.run_command","title":"<code>run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None)</code>","text":"<p>Call the given command(s).</p> Source code in <code>bamojax/_version.py</code> <pre><code>def run_command(\n    commands: List[str],\n    args: List[str],\n    cwd: Optional[str] = None,\n    verbose: bool = False,\n    hide_stderr: bool = False,\n    env: Optional[Dict[str, str]] = None,\n) -&gt; Tuple[Optional[str], Optional[int]]:\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs: Dict[str, Any] = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n                                       stdout=subprocess.PIPE,\n                                       stderr=(subprocess.PIPE if hide_stderr\n                                               else None), **popen_kwargs)\n            break\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.versions_from_parentdir","title":"<code>versions_from_parentdir(parentdir_prefix, root, verbose)</code>","text":"<p>Try to determine the version from the parent directory name.</p> <p>Source tarballs conventionally unpack into a directory that includes both the project name and a version string. We will also support searching up two directory levels for an appropriately named parent directory</p> Source code in <code>bamojax/_version.py</code> <pre><code>def versions_from_parentdir(\n    parentdir_prefix: str,\n    root: str,\n    verbose: bool,\n) -&gt; Dict[str, Any]:\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\"version\": dirname[len(parentdir_prefix):],\n                    \"full-revisionid\": None,\n                    \"dirty\": False, \"error\": None, \"date\": None}\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %s but none started with prefix %s\" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.git_get_keywords","title":"<code>git_get_keywords(versionfile_abs)</code>","text":"<p>Extract version information from the given file.</p> Source code in <code>bamojax/_version.py</code> <pre><code>@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs: str) -&gt; Dict[str, str]:\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords: Dict[str, str] = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.git_versions_from_keywords","title":"<code>git_versions_from_keywords(keywords, tag_prefix, verbose)</code>","text":"<p>Get version information from git keywords.</p> Source code in <code>bamojax/_version.py</code> <pre><code>@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(\n    keywords: Dict[str, str],\n    tag_prefix: str,\n    verbose: bool,\n) -&gt; Dict[str, Any]:\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git &lt; 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r'\\d', r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r'\\d', r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None,\n                    \"date\": date}\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.git_pieces_from_vcs","title":"<code>git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command)</code>","text":"<p>Get version from 'git describe' in the root of the source tree.</p> <p>This only gets called if the git-archive 'subst' keywords were not expanded, and _version.py hasn't already been rewritten with a short version string, meaning we're inside a checked out source tree.</p> Source code in <code>bamojax/_version.py</code> <pre><code>@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(\n    tag_prefix: str,\n    root: str,\n    verbose: bool,\n    runner: Callable = run_command\n) -&gt; Dict[str, Any]:\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                   hide_stderr=not verbose)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(GITS, [\n        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n    ], cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces: Dict[str, Any] = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n                             cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.plus_or_dot","title":"<code>plus_or_dot(pieces)</code>","text":"<p>Return a + if we don't already have one, else return a .</p> Source code in <code>bamojax/_version.py</code> <pre><code>def plus_or_dot(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_pep440","title":"<code>render_pep440(pieces)</code>","text":"<p>Build up version string, with post-release \"local version identifier\".</p> <p>Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty</p> <p>Exceptions: 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_pep440(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_pep440_branch","title":"<code>render_pep440_branch(pieces)</code>","text":"<p>TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .</p> <p>The \".dev0\" means not master branch. Note that .dev0 sorts backwards (a feature branch will appear \"older\" than the master branch).</p> <p>Exceptions: 1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_pep440_branch(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.pep440_split_post","title":"<code>pep440_split_post(ver)</code>","text":"<p>Split pep440 version string at the post-release segment.</p> <p>Returns the release segments before the post-release and the post-release version number (or -1 if no post-release segment is present).</p> Source code in <code>bamojax/_version.py</code> <pre><code>def pep440_split_post(ver: str) -&gt; Tuple[str, Optional[int]]:\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_pep440_pre","title":"<code>render_pep440_pre(pieces)</code>","text":"<p>TAG[.postN.devDISTANCE] -- No -dirty.</p> <p>Exceptions: 1: no tags. 0.post0.devDISTANCE</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_pep440_pre(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_pep440_post","title":"<code>render_pep440_post(pieces)</code>","text":"<p>TAG[.postDISTANCE[.dev0]+gHEX] .</p> <p>The \".dev0\" means dirty. Note that .dev0 sorts backwards (a dirty tree will appear \"older\" than the corresponding clean one), but you shouldn't be releasing software with -dirty anyways.</p> <p>Exceptions: 1: no tags. 0.postDISTANCE[.dev0]</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_pep440_post(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_pep440_post_branch","title":"<code>render_pep440_post_branch(pieces)</code>","text":"<p>TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .</p> <p>The \".dev0\" means not master branch.</p> <p>Exceptions: 1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_pep440_post_branch(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_pep440_old","title":"<code>render_pep440_old(pieces)</code>","text":"<p>TAG[.postDISTANCE[.dev0]] .</p> <p>The \".dev0\" means dirty.</p> <p>Exceptions: 1: no tags. 0.postDISTANCE[.dev0]</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_pep440_old(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_git_describe","title":"<code>render_git_describe(pieces)</code>","text":"<p>TAG[-DISTANCE-gHEX][-dirty].</p> <p>Like 'git describe --tags --dirty --always'.</p> <p>Exceptions: 1: no tags. HEX[-dirty]  (note: no 'g' prefix)</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_git_describe(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render_git_describe_long","title":"<code>render_git_describe_long(pieces)</code>","text":"<p>TAG-DISTANCE-gHEX[-dirty].</p> <p>Like 'git describe --tags --dirty --always -long'. The distance/hash is unconditional.</p> <p>Exceptions: 1: no tags. HEX[-dirty]  (note: no 'g' prefix)</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render_git_describe_long(pieces: Dict[str, Any]) -&gt; str:\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.render","title":"<code>render(pieces, style)</code>","text":"<p>Render the given version pieces into the requested style.</p> Source code in <code>bamojax/_version.py</code> <pre><code>def render(pieces: Dict[str, Any], style: str) -&gt; Dict[str, Any]:\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\n            \"date\": pieces.get(\"date\")}\n</code></pre>"},{"location":"api/bamojax._version/#bamojax._version.get_versions","title":"<code>get_versions()</code>","text":"<p>Get version information or return default if unable to do so.</p> Source code in <code>bamojax/_version.py</code> <pre><code>def get_versions() -&gt; Dict[str, Any]:\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\",\n                \"date\": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\", \"date\": None}\n</code></pre>"},{"location":"api/bamojax.base/","title":"<code>bamojax.base</code>","text":""},{"location":"api/bamojax.base/#bamojax.base.Node","title":"<code>Node</code>","text":"<p>The essential element of any Bayesian model is the variable, represented by a node in a DAG. </p> <p>Nodes can consist of stochastic or deterministic variables, and can be observed or latent.</p> <p>Hyperparameters of a model are implicitly observed, deterministic nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>class Node:\n    r\"\"\" The essential element of any Bayesian model is the variable, represented by a node in a DAG. \n\n    Nodes can consist of stochastic or deterministic variables, and can be observed or latent.\n\n    Hyperparameters of a model are implicitly observed, deterministic nodes.\n\n    \"\"\"\n\n    def __init__(self, name: str = 'root', \n                 observations: Array = None, \n                 distribution: Union[Distribution, Bijector] = None, \n                 parents = None, \n                 link_fn: Callable = None,\n                 shape: Union[Tuple, int] = None,\n                 bijector: Bijector = None):\n        self.name = name\n\n        if shape is None: \n            shape = ( )\n        self.shape = shape\n        if bijector is not None:\n            self.bijector = bijector\n        if observations is not None:\n            observations = jnp.asarray(observations) if jnp.isscalar(observations) else observations\n            self.observations = observations\n        if distribution is not None:\n            self.distribution = distribution\n            self.parents = {}\n            if parents is not None:\n                for param, parent in parents.items():\n                    self.add_parent(param, parent)            \n            if link_fn is None:\n                def identity(**kwargs):\n                    return kwargs\n                link_fn = identity\n            self.link_fn = link_fn        \n\n    #\n    def is_observed(self) -&gt; bool:\n        \"\"\" Check if a node is an observed variable.\n\n        \"\"\"\n        return hasattr(self, 'observations')\n\n    #\n    def is_stochastic(self) -&gt; bool:\n        \"\"\" Check whether a node is stochastic or deterministic.\n\n        \"\"\"\n        return hasattr(self, 'distribution')\n\n    #\n    def is_root(self) -&gt; bool:\n        \"\"\" Check whether a node is a root node.\n\n        \"\"\"\n        return not hasattr(self, 'parents') or len(self.parents) == 0\n\n    #\n    def add_parent(self, param, node):\n        \"\"\" Add a parent node.\n\n        \"\"\"\n        assert isinstance(node, Node)\n        self.parents[param] = node\n\n    #\n    def set_step_fn(self, step_fn):\n        self.step_fn = step_fn\n\n    #\n    def set_step_fn_parameter(self, step_fn_params):\n        self.step_fn_params = step_fn_params\n\n    #\n    def is_leaf(self):\n        \"\"\" Check whether a node is observed and has parents.\n\n        \"\"\"\n        return hasattr(self, 'parents') and self.is_observed()\n\n    #    \n    def get_distribution(self, state: dict = None, minibatch: dict = None) -&gt; Distribution:\n        r\"\"\" Derives the parametrized distribution p(node | Parents=x), where x is derived from the state object.\n\n        Args:\n            state: Current assignment of (parent) values.\n            minibatch: A additional set of assigned variables, useful for out-of-sample predictions.\n        Returns:\n            An instantiated distrax distribution object.\n\n        \"\"\"\n\n        # Root-level nodes can be defined as instantiated distrax distributions.\n        if isinstance(self.distribution, Distribution):\n            return self.distribution\n\n        if minibatch is None:\n            minibatch = {}\n\n        # Otherwise the distribution is instantiated from the state.\n        parent_values = {}\n        for parent_name, parent_node in self.parents.items():\n            if parent_node in state:\n                parent_values[parent_name] = state[parent_node]\n            else:\n                if parent_name in minibatch:\n                    parent_values[parent_name] = minibatch[parent_node]\n                else:\n                    parent_values[parent_name] = self.parents[parent_name].observations\n\n        transformed_parents = self.link_fn(**parent_values)\n        if hasattr(self, 'bijector'):\n            return dx.Transformed(self.distribution(**transformed_parents), self.bijector)\n        else:\n            return self.distribution(**transformed_parents)\n\n    #\n    def __repr__(self) -&gt; str:\n        return f'{self.name}'\n\n    #\n    def __hash__(self):\n        return hash((self.name))\n\n    #\n    def __eq__(self, other):\n        if isinstance(other, Node):\n            return self.name == other.name\n        return NotImplementedError\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Node.is_observed","title":"<code>is_observed()</code>","text":"<p>Check if a node is an observed variable.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_observed(self) -&gt; bool:\n    \"\"\" Check if a node is an observed variable.\n\n    \"\"\"\n    return hasattr(self, 'observations')\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Node.is_stochastic","title":"<code>is_stochastic()</code>","text":"<p>Check whether a node is stochastic or deterministic.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_stochastic(self) -&gt; bool:\n    \"\"\" Check whether a node is stochastic or deterministic.\n\n    \"\"\"\n    return hasattr(self, 'distribution')\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Node.is_root","title":"<code>is_root()</code>","text":"<p>Check whether a node is a root node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_root(self) -&gt; bool:\n    \"\"\" Check whether a node is a root node.\n\n    \"\"\"\n    return not hasattr(self, 'parents') or len(self.parents) == 0\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Node.add_parent","title":"<code>add_parent(param, node)</code>","text":"<p>Add a parent node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def add_parent(self, param, node):\n    \"\"\" Add a parent node.\n\n    \"\"\"\n    assert isinstance(node, Node)\n    self.parents[param] = node\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Node.is_leaf","title":"<code>is_leaf()</code>","text":"<p>Check whether a node is observed and has parents.</p> Source code in <code>bamojax/base.py</code> <pre><code>def is_leaf(self):\n    \"\"\" Check whether a node is observed and has parents.\n\n    \"\"\"\n    return hasattr(self, 'parents') and self.is_observed()\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Node.get_distribution","title":"<code>get_distribution(state=None, minibatch=None)</code>","text":"<p>Derives the parametrized distribution p(node | Parents=x), where x is derived from the state object.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Current assignment of (parent) values.</p> <code>None</code> <code>minibatch</code> <code>dict</code> <p>A additional set of assigned variables, useful for out-of-sample predictions.</p> <code>None</code> <p>Returns:     An instantiated distrax distribution object.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_distribution(self, state: dict = None, minibatch: dict = None) -&gt; Distribution:\n    r\"\"\" Derives the parametrized distribution p(node | Parents=x), where x is derived from the state object.\n\n    Args:\n        state: Current assignment of (parent) values.\n        minibatch: A additional set of assigned variables, useful for out-of-sample predictions.\n    Returns:\n        An instantiated distrax distribution object.\n\n    \"\"\"\n\n    # Root-level nodes can be defined as instantiated distrax distributions.\n    if isinstance(self.distribution, Distribution):\n        return self.distribution\n\n    if minibatch is None:\n        minibatch = {}\n\n    # Otherwise the distribution is instantiated from the state.\n    parent_values = {}\n    for parent_name, parent_node in self.parents.items():\n        if parent_node in state:\n            parent_values[parent_name] = state[parent_node]\n        else:\n            if parent_name in minibatch:\n                parent_values[parent_name] = minibatch[parent_node]\n            else:\n                parent_values[parent_name] = self.parents[parent_name].observations\n\n    transformed_parents = self.link_fn(**parent_values)\n    if hasattr(self, 'bijector'):\n        return dx.Transformed(self.distribution(**transformed_parents), self.bijector)\n    else:\n        return self.distribution(**transformed_parents)\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model","title":"<code>Model</code>","text":"<p>A Bayesian model is represented as a directed acyclic graph, in which nodes are associated with random variables.</p> <p>Typical use:</p> <pre><code>model = Model('model name')\n_ = model.add_node('x', observations=...)\n</code></pre> Source code in <code>bamojax/base.py</code> <pre><code>class Model:\n    r\"\"\" A Bayesian model is represented as a directed acyclic graph, in which nodes are associated with random variables.\n\n    Typical use:\n\n        model = Model('model name')\n        _ = model.add_node('x', observations=...)\n\n    \"\"\"\n\n    def __init__(self, name='Bayesian model', verbose=False):\n        self.name = name\n        self.nodes = {}\n        self.root_nodes = list()\n        self.leaf_nodes = list()\n        self.children = dict()\n        self.parents = dict()\n        self.verbose = verbose\n\n    #\n    def add_node(self, \n                 name: str = 'root', \n                 distribution: Union[Distribution, Bijector] = None, \n                 observations: Array = None, \n                 parents: dict = None, \n                 link_fn: Callable = None,\n                 shape: Union[Tuple, int] = None,\n                 bijector: Bijector = None) -&gt; Node:\n        r\"\"\" Adds a node to the Bayesian model DAG\n\n        Args:\n          name: The name of the variable.\n          distribution: The distrax distribution of the variable given its (transformed) parents.\n          observations: If the node is observed; the actual observations.\n          parents: The nodes that this node depends on.\n          link_fn: A link function combining the inputs to form the input to the corresponding distrax distribution.\n          shape: The dimensions of the variable.\n          bijector: A bijector can be passed to transform variables.\n        Returns:\n          New node\n\n        \"\"\"\n        if parents is not None:\n            new_parents = {}\n            for parent_name, parent in parents.items():\n                if not isinstance(parent, Node):\n                    # Parent is numeric\n                    parent_node = self.add_node(name=f'{parent_name}_{name}', observations=parent)\n                    new_parents[parent_name] = parent_node\n                else:\n                    new_parents[parent_name] = parent\n            parents = new_parents\n        new_node = Node(name=name, distribution=distribution, observations=observations, parents=parents, link_fn=link_fn, shape=shape, bijector=bijector)\n        self.nodes[name] = new_node\n        if self.verbose: print(f'Adding node ({name})')\n        if parents is not None:\n            for parent in parents.values():\n                self.add_edge(parent, new_node)\n        if new_node.is_root():\n            self.root_nodes.append(new_node)\n        if new_node.is_leaf():\n            self.leaf_nodes.append(new_node)\n        return new_node\n\n    #    \n    def add_edge(self, from_node, to_node):\n        r\"\"\" Store the dependence between two nodes.\n\n        Args:\n            from_node: source node\n            to_node: target node\n\n        \"\"\"\n        if self.verbose: print(f'Add edge ({from_node}) -&gt; ({to_node})')\n        if not from_node in self.children:\n            self.children[from_node] = set()\n        self.children[from_node].add(to_node)\n        if not to_node in self.parents:\n            self.parents[to_node] = set()\n        self.parents[to_node].add(from_node)\n\n    #\n    def get_children(self, node):\n        \"\"\" Returns the children of a node.\n\n        \"\"\"\n        if node in self.children:\n            return self.children[node]\n        return []\n\n    #\n    def get_parents(self, node):\n        \"\"\" Returns the parents of a node.\n\n        \"\"\"\n        if node in self.parents:\n            return self.parents[node]\n        return []\n\n    #\n    def get_root_nodes(self):\n        \"\"\" Return all nodes that are roots.\n\n        \"\"\"\n        return self.root_nodes\n\n    #\n    def get_leaf_nodes(self):\n        \"\"\" Returns all nodes that are leaves.\n\n        \"\"\"\n        return {self.nodes[k]: self.nodes[k] for k in self.nodes.keys() - self.children.keys()}\n\n    #\n    def get_stochastic_nodes(self):\n        \"\"\" Returns all stochastic nodes.\n\n        \"\"\"\n        return {k: v for k, v in self.nodes.items() if v.is_stochastic()}\n\n    #\n    def get_latent_nodes(self):\n        \"\"\" Returns all latent nodes.\n\n        \"\"\"\n        return {k: v for k, v in self.nodes.items() if v.is_stochastic() and not v.is_observed()}\n\n    #\n    def logprior_fn(self) -&gt; Callable:\n        r\"\"\" Returns a callable function that provides the log prior of the model given the current state of assigned variables.\n\n        \"\"\"\n\n        def logprior_fn_(state) -&gt; float:\n            sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n            logprob = 0.0\n            for node in sorted_free_variables:\n                if node.is_root():\n                    logprob += jnp.sum(node.get_distribution().log_prob(state[node.name]))\n                else:\n                    logprob += jnp.sum(node.get_distribution(state).log_prob(state[node.name]))\n            return logprob\n\n        #\n        return logprior_fn_\n\n    #\n    def loglikelihood_fn(self) -&gt; Callable:\n        r\"\"\" Returns a callable function that provides the log likelihood of the model given the current state of assigned variables.\n\n        \"\"\"\n\n        def loglikelihood_fn_(state) -&gt; float:\n            logprob = 0.0\n            for node in self.get_leaf_nodes():\n                if '__mask' in state:\n                    element_wise_logp = node.get_distribution(state).log_prob(value=node.observations)\n                    logprob += jnp.sum(jnp.nan_to_num(element_wise_logp, nan=0.0)*state['__mask'])\n                else:\n                    logprob += jnp.sum(node.get_distribution(state).log_prob(value=node.observations))\n            return logprob\n\n        #\n        return loglikelihood_fn_\n\n    #\n    def batched_loglikelihood_fn(self) -&gt; Callable:\n        r\"\"\" Batched loglikelihood function for stochastic-gradient methods.\n\n        Assumes `minibatch` is a dictionary containing a subset of observations for each observed leaf node.\n\n        \"\"\"\n\n        def loglikelihood_fn_(state, minibatch) -&gt; float:\n            logprob = 0.0\n            for node in self.get_leaf_nodes():\n                logprob += jnp.sum(node.get_distribution(state, minibatch=minibatch).log_prob(value=minibatch[node]))\n            return logprob\n\n        #\n        return loglikelihood_fn_\n\n    #\n    def get_model_size(self) -&gt; int:\n        r\"\"\" Returns the total dimensions of the model. \n\n        As node.distribution can be abstract, we create a concrete instantiation by drawing a sample from the prior and deriving the shape from this sample.\n\n        \"\"\"\n\n        draw = self.sample_prior(key=jrnd.PRNGKey(0)) \n        size = jnp.sum(jnp.array([jnp.size(v) for v in draw.values()]))\n        return size\n\n    #  \n    def get_node_order(self):\n        r\"\"\" Returns the latent variables in topological order; child nodes are always listed after their parents.\n\n        \"\"\"\n        if not hasattr(self, 'node_order'):\n            self.node_order = self.__get_topological_order()\n        return self.node_order\n\n    #\n    def __get_topological_order(self) -&gt; list:\n        r\"\"\" Traverses the directed acyclic graph that defines the Bayesian model and returns its nodes in topological order\n\n        Returns:\n            A list of sorted nodes.\n\n        \"\"\"\n        def traverse_dag_backwards(node: Node, visited_: dict, order: list):\n            if node in visited_:\n                return\n            visited_[node] = 1\n\n            for parent_node in node.parents.values():\n                if parent_node.is_stochastic():\n                    traverse_dag_backwards(parent_node, visited_, order)\n\n            order.append(node)\n\n        #\n        order = []\n        visited = {}\n        leaves = self.get_leaf_nodes()\n        for leaf in leaves:\n            traverse_dag_backwards(leaf, visited, order)\n        return order\n\n    #\n    def sample_prior(self, key) -&gt; dict:\n        r\"\"\" Samples from the (hierarchical) prior distribution of the model.\n\n        Args:\n            key: Random seed\n        Returns:\n            A state dictionary with one random value for each node.\n\n        \"\"\"\n        state = dict()\n        sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n        for node in sorted_free_variables:\n            key, subkey = jrnd.split(key)\n            if node.is_root(): \n                dist = node.get_distribution()               \n            else:\n                dist = node.get_distribution(state)\n            state[node.name] = dist.sample(seed=subkey, sample_shape=node.shape)   \n        return state\n\n    #\n    def sample_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        r\"\"\" Sample stochastic observed nodes\n\n        Args:\n            key: PRNGKey\n            state: a draw from either $p(x)$ or $p(x \\mid \\cdot)$\n            input_variables: a dictionary with values for observed non-stochastic nodes\n\n        Returns:\n            A dictionary which is the same as 'state' but appended with sampled values.\n\n        \"\"\"\n\n        for node in self.get_leaf_nodes():\n            key, key_obs = jrnd.split(key)\n            state[node.name] = node.get_distribution(state, minibatch=input_variables).sample(seed=key_obs, sample_shape=node.shape)\n        return state\n\n\n    def sample_prior_predictive(self, key, prediction_options: dict = None) -&gt; dict:\n        r\"\"\" Sample from the (hierarchical) prior predictive distribution of the model.\n\n        Args:\n            key: Random seed\n            prediction_options: A dictionary of options which can include minibatched input variables\n\n        Returns:\n            A dictionary with a random value for all stochastic observed nodes.\n\n        \"\"\"\n        key, key_latent = jrnd.split(key)\n        state = self.sample_prior(key_latent)\n        return self.sample_predictive(key, state, prediction_options)\n\n    #\n    def sample_posterior_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        r\"\"\" Sample from the posterior predictive\n\n        Args:\n            key: Random key\n            state: A draw from the posterior\n            input_variables: Potential predictors and other non-stochastic observations\n\n        Returns:\n            A dictionary containing values for all stochastic observed nodes, conditioned on the observations.\n\n        \"\"\"\n\n        return self.sample_predictive(key, state, input_variables)\n\n    #\n    def print_gibbs(self):\n        r\"\"\" Print the structure of conditional distributions. \n\n\n        \"\"\"\n\n        print('Gibbs structure:')\n        sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n        for node in sorted_free_variables:\n            # get prior density        \n            if node.is_root():\n                prior = f'p({node})'\n            else:\n                parents = {p for p in self.get_parents(node)}\n                prior = f'p({node} | {\", \".join([p.name for p in parents])})'\n\n            # get conditional density\n            conditionals = []\n            children = [c for c in self.get_children(node)]        \n            for child in children:\n                co_parents = set()\n                for parent in self.get_parents(child):\n                    if not parent == node or parent in co_parents:\n                        co_parents.add(parent)        \n                co_parents.add(node)\n                conditional = f'p({child} | {\", \".join([str(p) for p in co_parents])})'\n                conditionals.append(conditional)\n\n            print(f'{str(node):20s}: {\" \".join(conditionals)} {prior}')\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.add_node","title":"<code>add_node(name='root', distribution=None, observations=None, parents=None, link_fn=None, shape=None, bijector=None)</code>","text":"<p>Adds a node to the Bayesian model DAG</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable.</p> <code>'root'</code> <code>distribution</code> <code>Union[Distribution, Bijector]</code> <p>The distrax distribution of the variable given its (transformed) parents.</p> <code>None</code> <code>observations</code> <code>Array</code> <p>If the node is observed; the actual observations.</p> <code>None</code> <code>parents</code> <code>dict</code> <p>The nodes that this node depends on.</p> <code>None</code> <code>link_fn</code> <code>Callable</code> <p>A link function combining the inputs to form the input to the corresponding distrax distribution.</p> <code>None</code> <code>shape</code> <code>Union[Tuple, int]</code> <p>The dimensions of the variable.</p> <code>None</code> <code>bijector</code> <code>Bijector</code> <p>A bijector can be passed to transform variables.</p> <code>None</code> <p>Returns:   New node</p> Source code in <code>bamojax/base.py</code> <pre><code>def add_node(self, \n             name: str = 'root', \n             distribution: Union[Distribution, Bijector] = None, \n             observations: Array = None, \n             parents: dict = None, \n             link_fn: Callable = None,\n             shape: Union[Tuple, int] = None,\n             bijector: Bijector = None) -&gt; Node:\n    r\"\"\" Adds a node to the Bayesian model DAG\n\n    Args:\n      name: The name of the variable.\n      distribution: The distrax distribution of the variable given its (transformed) parents.\n      observations: If the node is observed; the actual observations.\n      parents: The nodes that this node depends on.\n      link_fn: A link function combining the inputs to form the input to the corresponding distrax distribution.\n      shape: The dimensions of the variable.\n      bijector: A bijector can be passed to transform variables.\n    Returns:\n      New node\n\n    \"\"\"\n    if parents is not None:\n        new_parents = {}\n        for parent_name, parent in parents.items():\n            if not isinstance(parent, Node):\n                # Parent is numeric\n                parent_node = self.add_node(name=f'{parent_name}_{name}', observations=parent)\n                new_parents[parent_name] = parent_node\n            else:\n                new_parents[parent_name] = parent\n        parents = new_parents\n    new_node = Node(name=name, distribution=distribution, observations=observations, parents=parents, link_fn=link_fn, shape=shape, bijector=bijector)\n    self.nodes[name] = new_node\n    if self.verbose: print(f'Adding node ({name})')\n    if parents is not None:\n        for parent in parents.values():\n            self.add_edge(parent, new_node)\n    if new_node.is_root():\n        self.root_nodes.append(new_node)\n    if new_node.is_leaf():\n        self.leaf_nodes.append(new_node)\n    return new_node\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.add_edge","title":"<code>add_edge(from_node, to_node)</code>","text":"<p>Store the dependence between two nodes.</p> <p>Parameters:</p> Name Type Description Default <code>from_node</code> <p>source node</p> required <code>to_node</code> <p>target node</p> required Source code in <code>bamojax/base.py</code> <pre><code>def add_edge(self, from_node, to_node):\n    r\"\"\" Store the dependence between two nodes.\n\n    Args:\n        from_node: source node\n        to_node: target node\n\n    \"\"\"\n    if self.verbose: print(f'Add edge ({from_node}) -&gt; ({to_node})')\n    if not from_node in self.children:\n        self.children[from_node] = set()\n    self.children[from_node].add(to_node)\n    if not to_node in self.parents:\n        self.parents[to_node] = set()\n    self.parents[to_node].add(from_node)\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_children","title":"<code>get_children(node)</code>","text":"<p>Returns the children of a node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_children(self, node):\n    \"\"\" Returns the children of a node.\n\n    \"\"\"\n    if node in self.children:\n        return self.children[node]\n    return []\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_parents","title":"<code>get_parents(node)</code>","text":"<p>Returns the parents of a node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_parents(self, node):\n    \"\"\" Returns the parents of a node.\n\n    \"\"\"\n    if node in self.parents:\n        return self.parents[node]\n    return []\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_root_nodes","title":"<code>get_root_nodes()</code>","text":"<p>Return all nodes that are roots.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_root_nodes(self):\n    \"\"\" Return all nodes that are roots.\n\n    \"\"\"\n    return self.root_nodes\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_leaf_nodes","title":"<code>get_leaf_nodes()</code>","text":"<p>Returns all nodes that are leaves.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_leaf_nodes(self):\n    \"\"\" Returns all nodes that are leaves.\n\n    \"\"\"\n    return {self.nodes[k]: self.nodes[k] for k in self.nodes.keys() - self.children.keys()}\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_stochastic_nodes","title":"<code>get_stochastic_nodes()</code>","text":"<p>Returns all stochastic nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_stochastic_nodes(self):\n    \"\"\" Returns all stochastic nodes.\n\n    \"\"\"\n    return {k: v for k, v in self.nodes.items() if v.is_stochastic()}\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_latent_nodes","title":"<code>get_latent_nodes()</code>","text":"<p>Returns all latent nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_latent_nodes(self):\n    \"\"\" Returns all latent nodes.\n\n    \"\"\"\n    return {k: v for k, v in self.nodes.items() if v.is_stochastic() and not v.is_observed()}\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.logprior_fn","title":"<code>logprior_fn()</code>","text":"<p>Returns a callable function that provides the log prior of the model given the current state of assigned variables.</p> Source code in <code>bamojax/base.py</code> <pre><code>def logprior_fn(self) -&gt; Callable:\n    r\"\"\" Returns a callable function that provides the log prior of the model given the current state of assigned variables.\n\n    \"\"\"\n\n    def logprior_fn_(state) -&gt; float:\n        sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n        logprob = 0.0\n        for node in sorted_free_variables:\n            if node.is_root():\n                logprob += jnp.sum(node.get_distribution().log_prob(state[node.name]))\n            else:\n                logprob += jnp.sum(node.get_distribution(state).log_prob(state[node.name]))\n        return logprob\n\n    #\n    return logprior_fn_\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.loglikelihood_fn","title":"<code>loglikelihood_fn()</code>","text":"<p>Returns a callable function that provides the log likelihood of the model given the current state of assigned variables.</p> Source code in <code>bamojax/base.py</code> <pre><code>def loglikelihood_fn(self) -&gt; Callable:\n    r\"\"\" Returns a callable function that provides the log likelihood of the model given the current state of assigned variables.\n\n    \"\"\"\n\n    def loglikelihood_fn_(state) -&gt; float:\n        logprob = 0.0\n        for node in self.get_leaf_nodes():\n            if '__mask' in state:\n                element_wise_logp = node.get_distribution(state).log_prob(value=node.observations)\n                logprob += jnp.sum(jnp.nan_to_num(element_wise_logp, nan=0.0)*state['__mask'])\n            else:\n                logprob += jnp.sum(node.get_distribution(state).log_prob(value=node.observations))\n        return logprob\n\n    #\n    return loglikelihood_fn_\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.batched_loglikelihood_fn","title":"<code>batched_loglikelihood_fn()</code>","text":"<p>Batched loglikelihood function for stochastic-gradient methods.</p> <p>Assumes <code>minibatch</code> is a dictionary containing a subset of observations for each observed leaf node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def batched_loglikelihood_fn(self) -&gt; Callable:\n    r\"\"\" Batched loglikelihood function for stochastic-gradient methods.\n\n    Assumes `minibatch` is a dictionary containing a subset of observations for each observed leaf node.\n\n    \"\"\"\n\n    def loglikelihood_fn_(state, minibatch) -&gt; float:\n        logprob = 0.0\n        for node in self.get_leaf_nodes():\n            logprob += jnp.sum(node.get_distribution(state, minibatch=minibatch).log_prob(value=minibatch[node]))\n        return logprob\n\n    #\n    return loglikelihood_fn_\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_model_size","title":"<code>get_model_size()</code>","text":"<p>Returns the total dimensions of the model. </p> <p>As node.distribution can be abstract, we create a concrete instantiation by drawing a sample from the prior and deriving the shape from this sample.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_model_size(self) -&gt; int:\n    r\"\"\" Returns the total dimensions of the model. \n\n    As node.distribution can be abstract, we create a concrete instantiation by drawing a sample from the prior and deriving the shape from this sample.\n\n    \"\"\"\n\n    draw = self.sample_prior(key=jrnd.PRNGKey(0)) \n    size = jnp.sum(jnp.array([jnp.size(v) for v in draw.values()]))\n    return size\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.get_node_order","title":"<code>get_node_order()</code>","text":"<p>Returns the latent variables in topological order; child nodes are always listed after their parents.</p> Source code in <code>bamojax/base.py</code> <pre><code>def get_node_order(self):\n    r\"\"\" Returns the latent variables in topological order; child nodes are always listed after their parents.\n\n    \"\"\"\n    if not hasattr(self, 'node_order'):\n        self.node_order = self.__get_topological_order()\n    return self.node_order\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.sample_prior","title":"<code>sample_prior(key)</code>","text":"<p>Samples from the (hierarchical) prior distribution of the model.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random seed</p> required <p>Returns:     A state dictionary with one random value for each node.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_prior(self, key) -&gt; dict:\n    r\"\"\" Samples from the (hierarchical) prior distribution of the model.\n\n    Args:\n        key: Random seed\n    Returns:\n        A state dictionary with one random value for each node.\n\n    \"\"\"\n    state = dict()\n    sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n    for node in sorted_free_variables:\n        key, subkey = jrnd.split(key)\n        if node.is_root(): \n            dist = node.get_distribution()               \n        else:\n            dist = node.get_distribution(state)\n        state[node.name] = dist.sample(seed=subkey, sample_shape=node.shape)   \n    return state\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.sample_predictive","title":"<code>sample_predictive(key, state, input_variables=None)</code>","text":"<p>Sample stochastic observed nodes</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>PRNGKey</p> required <code>state</code> <code>dict</code> <p>a draw from either \\(p(x)\\) or \\(p(x \\mid \\cdot)\\)</p> required <code>input_variables</code> <code>dict</code> <p>a dictionary with values for observed non-stochastic nodes</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary which is the same as 'state' but appended with sampled values.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n    r\"\"\" Sample stochastic observed nodes\n\n    Args:\n        key: PRNGKey\n        state: a draw from either $p(x)$ or $p(x \\mid \\cdot)$\n        input_variables: a dictionary with values for observed non-stochastic nodes\n\n    Returns:\n        A dictionary which is the same as 'state' but appended with sampled values.\n\n    \"\"\"\n\n    for node in self.get_leaf_nodes():\n        key, key_obs = jrnd.split(key)\n        state[node.name] = node.get_distribution(state, minibatch=input_variables).sample(seed=key_obs, sample_shape=node.shape)\n    return state\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.sample_prior_predictive","title":"<code>sample_prior_predictive(key, prediction_options=None)</code>","text":"<p>Sample from the (hierarchical) prior predictive distribution of the model.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random seed</p> required <code>prediction_options</code> <code>dict</code> <p>A dictionary of options which can include minibatched input variables</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with a random value for all stochastic observed nodes.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_prior_predictive(self, key, prediction_options: dict = None) -&gt; dict:\n    r\"\"\" Sample from the (hierarchical) prior predictive distribution of the model.\n\n    Args:\n        key: Random seed\n        prediction_options: A dictionary of options which can include minibatched input variables\n\n    Returns:\n        A dictionary with a random value for all stochastic observed nodes.\n\n    \"\"\"\n    key, key_latent = jrnd.split(key)\n    state = self.sample_prior(key_latent)\n    return self.sample_predictive(key, state, prediction_options)\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.sample_posterior_predictive","title":"<code>sample_posterior_predictive(key, state, input_variables=None)</code>","text":"<p>Sample from the posterior predictive</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random key</p> required <code>state</code> <code>dict</code> <p>A draw from the posterior</p> required <code>input_variables</code> <code>dict</code> <p>Potential predictors and other non-stochastic observations</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing values for all stochastic observed nodes, conditioned on the observations.</p> Source code in <code>bamojax/base.py</code> <pre><code>def sample_posterior_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n    r\"\"\" Sample from the posterior predictive\n\n    Args:\n        key: Random key\n        state: A draw from the posterior\n        input_variables: Potential predictors and other non-stochastic observations\n\n    Returns:\n        A dictionary containing values for all stochastic observed nodes, conditioned on the observations.\n\n    \"\"\"\n\n    return self.sample_predictive(key, state, input_variables)\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.Model.print_gibbs","title":"<code>print_gibbs()</code>","text":"<p>Print the structure of conditional distributions.</p> Source code in <code>bamojax/base.py</code> <pre><code>def print_gibbs(self):\n    r\"\"\" Print the structure of conditional distributions. \n\n\n    \"\"\"\n\n    print('Gibbs structure:')\n    sorted_free_variables = [node for node in self.get_node_order() if node.is_stochastic() and not node.is_observed()]\n    for node in sorted_free_variables:\n        # get prior density        \n        if node.is_root():\n            prior = f'p({node})'\n        else:\n            parents = {p for p in self.get_parents(node)}\n            prior = f'p({node} | {\", \".join([p.name for p in parents])})'\n\n        # get conditional density\n        conditionals = []\n        children = [c for c in self.get_children(node)]        \n        for child in children:\n            co_parents = set()\n            for parent in self.get_parents(child):\n                if not parent == node or parent in co_parents:\n                    co_parents.add(parent)        \n            co_parents.add(node)\n            conditional = f'p({child} | {\", \".join([str(p) for p in co_parents])})'\n            conditionals.append(conditional)\n\n        print(f'{str(node):20s}: {\" \".join(conditionals)} {prior}')\n</code></pre>"},{"location":"api/bamojax.base/#bamojax.base.MetaModel","title":"<code>MetaModel</code>","text":"<p>A meta-model is a collection of Bayesian models, which can be used for reversible jump MCMC.</p> Source code in <code>bamojax/base.py</code> <pre><code>class MetaModel():\n    r\"\"\" A meta-model is a collection of Bayesian models, which can be used for reversible jump MCMC.\n    \"\"\"\n\n    def __init__(self, \n                 model_list):\n        self.model_list = model_list\n        self.M = len(model_list)\n        self.model_sizes = [model.get_model_size() for model in model_list]        \n        self.indiv_latent_nodes = [set(model.get_latent_nodes().keys()) for model in model_list]\n\n        # find the number of auxiliary variables needed, as the largest difference in latent node sets between any two models\n        self.num_auxiliary = 0\n        for i in range(self.M):\n            for j in range(i+1, self.M):\n                diff_left = set.difference(self.indiv_latent_nodes[i], self.indiv_latent_nodes[j])\n                diff_right = set.difference(self.indiv_latent_nodes[j], self.indiv_latent_nodes[i])\n                max_diff = jnp.max(jnp.array([len(diff_left), len(diff_right)]))\n                if max_diff &gt; self.num_auxiliary:\n                    self.num_auxiliary = max_diff\n\n        self.latent_variables = set.union(*self.indiv_latent_nodes)\n        self.auxiliary_variables = [f'u_{i}' for i in range(self.num_auxiliary)]\n        self.meta_state = self.latent_variables.union(set(self.auxiliary_variables))\n\n        def make_model_sample_prior_fn(model_index):\n            \"\"\"Creates a function that samples from the prior of the specified model index.\"\"\"\n            def fn(key):\n                sample = self.model_list[model_index].sample_prior(key)\n                all_latents = {k: jnp.nan for k in self.latent_variables}\n                return {**all_latents, **sample}  \n\n            #   \n            return fn\n        #\n        self.model_sample_prior_fns = [make_model_sample_prior_fn(i) for i in range(self.M)]\n\n        def make_model_sample_predictive_fn(model_index):\n            \"\"\"Creates a function that samples from the predictive distribution of the specified model index.\"\"\"\n            def fn(input):\n                key, state, input_variables = input\n                return self.model_list[model_index].sample_predictive(key, state, input_variables)\n\n            #   \n            return fn\n\n        #\n        self.model_sample_predictive_fns = [make_model_sample_predictive_fn(i) for i in range(self.M)]\n\n        def make_logprior_fn(model_index):\n            \"\"\"Creates a function that computes the log prior of the specified model index.\"\"\"\n            def fn(state):\n                return self.model_list[model_index].logprior_fn()(state)\n\n            #   \n            return fn\n\n        #\n        self.model_logprior_fns = [make_logprior_fn(i) for i in range(self.M)]\n\n        def make_loglikelihood_fn(model_index):\n            \"\"\"Creates a function that computes the log likelihood of the specified model index.\"\"\"\n            def fn(state):\n                return self.model_list[model_index].loglikelihood_fn()(state)\n\n            #   \n            return fn\n\n        #\n        self.model_loglikelihood_fns = [make_loglikelihood_fn(i) for i in range(self.M)]\n\n    #\n    def sample_prior(self, key) -&gt; dict:\n        key_model, key_sample = jrnd.split(key)\n        model_index = jrnd.randint(key_model, shape=(), minval=0, maxval=self.M)\n        sample = jax.lax.switch(model_index, self.model_sample_prior_fns, operand=key_sample)\n        auxiliary_values = {f'u_{i}': jnp.nan for i in range(self.num_auxiliary)}  # to keep the same pytree structure across models in reversible jump MCMC\n        return {'model_index': model_index, **sample, **auxiliary_values}\n\n    #\n    def sample_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        return jax.lax.switch(state['model_index'], self.model_sample_predictive_fns, \n                              operand=[key, state, input_variables])\n\n    #\n    def sample_prior_predictive(self, key, **input_variables) -&gt; dict:\n        key_prior, key_predictive = jrnd.split(key)\n        prior_sample = self.sample_prior(key_prior)\n        return self.sample_predictive(key_predictive, prior_sample, input_variables=input_variables)\n\n    #\n    def sample_posterior_predictive(self, key, state: dict, input_variables: dict = None) -&gt; dict:\n        return self.sample_predictive(key, state, input_variables=input_variables)\n\n    #\n    def logprior_fn(self) -&gt; Callable:\n\n        def fn(state: dict) -&gt; float:\n            \"\"\"Computes the log prior of the state.\"\"\"\n            model_index = state['model_index']\n            return jax.lax.switch(model_index, self.model_logprior_fns, operand=state)\n\n        #\n        return fn\n\n    #\n    def loglikelihood_fn(self) -&gt; Callable:\n\n        def fn(state: dict) -&gt; float:   \n            \"\"\"Computes the log likelihood of the state.\"\"\"\n            model_index = state['model_index']\n            return jax.lax.switch(model_index, self.model_loglikelihood_fns, operand=state)\n        #\n        return fn   \n</code></pre>"},{"location":"api/bamojax.inference/","title":"<code>bamojax.inference</code>","text":""},{"location":"api/bamojax.inference/#bamojax.inference.InferenceEngine","title":"<code>InferenceEngine</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class for inference engines</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>the Bayesian model for which to run the approximate inference</p> <code>num_chains</code> <p>the number of parallel, independent chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>class InferenceEngine(ABC):\n    r\"\"\" The abstract class for inference engines\n\n    Attributes:\n        model: the Bayesian model for which to run the approximate inference\n        num_chains: the number of parallel, independent chains\n\n    \"\"\"\n\n    def __init__(self, model: Model, num_chains: int = 1):\n        self.model = model\n        self.num_chains = num_chains\n\n    #\n    @abstractmethod\n    def run_single_chain(self, key: PRNGKey):\n        pass\n\n    #\n    def run(self, key):\n        r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n        \"\"\"\n        if self.num_chains &gt; 1:\n            keys = jrnd.split(key, self.num_chains)\n            return jax.vmap(self.run_single_chain)(keys)\n        return self.run_single_chain(key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.InferenceEngine.run","title":"<code>run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.MCMCInference","title":"<code>MCMCInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The MCMC approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class MCMCInference(InferenceEngine):\n    r\"\"\" The MCMC approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model, \n                 num_chains: int = 1, \n                 mcmc_kernel: SamplingAlgorithm = None, \n                 num_samples: int = 10_000, \n                 num_burn: int = 10_000, \n                 num_warmup: int = 0,\n                 num_thin: int = 1, \n                 return_diagnostics: bool = True):\n        super().__init__(model, num_chains)\n        self.mcmc_kernel = mcmc_kernel\n        self.num_samples = num_samples\n        self.num_burn = num_burn\n        self.num_warmup = num_warmup\n        self.num_thin = num_thin\n        self.return_diagnostics = return_diagnostics\n\n    #    \n    def dense_step(self, key, state):\n        \"\"\" Take a MCMC step.\n\n        Can be a composite step to reduce autocorrelation and memory consumption (thinning). Rather than\n        post-hoc removing samples, a dense step can take N steps, then return only the final state, so \n        that we effective thin by a factor N.\n\n        Args:\n            key: Random seed\n            state: The current state\n        Returns:\n            A new state\n            (optional) Diagnostic information, depending on the `return_diagnostics` attribute\n\n        \"\"\"\n        @jax.jit\n        def one_step_fn(state, key):\n            state, info = self.mcmc_kernel.step(key, state)\n            return state, info\n\n        #\n        @jax.jit\n        def one_step_state_only_fn(state, key):\n            state, _ = self.mcmc_kernel.step(key, state)\n            return state, None\n\n        #\n        conditional_step = one_step_fn if self.return_diagnostics else one_step_state_only_fn\n\n        if self.num_thin &gt; 1:\n            keys = jrnd.split(key, self.num_thin)\n            state, infos = jax.lax.scan(conditional_step, state, keys)\n            info = tree_map(lambda x: x[-1, ...], infos)\n            return state, info if self.return_diagnostics else (state, None)\n\n        return conditional_step(state, key)\n\n    #\n    def run_single_chain(self, key: PRNGKey) -&gt; dict:\n        r\"\"\" Run one MCMC chain\n\n        Depending on different preferences, this \n            - optimizes the MCMC kernel hyperparameters\n            - runs `num_burn` burn-in samples, that are discarded\n            - performs the actual sampling for `num_samples` / `num_thin` dense steps\n\n        Args:\n            key: Random seed\n\n        Returns:\n            A dictionary containing the resulting collection of states, and optional diagnostic information\n\n        \"\"\"\n        def mcmc_body_fn(state, key):\n            state, info = self.dense_step(key, state)\n            return state, (state, info) \n\n        #\n        key, key_init = jrnd.split(key)        \n\n        if self.num_warmup &gt; 0:\n            print('Adapting NUTS HMC parameters...', end=\" \")\n            warm_state, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup) \n            print('done.')\n            adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n            self.mcmc_kernel = adapted_kernel\n            initial_state = self.mcmc_kernel.init(warm_state.position)\n        else:\n            initial_state = self.mcmc_kernel.init(self.model.sample_prior(key_init))        \n\n        if self.num_burn &gt; 0:\n            num_dense_burn_steps = int(self.num_burn / self.num_thin)\n            key, key_burn = jrnd.split(key)\n            keys = jrnd.split(key_burn, num_dense_burn_steps)\n            initial_state, _ = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n        num_dense_steps = int(self.num_samples / self.num_thin)\n        key, key_inference = jrnd.split(key)\n        keys = jrnd.split(key_inference, num_dense_steps)\n        _, (states, info) = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n        return dict(states=states.position, info=info) if self.return_diagnostics else dict(states=states.position)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.MCMCInference.dense_step","title":"<code>dense_step(key, state)</code>","text":"<p>Take a MCMC step.</p> <p>Can be a composite step to reduce autocorrelation and memory consumption (thinning). Rather than post-hoc removing samples, a dense step can take N steps, then return only the final state, so  that we effective thin by a factor N.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Random seed</p> required <code>state</code> <p>The current state</p> required <p>Returns:     A new state     (optional) Diagnostic information, depending on the <code>return_diagnostics</code> attribute</p> Source code in <code>bamojax/inference.py</code> <pre><code>def dense_step(self, key, state):\n    \"\"\" Take a MCMC step.\n\n    Can be a composite step to reduce autocorrelation and memory consumption (thinning). Rather than\n    post-hoc removing samples, a dense step can take N steps, then return only the final state, so \n    that we effective thin by a factor N.\n\n    Args:\n        key: Random seed\n        state: The current state\n    Returns:\n        A new state\n        (optional) Diagnostic information, depending on the `return_diagnostics` attribute\n\n    \"\"\"\n    @jax.jit\n    def one_step_fn(state, key):\n        state, info = self.mcmc_kernel.step(key, state)\n        return state, info\n\n    #\n    @jax.jit\n    def one_step_state_only_fn(state, key):\n        state, _ = self.mcmc_kernel.step(key, state)\n        return state, None\n\n    #\n    conditional_step = one_step_fn if self.return_diagnostics else one_step_state_only_fn\n\n    if self.num_thin &gt; 1:\n        keys = jrnd.split(key, self.num_thin)\n        state, infos = jax.lax.scan(conditional_step, state, keys)\n        info = tree_map(lambda x: x[-1, ...], infos)\n        return state, info if self.return_diagnostics else (state, None)\n\n    return conditional_step(state, key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.MCMCInference.run_single_chain","title":"<code>run_single_chain(key)</code>","text":"<p>Run one MCMC chain</p> <p>Depending on different preferences, this      - optimizes the MCMC kernel hyperparameters     - runs <code>num_burn</code> burn-in samples, that are discarded     - performs the actual sampling for <code>num_samples</code> / <code>num_thin</code> dense steps</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>Random seed</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the resulting collection of states, and optional diagnostic information</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey) -&gt; dict:\n    r\"\"\" Run one MCMC chain\n\n    Depending on different preferences, this \n        - optimizes the MCMC kernel hyperparameters\n        - runs `num_burn` burn-in samples, that are discarded\n        - performs the actual sampling for `num_samples` / `num_thin` dense steps\n\n    Args:\n        key: Random seed\n\n    Returns:\n        A dictionary containing the resulting collection of states, and optional diagnostic information\n\n    \"\"\"\n    def mcmc_body_fn(state, key):\n        state, info = self.dense_step(key, state)\n        return state, (state, info) \n\n    #\n    key, key_init = jrnd.split(key)        \n\n    if self.num_warmup &gt; 0:\n        print('Adapting NUTS HMC parameters...', end=\" \")\n        warm_state, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup) \n        print('done.')\n        adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n        self.mcmc_kernel = adapted_kernel\n        initial_state = self.mcmc_kernel.init(warm_state.position)\n    else:\n        initial_state = self.mcmc_kernel.init(self.model.sample_prior(key_init))        \n\n    if self.num_burn &gt; 0:\n        num_dense_burn_steps = int(self.num_burn / self.num_thin)\n        key, key_burn = jrnd.split(key)\n        keys = jrnd.split(key_burn, num_dense_burn_steps)\n        initial_state, _ = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n    num_dense_steps = int(self.num_samples / self.num_thin)\n    key, key_inference = jrnd.split(key)\n    keys = jrnd.split(key_inference, num_dense_steps)\n    _, (states, info) = jax.lax.scan(mcmc_body_fn, initial_state, keys)\n\n    return dict(states=states.position, info=info) if self.return_diagnostics else dict(states=states.position)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.MCMCInference.run","title":"<code>run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SGMCMCInference","title":"<code>SGMCMCInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The Stochastic Gradient MCMC approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class SGMCMCInference(InferenceEngine):\n    r\"\"\" The Stochastic Gradient MCMC approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,                   \n                 sgmcmc_kernel: SamplingAlgorithm, \n                 data_size: int, \n                 batch_size: int, \n                 stepsize: float, \n                 batch_nodes: list, \n                 num_chains: int = 1,\n                 num_samples: int = 10_000, \n                 num_burn: int = 10_000, \n                 num_thin: int = 1,  \n                 sgmcmc_params: dict = None):\n        assert batch_size &lt; data_size, f'Batch size must be smaller than data set size, but found batch size: {batch_size} and data size: {data_size}.'\n        super().__init__(model, num_chains)\n\n        if sgmcmc_params is None:\n            sgmcmc_params = {}\n\n        self.num_samples = num_samples\n        self.num_burn = num_burn\n        self.num_thin = num_thin\n\n        self.sgmcmc_params = sgmcmc_params\n        self.data_size = data_size\n        self.batch_size = batch_size\n        self.batch_nodes = batch_nodes\n        self.stepsize = stepsize\n        self.grad_fn = self.grad_estimator()\n        self.sgmcmc_kernel = sgmcmc_kernel(self.grad_fn, **sgmcmc_params)\n\n    #\n    def grad_estimator(self) -&gt; Callable:\n        \"\"\" The stochastic gradient estimator\n\n        Build a simple estimator for the gradient of the log-density, assuming data is mini-batched.\n\n        Returns:\n            The gradient of the batched log-density        \n\n        \"\"\"\n\n        logprior_fn = self.model.logprior_fn()\n        loglikelihood_fn = self.model.batched_loglikelihood_fn()\n\n        def logdensity_estimator_fn(position, minibatch):\n            return logprior_fn(position) + self.data_size / self.batch_size * loglikelihood_fn(position, minibatch)\n\n        #\n        return jax.grad(logdensity_estimator_fn)\n\n    #\n    def get_minibatch(self, data, indices):\n        r\"\"\" Slice a minibatch of data\n\n        Args:\n            data: the data array\n            indices: a set of indices into `data` \n\n        Returns:\n            Dynamic slicing of data[indices, ...]\n\n        \"\"\"\n        if jnp.ndim(data) == 1:\n            data = data[:, jnp.newaxis]    \n        slice_size = (1,) + data.shape[1:]  # For (N, p), this will be (1, p)    \n        return jnp.squeeze(jax.vmap(lambda i: jax.lax.dynamic_slice(data, (i,) + (0,) * (data.ndim - 1), slice_size))(indices))\n\n    #\n    def one_step(self, state, key):\n        r\"\"\" Take one step of stochastic gradient MCMC\n\n        Args:\n            state: Current state\n            key: Random seed\n\n        Returns:\n            The updated state\n\n        \"\"\"\n        key_sgmcmc, key_batch = jrnd.split(key)\n        idx = jrnd.choice(key_batch, self.data_size, shape=(self.batch_size, ), replace=False)\n        minibatch = {node.name: self.get_minibatch(node.observations, idx) for node in self.batch_nodes}\n        state = self.sgmcmc_kernel.step(key_sgmcmc, state, minibatch, self.stepsize)   \n        return state, state\n\n    #\n    def run_single_chain(self, key: PRNGKey):\n        r\"\"\" Run Stochastic Gradient MCMC\n\n        Args: \n            key: PRNGKey\n        Returns:\n            A dictionary of samples for each variable in the Bayesian model.\n\n        \"\"\"\n\n        key, key_init = jrnd.split(key)\n        initial_state = self.sgmcmc_kernel.init(self.model.sample_prior(key_init))\n        step_fn = jax.jit(self.one_step)\n\n        if self.num_burn &gt; 0:\n            key, key_burn = jrnd.split(key)\n            keys = jrnd.split(key_burn, self.num_burn)\n            initial_state, _ = jax.lax.scan(step_fn, initial_state, keys)\n\n        keys = jrnd.split(key, self.num_samples)\n        _, states = jax.lax.scan(step_fn, initial_state, keys)\n\n        return dict(states=states)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SGMCMCInference.grad_estimator","title":"<code>grad_estimator()</code>","text":"<p>The stochastic gradient estimator</p> <p>Build a simple estimator for the gradient of the log-density, assuming data is mini-batched.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>The gradient of the batched log-density</p> Source code in <code>bamojax/inference.py</code> <pre><code>def grad_estimator(self) -&gt; Callable:\n    \"\"\" The stochastic gradient estimator\n\n    Build a simple estimator for the gradient of the log-density, assuming data is mini-batched.\n\n    Returns:\n        The gradient of the batched log-density        \n\n    \"\"\"\n\n    logprior_fn = self.model.logprior_fn()\n    loglikelihood_fn = self.model.batched_loglikelihood_fn()\n\n    def logdensity_estimator_fn(position, minibatch):\n        return logprior_fn(position) + self.data_size / self.batch_size * loglikelihood_fn(position, minibatch)\n\n    #\n    return jax.grad(logdensity_estimator_fn)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SGMCMCInference.get_minibatch","title":"<code>get_minibatch(data, indices)</code>","text":"<p>Slice a minibatch of data</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>the data array</p> required <code>indices</code> <p>a set of indices into <code>data</code> </p> required <p>Returns:</p> Type Description <p>Dynamic slicing of data[indices, ...]</p> Source code in <code>bamojax/inference.py</code> <pre><code>def get_minibatch(self, data, indices):\n    r\"\"\" Slice a minibatch of data\n\n    Args:\n        data: the data array\n        indices: a set of indices into `data` \n\n    Returns:\n        Dynamic slicing of data[indices, ...]\n\n    \"\"\"\n    if jnp.ndim(data) == 1:\n        data = data[:, jnp.newaxis]    \n    slice_size = (1,) + data.shape[1:]  # For (N, p), this will be (1, p)    \n    return jnp.squeeze(jax.vmap(lambda i: jax.lax.dynamic_slice(data, (i,) + (0,) * (data.ndim - 1), slice_size))(indices))\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SGMCMCInference.one_step","title":"<code>one_step(state, key)</code>","text":"<p>Take one step of stochastic gradient MCMC</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <p>Current state</p> required <code>key</code> <p>Random seed</p> required <p>Returns:</p> Type Description <p>The updated state</p> Source code in <code>bamojax/inference.py</code> <pre><code>def one_step(self, state, key):\n    r\"\"\" Take one step of stochastic gradient MCMC\n\n    Args:\n        state: Current state\n        key: Random seed\n\n    Returns:\n        The updated state\n\n    \"\"\"\n    key_sgmcmc, key_batch = jrnd.split(key)\n    idx = jrnd.choice(key_batch, self.data_size, shape=(self.batch_size, ), replace=False)\n    minibatch = {node.name: self.get_minibatch(node.observations, idx) for node in self.batch_nodes}\n    state = self.sgmcmc_kernel.step(key_sgmcmc, state, minibatch, self.stepsize)   \n    return state, state\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SGMCMCInference.run_single_chain","title":"<code>run_single_chain(key)</code>","text":"<p>Run Stochastic Gradient MCMC</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <p>Returns:     A dictionary of samples for each variable in the Bayesian model.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey):\n    r\"\"\" Run Stochastic Gradient MCMC\n\n    Args: \n        key: PRNGKey\n    Returns:\n        A dictionary of samples for each variable in the Bayesian model.\n\n    \"\"\"\n\n    key, key_init = jrnd.split(key)\n    initial_state = self.sgmcmc_kernel.init(self.model.sample_prior(key_init))\n    step_fn = jax.jit(self.one_step)\n\n    if self.num_burn &gt; 0:\n        key, key_burn = jrnd.split(key)\n        keys = jrnd.split(key_burn, self.num_burn)\n        initial_state, _ = jax.lax.scan(step_fn, initial_state, keys)\n\n    keys = jrnd.split(key, self.num_samples)\n    _, states = jax.lax.scan(step_fn, initial_state, keys)\n\n    return dict(states=states)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SGMCMCInference.run","title":"<code>run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SMCInference","title":"<code>SMCInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The SMC approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class SMCInference(InferenceEngine):\n    r\"\"\" The SMC approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,                   \n                 mcmc_kernel: SamplingAlgorithm, \n                 num_particles: int, \n                 num_mutations: int, \n                 num_chains: int = 1,\n                 mcmc_parameters: dict = None,\n                 resampling_fn = systematic, \n                 target_ess: float = 0.5, \n                 return_trace: bool = False, \n                 return_diagnostics: bool = True,\n                 num_warmup: int = 0,\n                 max_iter: int = 40):\n        super().__init__(model, num_chains)\n        self.mcmc_kernel = mcmc_kernel\n        if mcmc_parameters is None:\n            mcmc_parameters = {}\n        self.mcmc_parameters = mcmc_parameters\n        self.num_particles = num_particles\n        self.num_mutations = num_mutations\n        self.resampling_fn = resampling_fn\n        self.target_ess = target_ess\n        self.return_trace = return_trace\n        self.return_diagnostics = return_diagnostics\n        self.num_warmup = num_warmup\n        self.max_iter = max_iter\n        assert not (return_diagnostics and return_trace), 'Returning both the trace and diagnostics is not supported.'\n\n    #\n    def create_smc_kernel(self) -&gt; SamplingAlgorithm:\n        r\"\"\" Creates an SMC kernel\n\n        Currently a lightweight wrapper around the blackjax adaptive-tempered SMC object.\n\n        Returns:\n            A blackjax SMC `SamplingAlgorithm`\n\n        \"\"\"\n        return adaptive_tempered_smc(logprior_fn=self.model.logprior_fn(),\n                                     loglikelihood_fn=self.model.loglikelihood_fn(),\n                                     mcmc_step_fn=self.mcmc_kernel.step,\n                                     mcmc_init_fn=self.mcmc_kernel.init,\n                                     mcmc_parameters=self.mcmc_parameters,\n                                     resampling_fn=self.resampling_fn,\n                                     target_ess=self.target_ess,\n                                     num_mcmc_steps=self.num_mutations)\n\n    #    \n    def tempering_condition(self):\n        r\"\"\" Checks whether the SMC procedure terminates.\n\n        Returns:\n            A boolean deciding on whether the SMC tempering procedure is finished\n\n        \"\"\"\n        def cond(carry):\n            i, state, *_ = carry\n            if self.return_trace:\n                return jnp.logical_and(state.lmbda &lt; 1, i &lt; self.max_iter)\n            return state.lmbda &lt; 1\n\n        #\n        return cond\n\n    #\n    def smc_cycle(self, smc_kernel) -&gt; Callable:\n        r\"\"\" One iteration of the adaptive-tempered SMC algorithm.\n\n        Args:\n            smc_kernel: A Blackjax SamplingAlgorithm containing the SMC logic.\n        Returns:\n            A Callable step function that performs one iteration.\n\n        \"\"\"\n        @jax.jit\n        def one_step(carry): \n            if self.return_trace:\n                i, state, k, curr_log_likelihood, state_hist = carry\n            else:               \n                if self.return_diagnostics:\n                    i, state, k, curr_log_likelihood, _ = carry \n                else:\n                    i, state, k, curr_log_likelihood = carry \n            k, subk = jrnd.split(k)\n            state, info = smc_kernel.step(subk, state)    \n            base_return_tuple = (i + 1, state, k, curr_log_likelihood + info.log_likelihood_increment)\n            if self.return_trace:\n                state_hist = tree_map(lambda arr, val: arr.at[i].set(val), state_hist, state)\n                return base_return_tuple + (state_hist, )            \n\n            return base_return_tuple + (info, ) if self.return_diagnostics else base_return_tuple\n\n        #\n        return one_step\n\n    #\n\n    def run_single_chain(self, key: PRNGKey) -&gt; dict:\n        r\"\"\" Run one chain of Sequential Monte Carlo.\n\n        This returns SMC particles in different ways, depending on user provided settings.\n\n        Args:\n            key: PRNGKey\n        Returns:\n            A dictionary with the final SMC state, the number of iterations, the log marginal likelihood\n            (Optional) diagnostics and the trace of particles over each tempering step\n\n        \"\"\"\n\n        if self.num_warmup &gt; 0:\n            key, key_init = jrnd.split(key)\n            print('Adapting NUTS HMC parameters...', end=\" \")\n            _, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup)\n            print('done.')\n            adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n            self.mcmc_kernel = adapted_kernel\n\n        smc_kernel = self.create_smc_kernel()\n        smc_cycle = self.smc_cycle(smc_kernel)\n        cond = self.tempering_condition()\n\n        key, key_init = jrnd.split(key)        \n        keys = jrnd.split(key_init, self.num_particles)\n        initial_particles = smc_kernel.init(jax.vmap(self.model.sample_prior)(keys))\n\n        if self.return_trace or self.return_diagnostics:\n            # Call SMC once to determine PyTree structures\n            key_smc, key_init = jrnd.split(key)\n            initial_particles, sample_info = smc_kernel.step(key_init, initial_particles)\n            initial_info = tree_map(lambda x: jax.numpy.zeros_like(x), sample_info)\n            initial_log_likelihood = sample_info.log_likelihood_increment\n\n        if self.return_trace:\n             # Preallocate arrays for state and info history\n            trace = tree_map(lambda x: jnp.zeros((self.max_iter,) + x.shape, dtype=x.dtype), initial_particles)\n            trace = jax.tree_util.tree_map(lambda arr, val: arr.at[0].set(val), trace, initial_particles)\n            n_iter, final_state, _, lml, trace = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, trace))\n            trace = tree_map(lambda x: x[:n_iter], trace)\n            return dict(\n                n_iter=n_iter, \n                final_state=final_state, \n                lml=lml, \n                trace=trace\n            )\n        if self.return_diagnostics:            \n            n_iter, final_state, _, lml, final_info = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, initial_info))\n            return dict(\n                n_iter=n_iter, \n                final_state=final_state, \n                lml=lml, \n                final_info=final_info\n            )            \n        else:\n            n_iter, final_state, _, lml, = jax.lax.while_loop(cond, smc_cycle, (0, initial_particles, key, 0))\n            return dict(\n                n_iter=n_iter, \n                final_state=final_state, \n                lml=lml\n            )  \n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SMCInference.create_smc_kernel","title":"<code>create_smc_kernel()</code>","text":"<p>Creates an SMC kernel</p> <p>Currently a lightweight wrapper around the blackjax adaptive-tempered SMC object.</p> <p>Returns:</p> Type Description <code>SamplingAlgorithm</code> <p>A blackjax SMC <code>SamplingAlgorithm</code></p> Source code in <code>bamojax/inference.py</code> <pre><code>def create_smc_kernel(self) -&gt; SamplingAlgorithm:\n    r\"\"\" Creates an SMC kernel\n\n    Currently a lightweight wrapper around the blackjax adaptive-tempered SMC object.\n\n    Returns:\n        A blackjax SMC `SamplingAlgorithm`\n\n    \"\"\"\n    return adaptive_tempered_smc(logprior_fn=self.model.logprior_fn(),\n                                 loglikelihood_fn=self.model.loglikelihood_fn(),\n                                 mcmc_step_fn=self.mcmc_kernel.step,\n                                 mcmc_init_fn=self.mcmc_kernel.init,\n                                 mcmc_parameters=self.mcmc_parameters,\n                                 resampling_fn=self.resampling_fn,\n                                 target_ess=self.target_ess,\n                                 num_mcmc_steps=self.num_mutations)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SMCInference.tempering_condition","title":"<code>tempering_condition()</code>","text":"<p>Checks whether the SMC procedure terminates.</p> <p>Returns:</p> Type Description <p>A boolean deciding on whether the SMC tempering procedure is finished</p> Source code in <code>bamojax/inference.py</code> <pre><code>def tempering_condition(self):\n    r\"\"\" Checks whether the SMC procedure terminates.\n\n    Returns:\n        A boolean deciding on whether the SMC tempering procedure is finished\n\n    \"\"\"\n    def cond(carry):\n        i, state, *_ = carry\n        if self.return_trace:\n            return jnp.logical_and(state.lmbda &lt; 1, i &lt; self.max_iter)\n        return state.lmbda &lt; 1\n\n    #\n    return cond\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SMCInference.smc_cycle","title":"<code>smc_cycle(smc_kernel)</code>","text":"<p>One iteration of the adaptive-tempered SMC algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>smc_kernel</code> <p>A Blackjax SamplingAlgorithm containing the SMC logic.</p> required <p>Returns:     A Callable step function that performs one iteration.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def smc_cycle(self, smc_kernel) -&gt; Callable:\n    r\"\"\" One iteration of the adaptive-tempered SMC algorithm.\n\n    Args:\n        smc_kernel: A Blackjax SamplingAlgorithm containing the SMC logic.\n    Returns:\n        A Callable step function that performs one iteration.\n\n    \"\"\"\n    @jax.jit\n    def one_step(carry): \n        if self.return_trace:\n            i, state, k, curr_log_likelihood, state_hist = carry\n        else:               \n            if self.return_diagnostics:\n                i, state, k, curr_log_likelihood, _ = carry \n            else:\n                i, state, k, curr_log_likelihood = carry \n        k, subk = jrnd.split(k)\n        state, info = smc_kernel.step(subk, state)    \n        base_return_tuple = (i + 1, state, k, curr_log_likelihood + info.log_likelihood_increment)\n        if self.return_trace:\n            state_hist = tree_map(lambda arr, val: arr.at[i].set(val), state_hist, state)\n            return base_return_tuple + (state_hist, )            \n\n        return base_return_tuple + (info, ) if self.return_diagnostics else base_return_tuple\n\n    #\n    return one_step\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SMCInference.run_single_chain","title":"<code>run_single_chain(key)</code>","text":"<p>Run one chain of Sequential Monte Carlo.</p> <p>This returns SMC particles in different ways, depending on user provided settings.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <p>Returns:     A dictionary with the final SMC state, the number of iterations, the log marginal likelihood     (Optional) diagnostics and the trace of particles over each tempering step</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey) -&gt; dict:\n    r\"\"\" Run one chain of Sequential Monte Carlo.\n\n    This returns SMC particles in different ways, depending on user provided settings.\n\n    Args:\n        key: PRNGKey\n    Returns:\n        A dictionary with the final SMC state, the number of iterations, the log marginal likelihood\n        (Optional) diagnostics and the trace of particles over each tempering step\n\n    \"\"\"\n\n    if self.num_warmup &gt; 0:\n        key, key_init = jrnd.split(key)\n        print('Adapting NUTS HMC parameters...', end=\" \")\n        _, adapted_parameters = run_window_adaptation(self.model, key_init, self.num_warmup)\n        print('done.')\n        adapted_kernel = mcmc_sampler(model=self.model, mcmc_kernel=nuts, mcmc_parameters=adapted_parameters)\n        self.mcmc_kernel = adapted_kernel\n\n    smc_kernel = self.create_smc_kernel()\n    smc_cycle = self.smc_cycle(smc_kernel)\n    cond = self.tempering_condition()\n\n    key, key_init = jrnd.split(key)        \n    keys = jrnd.split(key_init, self.num_particles)\n    initial_particles = smc_kernel.init(jax.vmap(self.model.sample_prior)(keys))\n\n    if self.return_trace or self.return_diagnostics:\n        # Call SMC once to determine PyTree structures\n        key_smc, key_init = jrnd.split(key)\n        initial_particles, sample_info = smc_kernel.step(key_init, initial_particles)\n        initial_info = tree_map(lambda x: jax.numpy.zeros_like(x), sample_info)\n        initial_log_likelihood = sample_info.log_likelihood_increment\n\n    if self.return_trace:\n         # Preallocate arrays for state and info history\n        trace = tree_map(lambda x: jnp.zeros((self.max_iter,) + x.shape, dtype=x.dtype), initial_particles)\n        trace = jax.tree_util.tree_map(lambda arr, val: arr.at[0].set(val), trace, initial_particles)\n        n_iter, final_state, _, lml, trace = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, trace))\n        trace = tree_map(lambda x: x[:n_iter], trace)\n        return dict(\n            n_iter=n_iter, \n            final_state=final_state, \n            lml=lml, \n            trace=trace\n        )\n    if self.return_diagnostics:            \n        n_iter, final_state, _, lml, final_info = jax.lax.while_loop(cond, smc_cycle, (1, initial_particles, key_smc, initial_log_likelihood, initial_info))\n        return dict(\n            n_iter=n_iter, \n            final_state=final_state, \n            lml=lml, \n            final_info=final_info\n        )            \n    else:\n        n_iter, final_state, _, lml, = jax.lax.while_loop(cond, smc_cycle, (0, initial_particles, key, 0))\n        return dict(\n            n_iter=n_iter, \n            final_state=final_state, \n            lml=lml\n        )  \n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.SMCInference.run","title":"<code>run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.VIInference","title":"<code>VIInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The VI approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class VIInference(InferenceEngine):\n    r\"\"\" The VI approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,                  \n                 num_steps: int,\n                 num_chains: int = 1,\n                 num_gradient_samples: int = 10,\n                 optimizer: Callable = optax.sgd,\n                 optimizer_chain_args: list = None):\n        super().__init__(model, num_chains)\n        self.num_steps = num_steps\n        self.num_gradient_samples = num_gradient_samples\n        if optimizer_chain_args is None:\n            self.optimizer = optimizer\n        else:\n            if not isinstance(optimizer_chain_args, list):\n                optimizer_chain_args = [optimizer_chain_args]\n            self.optimizer = optax.chain(*optimizer_chain_args, optimizer)\n        self.bijectors = get_model_bijectors(self.model)\n        self.is_leaf_fn = lambda x: hasattr(x, 'forward') and hasattr(x, 'inverse')\n        self.forward_bijectors = lambda x: jax.tree.map(lambda b, v: b.forward(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n        self.backward_bijectors = lambda x: jax.tree.map(lambda b, v: b.inverse(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n\n        def logdensity_fn(z):\n            z = self.forward_bijectors(z)\n            return model.loglikelihood_fn()(z) + model.logprior_fn()(z)\n\n        #\n        self.logdensity_fn = logdensity_fn\n\n    #   \n    def sample_from_variational(self, key: PRNGKey, vi_result: dict, num_draws: int) -&gt; dict:\n        r\"\"\" Draw samples x ~ q(x | mu, rho)\n\n        Args:\n            key: PRNGKey\n            vi_result: a dictionary containing the variational approximation\n            num_draws: the number of samples to draw from the variational distribution.\n\n        Returns:\n            A dictionary with samples from the variational distribution, for each variable in the model.        \n\n        \"\"\"\n\n        if self.num_chains &gt; 1:\n            final_state = tree_map(lambda x: x[:, -1, ...], vi_result['states'])\n        else:\n            final_state = tree_map(lambda x: x[-1, ...], vi_result['states'])\n\n        def sample_fn(key, loc, scale):\n            return loc + scale*jrnd.normal(key, shape=(num_draws, ) + loc.shape)\n\n        #\n        vi_mu = final_state.mu\n        vi_rho = tree_map(lambda x: jnp.exp(x), final_state.rho)\n\n        flat_pytree, treedef = tree_flatten(vi_mu)\n        num_leaves = len(flat_pytree)\n        keys = jrnd.split(key, num_leaves)\n        keys_pytree = tree_unflatten(treedef, keys)\n\n        vi_samples = tree_map(sample_fn, keys_pytree, vi_mu, vi_rho)\n        if self.num_chains &gt; 1:\n            vi_samples = tree_map(lambda x: jnp.swapaxes(x, 0, 1), vi_samples)\n\n        # apply bijectors\n        vi_samples = self.forward_bijectors(vi_samples)\n        return vi_samples\n\n    #\n    def run_single_chain(self, key: PRNGKey) -&gt; dict:\n        r\"\"\" Run variational inference.\n\n        Args: \n            key: PRNGKey\n        Returns:\n            A dictionary with the variational parameters across iterations, and logging results such as the ELBO.\n\n        \"\"\"\n        mfvi = meanfield_vi(self.logdensity_fn, self.optimizer, self.num_gradient_samples)\n        initial_position = self.model.sample_prior(key=jrnd.PRNGKey(0))  # these are overriden by Blackjax\n        initial_state = mfvi.init(initial_position)\n\n        @jax.jit\n        def one_step(state, rng_key):\n            state, info = mfvi.step(rng_key, state)\n            return state, (state, info)\n\n        #\n        keys = jrnd.split(key, self.num_steps)\n        _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n        return dict(\n            states=states, \n            info=infos\n        )\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.VIInference.sample_from_variational","title":"<code>sample_from_variational(key, vi_result, num_draws)</code>","text":"<p>Draw samples x ~ q(x | mu, rho)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <code>vi_result</code> <code>dict</code> <p>a dictionary containing the variational approximation</p> required <code>num_draws</code> <code>int</code> <p>the number of samples to draw from the variational distribution.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with samples from the variational distribution, for each variable in the model.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def sample_from_variational(self, key: PRNGKey, vi_result: dict, num_draws: int) -&gt; dict:\n    r\"\"\" Draw samples x ~ q(x | mu, rho)\n\n    Args:\n        key: PRNGKey\n        vi_result: a dictionary containing the variational approximation\n        num_draws: the number of samples to draw from the variational distribution.\n\n    Returns:\n        A dictionary with samples from the variational distribution, for each variable in the model.        \n\n    \"\"\"\n\n    if self.num_chains &gt; 1:\n        final_state = tree_map(lambda x: x[:, -1, ...], vi_result['states'])\n    else:\n        final_state = tree_map(lambda x: x[-1, ...], vi_result['states'])\n\n    def sample_fn(key, loc, scale):\n        return loc + scale*jrnd.normal(key, shape=(num_draws, ) + loc.shape)\n\n    #\n    vi_mu = final_state.mu\n    vi_rho = tree_map(lambda x: jnp.exp(x), final_state.rho)\n\n    flat_pytree, treedef = tree_flatten(vi_mu)\n    num_leaves = len(flat_pytree)\n    keys = jrnd.split(key, num_leaves)\n    keys_pytree = tree_unflatten(treedef, keys)\n\n    vi_samples = tree_map(sample_fn, keys_pytree, vi_mu, vi_rho)\n    if self.num_chains &gt; 1:\n        vi_samples = tree_map(lambda x: jnp.swapaxes(x, 0, 1), vi_samples)\n\n    # apply bijectors\n    vi_samples = self.forward_bijectors(vi_samples)\n    return vi_samples\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.VIInference.run_single_chain","title":"<code>run_single_chain(key)</code>","text":"<p>Run variational inference.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNGKey</p> required <p>Returns:     A dictionary with the variational parameters across iterations, and logging results such as the ELBO.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run_single_chain(self, key: PRNGKey) -&gt; dict:\n    r\"\"\" Run variational inference.\n\n    Args: \n        key: PRNGKey\n    Returns:\n        A dictionary with the variational parameters across iterations, and logging results such as the ELBO.\n\n    \"\"\"\n    mfvi = meanfield_vi(self.logdensity_fn, self.optimizer, self.num_gradient_samples)\n    initial_position = self.model.sample_prior(key=jrnd.PRNGKey(0))  # these are overriden by Blackjax\n    initial_state = mfvi.init(initial_position)\n\n    @jax.jit\n    def one_step(state, rng_key):\n        state, info = mfvi.step(rng_key, state)\n        return state, (state, info)\n\n    #\n    keys = jrnd.split(key, self.num_steps)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n    return dict(\n        states=states, \n        info=infos\n    )\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.VIInference.run","title":"<code>run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.LaplaceInference","title":"<code>LaplaceInference</code>","text":"<p>               Bases: <code>InferenceEngine</code></p> <p>The Laplace approximate inference class</p> Source code in <code>bamojax/inference.py</code> <pre><code>class LaplaceInference(InferenceEngine):\n    r\"\"\" The Laplace approximate inference class\n\n    \"\"\"\n\n    def __init__(self, \n                 model: Model,   \n                 num_chains: int = 1,\n                 optimizer: Callable = jaxopt.ScipyMinimize,\n                 optimizer_args: dict = None,\n                 bounds: dict = None):\n        super().__init__(model, num_chains)\n        if optimizer_args is None:\n            optimizer_args = {}\n        self.bounds = bounds\n\n        self.bijectors = get_model_bijectors(model)\n        self.is_leaf_fn = lambda x: hasattr(x, 'forward') and hasattr(x, 'inverse')\n        self.forward_bijectors = lambda x: jax.tree.map(lambda b, v: b.forward(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n        self.backward_bijectors = lambda x: jax.tree.map(lambda b, v: b.inverse(v), self.bijectors, x, is_leaf=self.is_leaf_fn)\n\n        @jax.jit\n        def logdensity_fn(z):\n            z = self.forward_bijectors(z)\n            return -1.0 * (model.loglikelihood_fn()(z) + model.logprior_fn()(z))\n\n        #\n        self.obj_fun = logdensity_fn\n\n        if self.bounds is not None:\n            optimizer = jaxopt.ScipyBoundedMinimize(fun=self.obj_fun, **optimizer_args)\n        else:\n            optimizer = optimizer(fun=self.obj_fun, **optimizer_args)\n        self.optimizer = optimizer\n        self.optimizer_args = optimizer_args if optimizer_args is not None else {}        \n        self.D = model.get_model_size()\n\n    #\n    def run_single_chain(self, key):\n        def get_unconstrained_init(model, key):\n            constrained = tree_map(jnp.asarray, model.sample_prior(key))\n            unconstrained = self.backward_bijectors(constrained)\n            return unconstrained\n\n        #\n        init_params = get_unconstrained_init(self.model, key)\n\n        if self.bounds is not None:\n            sol = self.optimizer.run(init_params, bounds=self.bounds)   \n        else:\n            sol = self.optimizer.run(init_params)   \n\n        # We fit a Gaussian(\\hat{\\theta}, \\Sigma) with \n        # \\hat{\\theta} = \\argmax_\\theta p(\\theta \\mid y)\n        # \\Sigma^-1 is the Hessian of -\\log p(\\theta \\mid y) at \\theta=\\hat{\\theta}\n\n        mode = self.forward_bijectors(sol.params)\n        H = jax.hessian(self.obj_fun)(mode)\n        theta_hat_flat, unravel_fn = ravel_pytree(mode)\n\n        def flat_obj_fn(flat_params):\n            params = unravel_fn(flat_params)\n            return self.obj_fun(params)\n\n        H = jax.hessian(flat_obj_fn)(theta_hat_flat)\n\n        Sigma = jnp.linalg.inv(H)\n\n        if theta_hat_flat.shape == () or theta_hat_flat.shape == (1,):  # Univariate case\n            dist = dx.Normal(loc=theta_hat_flat, scale=jnp.sqrt(Sigma))\n        else:\n            dist = dx.MultivariateNormalFullCovariance(loc=theta_hat_flat, covariance_matrix=Sigma)\n\n        _, logdet = jnp.linalg.slogdet(Sigma)\n\n        log_posterior = -1.0 * self.obj_fun(mode)\n        lml = log_posterior + 1/2*logdet + self.D/2 * jnp.log(2*jnp.pi)\n\n        return dict(\n            distribution=dist,\n            mode=mode,\n            flat_mode=theta_hat_flat,\n            covariance=Sigma,\n            hessian=H,\n            unravel_fn=unravel_fn,\n            lml=lml\n        )\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.LaplaceInference.run","title":"<code>run(key)</code>","text":"<p>Runs the inference algorithm, optionally vmapped over mutiple chains</p> Source code in <code>bamojax/inference.py</code> <pre><code>def run(self, key):\n    r\"\"\" Runs the inference algorithm, optionally vmapped over mutiple chains\n\n    \"\"\"\n    if self.num_chains &gt; 1:\n        keys = jrnd.split(key, self.num_chains)\n        return jax.vmap(self.run_single_chain)(keys)\n    return self.run_single_chain(key)\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.run_window_adaptation","title":"<code>run_window_adaptation(model, key, num_warmup_steps)</code>","text":"<p>Find optimized parameters for HMC-based inference.</p> <p>A wrapper for blackjax HMC window adaptation.</p> <p>Parameters:</p> Name Type Description Default <code>Model</code> <p>A bamojax model (DAG)</p> required <code>key</code> <code>PRNGKey</code> <p>Random seed</p> required <code>num_warmum_steps</code> <p>int, number of warmup steps to take</p> required <p>Returns:</p> Type Description <ul> <li>A dictionary of the state after performing the warmup steps</li> </ul> <ul> <li>A set of HMC kernel parameters that result in optimal acceptance rates</li> </ul> Source code in <code>bamojax/inference.py</code> <pre><code>def run_window_adaptation(model, key: PRNGKey, num_warmup_steps):\n    r\"\"\" Find optimized parameters for HMC-based inference.\n\n    A wrapper for blackjax HMC window adaptation.\n\n    Args:\n        Model: A bamojax model (DAG)\n        key: Random seed\n        num_warmum_steps: int, number of warmup steps to take\n\n    Returns:\n        - A dictionary of the state after performing the warmup steps\n        - A set of HMC kernel parameters that result in optimal acceptance rates\n\n    \"\"\"\n    logdensity_fn = lambda state: model.loglikelihood_fn()(state) + model.logprior_fn()(state)\n    warmup = window_adaptation(nuts, logdensity_fn)\n    key_init, key_warmup = jrnd.split(key)\n    initial_state = model.sample_prior(key_init)\n    (warm_state, warm_parameters), _ = warmup.run(key_warmup, initial_state, num_steps=num_warmup_steps)  \n    return warm_state, warm_parameters\n</code></pre>"},{"location":"api/bamojax.inference/#bamojax.inference.get_model_bijectors","title":"<code>get_model_bijectors(model)</code>","text":"<p>Meanfield VI imposes a Gaussian variational distribution. </p> <p>In order to use the correct parameter constraints this function determines all relevant bijectors.</p> <p>It's a bijector detector! :-)</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with bijectors for the variables that use it, and an identity bijector otherwise.</p> Source code in <code>bamojax/inference.py</code> <pre><code>def get_model_bijectors(model) -&gt; dict:\n    r\"\"\" Meanfield VI imposes a Gaussian variational distribution. \n\n    In order to use the correct parameter constraints this function determines all relevant bijectors.\n\n    It's a bijector detector! :-)\n\n    Returns:\n        A dictionary with bijectors for the variables that use it, and an identity bijector otherwise.\n\n    \"\"\"\n    latent_nodes = model.get_latent_nodes()\n    bijectors = {}\n    for node_name, node in latent_nodes.items():\n        if hasattr(node.distribution, '_bijector'):\n            bij = node.distribution._bijector\n            transform = bij\n        else:\n            transform = tfb.Identity()  # Use TensorFlow Probability's Identity bijector\n        bijectors[node_name] = transform\n    return bijectors\n</code></pre>"},{"location":"api/bamojax.marginal_likelihoods/","title":"<code>bamojax.marginal_likelihoods</code>","text":""},{"location":"api/bamojax.marginal_likelihoods/#bamojax.marginal_likelihoods.iid_likelihood","title":"<code>iid_likelihood(L)</code>","text":"<p>We typically have multiple observations and assume the likelihood factorizes  as: </p> \\[         \\log p\\left(Y \\mid \\theta\\right) = \\sum_{i=1}^N \\log p\\left(y_i \\mid \\theta\\right) \\enspace. \\] Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def iid_likelihood(L: Callable):\n    r\"\"\"\n\n    We typically have multiple observations and assume the likelihood factorizes \n    as: \n\n    $$    \n        \\log p\\left(Y \\mid \\theta\\right) = \\sum_{i=1}^N \\log p\\left(y_i \\mid \\theta\\right) \\enspace.\n    $$\n\n    \"\"\"\n    return lambda x: jnp.sum(L()(x))\n</code></pre>"},{"location":"api/bamojax.marginal_likelihoods/#bamojax.marginal_likelihoods.naive_monte_carlo","title":"<code>naive_monte_carlo(key, model, num_prior_draws=1000, num_chunks=5, iid_obs=True, pb=True)</code>","text":"<p>The Naive Monte Carlo (NMC) estimator</p> <p>The marginal likelihood is defined as  $$     p(D) = \\int_\\Theta p\\left(D \\mid \\theta\\right) p(\\theta) \\,\\text{d}\\theta \\enspace . $$</p> <p>In NMC we draw samples from the prior and approximate the ML as $$     p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right), with \\theta_i \\sim p(\\theta) \\enspace. $$ In nontrivial models, we need a large \\(N\\) for this approximation to be  reasonable.</p> Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def naive_monte_carlo(key, \n                      model: Model, \n                      num_prior_draws: int = 1_000, \n                      num_chunks: int = 5,\n                      iid_obs: bool = True,\n                      pb = True) -&gt; Float:   \n    r\"\"\"The Naive Monte Carlo (NMC) estimator\n\n    The marginal likelihood is defined as \n    $$\n        p(D) = \\int_\\Theta p\\left(D \\mid \\theta\\right) p(\\theta) \\,\\text{d}\\theta \\enspace .\n    $$\n\n    In NMC we draw samples from the prior and approximate the ML as\n    $$\n        p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right), with \\theta_i \\sim p(\\theta) \\enspace.\n    $$\n    In nontrivial models, we need a *large* $N$ for this approximation to be \n    reasonable.\n\n    \"\"\"\n\n    if iid_obs:\n        loglikelihood_fn = iid_likelihood(model.loglikelihood_fn)\n    else:\n        loglikelihood_fn = model.loglikelihood_fn\n\n    loglikelihoods = jnp.zeros((num_prior_draws, \n                                num_chunks))\n\n    # We don't want to vmap this loop, as the reason for the loop is to avoid\n    # running out of memory!\n    for i in tqdm(range(num_chunks), disable=not pb):\n        key, subkey = jrnd.split(key)\n        keys = jrnd.split(subkey, num_prior_draws)\n        prior_draws = jax.vmap(model.sample_prior)(keys)\n        loglikelihoods = loglikelihoods.at[:, i].set(jax.vmap(loglikelihood_fn)(prior_draws))\n    return logsumexp(loglikelihoods.flatten()) - jnp.log(num_prior_draws*num_chunks)\n</code></pre>"},{"location":"api/bamojax.marginal_likelihoods/#bamojax.marginal_likelihoods.importance_sampling","title":"<code>importance_sampling(key, model, g_IS, num_samples=1000, iid_obs=True)</code>","text":"<p>Importance sampling routine for a given BayesianModel.</p> <p>Importance sampling is based around the following approximation to the log marginal likelihood [Gronau et al., 2017]:</p> <p>$$ p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right) \\frac{p(\\theta_i)}{g_IS(\\theta_i)}\\enspace, $$ with \\(\\theta_i \\sim g_IS(\\theta)\\)</p> <p>Here, g_IS is the importance density, which should meet these criteria:</p> <ol> <li>It is easy to evaluate.</li> <li>It has the same domain as the posterior p(\\theta \\mid D).</li> <li>It matches the posterior as closely as possible.</li> <li>It has fatter tails than the posterior.</li> </ol> <p>There is no one-size-fits-all importance density; this needs to be crafted carefully for each specific problem.</p> <p>Note that the importance density can also be a mixture distribution, which  can make it easier to introduce heavy tails.</p> <p>References:</p> <ul> <li>Gronau, Q. F., Sarafoglou, A., Matzke, D., Ly, A., Boehm, U., Marsman, M., Leslie, D. S., Forster, J. J., Wagenmakers, E.-J., &amp; Steingroever, H. (2017). A tutorial on bridge sampling. Journal of Mathematical Psychology, 81, 80-97. https://doi.org/10.1016/j.jmp.2017.09.005</li> </ul> Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def importance_sampling(key, \n                        model: Model, \n                        g_IS: Distribution,\n                        num_samples: int = 1_000,\n                        iid_obs: bool = True) -&gt; Float:\n\n    r\"\"\"Importance sampling routine for a given BayesianModel.\n\n    Importance sampling is based around the following approximation to the log\n    marginal likelihood [Gronau et al., 2017]:\n\n    $$\n    p(D) \\approx \\frac{1}{N} \\sum_{i=1}^N p\\left(D \\mid \\theta_i\\right) \\frac{p(\\theta_i)}{g_IS(\\theta_i)}\\enspace,\n    $$\n    with $\\theta_i \\sim g_IS(\\theta)$\n\n    Here, g_IS is the importance density, which should meet these criteria:\n\n    1. It is easy to evaluate.\n    2. It has the same domain as the posterior p(\\theta \\mid D).\n    3. It matches the posterior as closely as possible.\n    4. It has fatter tails than the posterior.\n\n    There is no one-size-fits-all importance density; this needs to be crafted\n    carefully for each specific problem.\n\n    Note that the importance density can also be a mixture distribution, which \n    can make it easier to introduce heavy tails.\n\n    References:\n\n    - Gronau, Q. F., Sarafoglou, A., Matzke, D., Ly, A., Boehm, U., Marsman, M., Leslie, D. S., Forster, J. J., Wagenmakers, E.-J., &amp; Steingroever, H. (2017). A tutorial on bridge sampling. Journal of Mathematical Psychology, 81, 80-97. https://doi.org/10.1016/j.jmp.2017.09.005\n\n\n    \"\"\"\n\n    def g_eval(state):\n        logprob = 0        \n        values_flat, _ = tree_flatten(state)\n        for value, dist in zip(values_flat, g_flat):\n            logprob += jnp.sum(dist.log_prob(value))\n        return logprob\n\n    # \n    def adjusted_likelihood(state):\n        return loglikelihood_fn(state) + logprior_fn(state) - g_eval(state)\n\n    #\n\n    if iid_obs:\n        loglikelihood_fn = iid_likelihood(model.loglikelihood_fn)\n    else:\n        loglikelihood_fn = model.loglikelihood_fn\n\n    logprior_fn = model.logprior_fn()\n\n    g_flat, g_treedef = tree_flatten(g_IS, \n                                     lambda l: isinstance(l, (Distribution, Bijector)))\n\n    samples = list()\n    for g in g_flat:\n        key, subkey = jrnd.split(key)\n        samples.append(g.sample(seed=subkey, sample_shape=(num_samples, )))\n\n    importance_samples = tree_unflatten(g_treedef, samples)\n    adjusted_likelihoods = jax.vmap(adjusted_likelihood)(importance_samples)\n    return logsumexp(adjusted_likelihoods) - jnp.log(num_samples)\n</code></pre>"},{"location":"api/bamojax.marginal_likelihoods/#bamojax.marginal_likelihoods.laplace_approximation","title":"<code>laplace_approximation(key, model, iid_obs=True, **opt_args)</code>","text":"<p>Compute the Laplace approximation of the log marginal likelihood of model</p> <p>The Laplace approximation approximates the posterior density of the model  with a Gaussian, centered at the mode of the density and with its curvature determined by the Hessian matrix of the negative log posterior density.</p> <p>The marginal likelihood of this proxy distribution is known in closed-form, and is used to approximate the actual marginal likelihood.</p> <p>See https://en.wikipedia.org/wiki/Laplace%27s_approximation</p> Source code in <code>bamojax/marginal_likelihoods.py</code> <pre><code>def laplace_approximation(key,\n                          model: Model,\n                          iid_obs: bool= True,\n                          **opt_args):\n\n    r\"\"\"Compute the Laplace approximation of the log marginal likelihood of model\n\n    The Laplace approximation approximates the posterior density of the model \n    with a Gaussian, centered at the mode of the density and with its curvature\n    determined by the Hessian matrix of the negative log posterior density.\n\n    The marginal likelihood of this proxy distribution is known in closed-form,\n    and is used to approximate the actual marginal likelihood.\n\n    See https://en.wikipedia.org/wiki/Laplace%27s_approximation\n\n    \"\"\"\n\n    # The objective function is the unnormalized posterior\n    @jax.jit\n    def fun(x):\n        return -1.0 * (loglikelihood_fn(x) + logprior_fn(x))\n\n    #\n    if iid_obs:\n        loglikelihood_fn = iid_likelihood(model.loglikelihood_fn)\n    else:\n        loglikelihood_fn = model.loglikelihood_fn\n    logprior_fn = model.logprior_fn()\n\n    # Get initial values in the same PyTree structure as the model expects\n    init_params = tree_map(jnp.asarray, \n                           model.sample_prior(key))\n\n    # For some models, the parameters are bounded\n    if 'bounds' in opt_args:\n        print(opt_args['bounds'])\n        solver = jaxopt.ScipyBoundedMinimize(fun=fun)\n    else:\n        solver = jaxopt.ScipyMinimize(fun=fun)\n\n\n    # Derive the number of parameters\n    D = 0\n    vars_flattened, _ = tree_flatten(init_params)\n    for varval in vars_flattened:\n        D += varval.shape[0] if varval.shape else 1\n\n    # Compute MAP\n    sol = solver.run(init_params, **opt_args)   \n\n    # We fit a Gaussian(\\hat{\\theta}, \\Sigma) with \n    # \\hat{\\theta} = \\argmax_\\theta p(\\theta \\mid y)\n    # \\Sigma^-1 is the Hessian of -\\log p(\\theta \\mid y) at \\theta=\\hat{\\theta}\n\n    mode = sol.params\n    H = jax.hessian(fun)(mode)\n    h, _ = tree_flatten(H)\n    if D &gt; 1:\n        S = jnp.squeeze(jnp.linalg.inv(jnp.reshape(jnp.asarray(h), \n                                                   newshape=(D, D))))\n        _, logdet = jnp.linalg.slogdet(S)\n    else: \n        S = 1.0 / jnp.squeeze(jnp.asarray(h))\n        logdet = jnp.log(S)\n\n    log_posterior = -1.0 * sol.state.fun_val\n    lml = log_posterior + 1/2*logdet + D/2 * jnp.log(2*jnp.pi)\n    return lml\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_adaptive_tempered/","title":"<code>bamojax.modified_blackjax.modified_adaptive_tempered</code>","text":""},{"location":"api/bamojax.modified_blackjax.modified_adaptive_tempered/#bamojax.modified_blackjax.modified_adaptive_tempered.build_kernel","title":"<code>build_kernel(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, resampling_fn, target_ess, root_solver=solver.dichotomy, **extra_parameters)</code>","text":"<p>Build a Tempered SMC step using an adaptive schedule.</p>"},{"location":"api/bamojax.modified_blackjax.modified_adaptive_tempered/#bamojax.modified_blackjax.modified_adaptive_tempered.build_kernel--parameters","title":"Parameters","text":"<p>logprior_fn: Callable     A function that computes the log-prior density. loglikelihood_fn: Callable     A function that returns the log-likelihood density. mcmc_kernel_factory: Callable     A callable function that creates a mcmc kernel from a log-probability     density function. make_mcmc_state: Callable     A function that creates a new mcmc state from a position and a     log-probability density function. resampling_fn: Callable     A random function that resamples generated particles based of weights target_ess: float     The target ESS for the adaptive MCMC tempering root_solver: Callable, optional     A solver utility to find delta matching the target ESS. Signature is     <code>root_solver(fun, delta_0, min_delta, max_delta)</code>, default is a dichotomy solver use_log_ess: bool, optional     Use ESS in log space to solve for delta, default is <code>True</code>.     This is usually more stable when using gradient based solvers.</p>"},{"location":"api/bamojax.modified_blackjax.modified_adaptive_tempered/#bamojax.modified_blackjax.modified_adaptive_tempered.build_kernel--returns","title":"Returns","text":"<p>A callable that takes a rng_key and a TemperedSMCState that contains the current state of the chain and that returns a new state of the chain along with information about the transition.</p> Source code in <code>bamojax/modified_blackjax/modified_adaptive_tempered.py</code> <pre><code>def build_kernel(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    resampling_fn: Callable,\n    target_ess: float,\n    root_solver: Callable = solver.dichotomy,\n    **extra_parameters,\n) -&gt; Callable:\n    r\"\"\"Build a Tempered SMC step using an adaptive schedule.\n\n    Parameters\n    ----------\n    logprior_fn: Callable\n        A function that computes the log-prior density.\n    loglikelihood_fn: Callable\n        A function that returns the log-likelihood density.\n    mcmc_kernel_factory: Callable\n        A callable function that creates a mcmc kernel from a log-probability\n        density function.\n    make_mcmc_state: Callable\n        A function that creates a new mcmc state from a position and a\n        log-probability density function.\n    resampling_fn: Callable\n        A random function that resamples generated particles based of weights\n    target_ess: float\n        The target ESS for the adaptive MCMC tempering\n    root_solver: Callable, optional\n        A solver utility to find delta matching the target ESS. Signature is\n        `root_solver(fun, delta_0, min_delta, max_delta)`, default is a dichotomy solver\n    use_log_ess: bool, optional\n        Use ESS in log space to solve for delta, default is `True`.\n        This is usually more stable when using gradient based solvers.\n\n    Returns\n    -------\n    A callable that takes a rng_key and a TemperedSMCState that contains the current state\n    of the chain and that returns a new state of the chain along with\n    information about the transition.\n\n    \"\"\"\n\n    def compute_delta(state: tempered.TemperedSMCState) -&gt; float:\n        lmbda = state.lmbda\n        max_delta = 1 - lmbda\n        delta = ess.ess_solver(\n            jax.vmap(loglikelihood_fn),\n            state.particles,\n            target_ess,\n            max_delta,\n            root_solver,\n        )\n        delta = jnp.clip(delta, 0.0, max_delta)\n\n        return delta\n\n    tempered_kernel = tempered.build_kernel(\n        logprior_fn,\n        loglikelihood_fn,\n        mcmc_step_fn,\n        mcmc_init_fn,\n        resampling_fn,\n        **extra_parameters,\n    )\n\n    def kernel(\n        rng_key: PRNGKey,\n        state: tempered.TemperedSMCState,\n        num_mcmc_steps: int,\n        mcmc_parameters: dict,\n    ) -&gt; tuple[tempered.TemperedSMCState, base.SMCInfo]:\n        delta = compute_delta(state)\n        lmbda = delta + state.lmbda\n        return tempered_kernel(rng_key, state, num_mcmc_steps, lmbda, mcmc_parameters)\n\n    return kernel\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_adaptive_tempered/#bamojax.modified_blackjax.modified_adaptive_tempered.as_top_level_api","title":"<code>as_top_level_api(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, mcmc_parameters, resampling_fn, target_ess, root_solver=solver.dichotomy, num_mcmc_steps=10, **extra_parameters)</code>","text":"<p>Implements the (basic) user interface for the Adaptive Tempered SMC kernel.</p>"},{"location":"api/bamojax.modified_blackjax.modified_adaptive_tempered/#bamojax.modified_blackjax.modified_adaptive_tempered.as_top_level_api--parameters","title":"Parameters","text":"<p>logprior_fn     The log-prior function of the model we wish to draw samples from. loglikelihood_fn     The log-likelihood function of the model we wish to draw samples from. mcmc_step_fn     The MCMC step function used to update the particles. mcmc_init_fn     The MCMC init function used to build a MCMC state from a particle position. mcmc_parameters     The parameters of the MCMC step function.  Parameters with leading dimension     length of 1 are shared amongst the particles. resampling_fn     The function used to resample the particles. target_ess     The number of effective sample size to aim for at each step. root_solver     The solver used to adaptively compute the temperature given a target number     of effective samples. num_mcmc_steps     The number of times the MCMC kernel is applied to the particles per step.</p>"},{"location":"api/bamojax.modified_blackjax.modified_adaptive_tempered/#bamojax.modified_blackjax.modified_adaptive_tempered.as_top_level_api--returns","title":"Returns","text":"<p>A <code>SamplingAlgorithm</code>.</p> Source code in <code>bamojax/modified_blackjax/modified_adaptive_tempered.py</code> <pre><code>def as_top_level_api(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    mcmc_parameters: dict,\n    resampling_fn: Callable,\n    target_ess: float,\n    root_solver: Callable = solver.dichotomy,\n    num_mcmc_steps: int = 10,\n    **extra_parameters,\n) -&gt; SamplingAlgorithm:\n    \"\"\"Implements the (basic) user interface for the Adaptive Tempered SMC kernel.\n\n    Parameters\n    ----------\n    logprior_fn\n        The log-prior function of the model we wish to draw samples from.\n    loglikelihood_fn\n        The log-likelihood function of the model we wish to draw samples from.\n    mcmc_step_fn\n        The MCMC step function used to update the particles.\n    mcmc_init_fn\n        The MCMC init function used to build a MCMC state from a particle position.\n    mcmc_parameters\n        The parameters of the MCMC step function.  Parameters with leading dimension\n        length of 1 are shared amongst the particles.\n    resampling_fn\n        The function used to resample the particles.\n    target_ess\n        The number of effective sample size to aim for at each step.\n    root_solver\n        The solver used to adaptively compute the temperature given a target number\n        of effective samples.\n    num_mcmc_steps\n        The number of times the MCMC kernel is applied to the particles per step.\n\n    Returns\n    -------\n    A ``SamplingAlgorithm``.\n\n    \"\"\"\n    kernel = build_kernel(\n        logprior_fn,\n        loglikelihood_fn,\n        mcmc_step_fn,\n        mcmc_init_fn,\n        resampling_fn,\n        target_ess,\n        root_solver,\n        **extra_parameters,\n    )\n\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return init(position)\n\n    def step_fn(rng_key: PRNGKey, state):\n        return kernel(\n            rng_key,\n            state,\n            num_mcmc_steps,\n            mcmc_parameters,\n        )\n\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/","title":"<code>bamojax.modified_blackjax.modified_elliptical_slice_nd</code>","text":"<p>Public API for the Elliptical Slice sampling Kernel</p>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.EllipSliceState","title":"<code>EllipSliceState</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>State of the Elliptical Slice sampling algorithm.</p> <p>position     Current position of the chain. logdensity     Current value of the logdensity (evaluated at current position).</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>class EllipSliceState(NamedTuple):\n    \"\"\"State of the Elliptical Slice sampling algorithm.\n\n    position\n        Current position of the chain.\n    logdensity\n        Current value of the logdensity (evaluated at current position).\n\n    \"\"\"\n\n    position: ArrayTree\n    logdensity: ArrayTree\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.EllipSliceInfo","title":"<code>EllipSliceInfo</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Additional information on the Elliptical Slice sampling chain.</p> <p>This additional information can be used for debugging or computing diagnostics.</p> <p>momentum     The latent momentum variable returned at the end of the transition. theta     A value between [-2\\pi, 2\\pi] identifying points in the ellipsis drawn     from the positon and mommentum variables. This value indicates the theta     value of the accepted proposal. subiter     Number of sub iterations needed to accept a proposal. The more subiterations     needed the less efficient the algorithm will be, and the more dependent the     new value is likely to be to the previous value.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>class EllipSliceInfo(NamedTuple):\n    r\"\"\"Additional information on the Elliptical Slice sampling chain.\n\n    This additional information can be used for debugging or computing\n    diagnostics.\n\n    momentum\n        The latent momentum variable returned at the end of the transition.\n    theta\n        A value between [-2\\pi, 2\\pi] identifying points in the ellipsis drawn\n        from the positon and mommentum variables. This value indicates the theta\n        value of the accepted proposal.\n    subiter\n        Number of sub iterations needed to accept a proposal. The more subiterations\n        needed the less efficient the algorithm will be, and the more dependent the\n        new value is likely to be to the previous value.\n\n    \"\"\"\n\n    momentum: ArrayTree\n    theta: float\n    subiter: int\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.build_kernel","title":"<code>build_kernel(cov_matrix, mean, nd=None)</code>","text":"<p>Build an Elliptical Slice sampling kernel :cite:p:<code>murray2010elliptical</code>.</p>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.build_kernel--parameters","title":"Parameters","text":"<p>cov_matrix     The value of the covariance matrix of the gaussian prior distribution from     the posterior we wish to sample.</p>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.build_kernel--returns","title":"Returns","text":"<p>A kernel that takes a rng_key and a Pytree that contains the current state of the chain and that returns a new state of the chain along with information about the transition.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>def build_kernel(cov_matrix: Array, mean: Array, nd: tuple = None):\n    \"\"\"Build an Elliptical Slice sampling kernel :cite:p:`murray2010elliptical`.\n\n    Parameters\n    ----------\n    cov_matrix\n        The value of the covariance matrix of the gaussian prior distribution from\n        the posterior we wish to sample.\n\n    Returns\n    -------\n    A kernel that takes a rng_key and a Pytree that contains the current state\n    of the chain and that returns a new state of the chain along with\n    information about the transition.\n\n    \"\"\"\n    ndim = jnp.ndim(cov_matrix)  # type: ignore[arg-type]\n    n = cov_matrix.shape[0]\n\n    if len(nd) == 2:\n        d, nu = nd        \n        flat_shape = (d*nu*n, )\n    elif len(nd) == 1:\n        d = 1\n        nu = nd[0]\n        flat_shape = (nu*n, )\n    else:\n        raise NotImplementedError(f'Elliptical slice sampling is not implemented for nd = {nd}')\n\n    if ndim == 1:  # diagonal covariance matrix\n        cov_matrix_sqrt = jnp.sqrt(cov_matrix)\n\n    elif ndim == 2:\n        cov_matrix_sqrt = jax.lax.linalg.cholesky(cov_matrix)\n\n    else:\n        raise ValueError(\n            \"The mass matrix has the wrong number of dimensions:\"\n            f\" expected 1 or 2, got {jnp.ndim(cov_matrix)}.\"  # type: ignore[arg-type]\n        )\n\n    def to_flat(u):\n        return jax.tree_util.tree_map(lambda l: jnp.reshape(l, shape=flat_shape), u)\n\n    def to_nd(u):\n        return jax.tree_util.tree_map(lambda l: jnp.reshape(l, shape=nd + (n, )), u)\n\n    def momentum_generator(rng_key, position):\n        generate_noise_fn = lambda k, p: generate_gaussian_noise(k, p, mean, cov_matrix_sqrt)\n        # [MODIFICATIONS] vmap the noise generation over the dimensions nd, then reshape into flattened array\n\n        u = to_nd(position)\n\n        if d==1 and nu==1:\n            z = generate_noise_fn(rng_key, position)\n        elif d&gt;1 and nu==1:\n            keys_d = jax.random.split(rng_key, d)\n            z = jax.vmap(generate_noise_fn, in_axes=(0, 0))(keys_d, u)\n        elif d==1 and nu &gt; 1:\n            keys_nu = jax.random.split(rng_key, nu)\n            z = jax.vmap(generate_noise_fn, in_axes=(0, 0))(keys_nu, u)\n        elif d&gt;1 and nu&gt;1:\n            keys_d = jax.random.split(rng_key, d)\n            keys_dnu = jax.vmap(lambda k: jax.random.split(k, nu))(keys_d)\n            z = jax.vmap(jax.vmap(generate_noise_fn, in_axes=(0, 0)), in_axes=(0, 0))(keys_dnu, u)\n        else:\n            raise NotImplementedError\n        z_reshaped = to_flat(z)\n        return z_reshaped\n\n    def elliptical_proposal(\n        logdensity_fn: Callable,\n        momentum_generator: Callable,\n        mean: Array,\n    ) -&gt; Callable:\n        \"\"\"Build an Ellitpical slice sampling kernel.\n\n        The algorithm samples a latent parameter, traces an ellipse connecting the\n        initial position and the latent parameter and does slice sampling on this\n        ellipse to output a new sample from the posterior distribution.\n\n        Parameters\n        ----------\n        logdensity_fn\n            A function that returns the log-likelihood at a given position.\n        momentum_generator\n            A function that generates a new latent momentum variable.\n\n        Returns\n        -------\n        A kernel that takes a rng_key and a Pytree that contains the current state\n        of the chain and that returns a new state of the chain along with\n        information about the transition.\n\n        \"\"\"\n        num_el = d*nu\n        mean = jnp.tile(mean, reps=num_el)\n\n        def generate(\n            rng_key: PRNGKey, state: EllipSliceState\n        ) -&gt; tuple[EllipSliceState, EllipSliceInfo]:\n            position, logdensity = state\n            position = to_flat(position)\n            key_slice, key_momentum, key_uniform, key_theta = jax.random.split(rng_key, 4)\n            # step 1: sample momentum\n            momentum = momentum_generator(key_momentum, position) \n            # step 2: get slice (y)\n            logy = logdensity + jnp.log(jax.random.uniform(key_uniform))\n            # step 3: get theta (ellipsis move), set inital interval\n            theta = 2 * jnp.pi * jax.random.uniform(key_theta)\n            theta_min = theta - 2 * jnp.pi\n            theta_max = theta\n            # step 4: proposal\n            p, m = ellipsis(position, momentum, theta, mean)\n            # step 5: acceptance\n            logdensity = logdensity_fn(p)\n\n            def slice_fn(vals):\n                \"\"\"Perform slice sampling around the ellipsis.\n\n                Checks if the proposed position's likelihood is larger than the slice\n                variable. Returns the position if True, shrinks the bracket for sampling\n                `theta` and samples a new proposal if False.\n\n                As the bracket `[theta_min, theta_max]` shrinks, the proposal gets closer\n                to the original position, which has likelihood larger than the slice variable.\n                It is guaranteed to stop in a finite number of iterations as long as the\n                likelihood is continuous with respect to the parameter being sampled.\n\n                \"\"\"\n                _, subiter, theta, theta_min, theta_max, *_ = vals\n                thetak = jax.random.fold_in(key_slice, subiter)\n                theta = jax.random.uniform(thetak, minval=theta_min, maxval=theta_max)\n                p, m = ellipsis(position, momentum, theta, mean)\n                logdensity = logdensity_fn(p)\n                theta_min = jnp.where(theta &lt; 0, theta, theta_min)\n                theta_max = jnp.where(theta &gt; 0, theta, theta_max)\n                subiter += 1\n                return logdensity, subiter, theta, theta_min, theta_max, p, m\n\n            logdensity, subiter, theta, *_, position, momentum = jax.lax.while_loop(\n                lambda vals: vals[0] &lt;= logy,\n                slice_fn,\n                (logdensity, 1, theta, theta_min, theta_max, p, m),\n            )\n            position = to_nd(position)\n            return (\n                EllipSliceState(position, logdensity),\n                EllipSliceInfo(momentum, theta, subiter),\n            )\n\n        return generate\n\n\n    def kernel(\n        rng_key: PRNGKey,\n        state: EllipSliceState,\n        logdensity_fn: Callable,\n    ) -&gt; tuple[EllipSliceState, EllipSliceInfo]:\n        proposal_generator = elliptical_proposal(\n            logdensity_fn, momentum_generator, mean, \n        )       \n        return proposal_generator(rng_key, state)\n\n    return kernel\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api","title":"<code>as_top_level_api(loglikelihood_fn, *, mean, cov, nd)</code>","text":"<p>Implements the (basic) user interface for the Elliptical Slice sampling kernel.</p>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api--examples","title":"Examples","text":"<p>A new Elliptical Slice sampling kernel can be initialized and used with the following code:</p> <p>.. code::</p> <pre><code>ellip_slice = blackjax.elliptical_slice(loglikelihood_fn, cov_matrix)\nstate = ellip_slice.init(position)\nnew_state, info = ellip_slice.step(rng_key, state)\n</code></pre> <p>We can JIT-compile the step function for better performance</p> <p>.. code::</p> <pre><code>step = jax.jit(ellip_slice.step)\nnew_state, info = step(rng_key, state)\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api--parameters","title":"Parameters","text":"<p>loglikelihood_fn     Only the log likelihood function from the posterior distributon we wish to sample. cov_matrix     The value of the covariance matrix of the gaussian prior distribution from the posterior we wish to sample.</p>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.as_top_level_api--returns","title":"Returns","text":"<p>A <code>SamplingAlgorithm</code>.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>def as_top_level_api(\n    loglikelihood_fn: Callable,\n    *,\n    mean: Array,\n    cov: Array,\n    nd: tuple,\n) -&gt; SamplingAlgorithm:\n    \"\"\"Implements the (basic) user interface for the Elliptical Slice sampling kernel.\n\n    Examples\n    --------\n\n    A new Elliptical Slice sampling kernel can be initialized and used with the following code:\n\n    .. code::\n\n        ellip_slice = blackjax.elliptical_slice(loglikelihood_fn, cov_matrix)\n        state = ellip_slice.init(position)\n        new_state, info = ellip_slice.step(rng_key, state)\n\n    We can JIT-compile the step function for better performance\n\n    .. code::\n\n        step = jax.jit(ellip_slice.step)\n        new_state, info = step(rng_key, state)\n\n    Parameters\n    ----------\n    loglikelihood_fn\n        Only the log likelihood function from the posterior distributon we wish to sample.\n    cov_matrix\n        The value of the covariance matrix of the gaussian prior distribution from the posterior we wish to sample.\n\n    Returns\n    -------\n    A ``SamplingAlgorithm``.\n    \"\"\"\n    kernel = build_kernel(cov, mean, nd)\n\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return init(position, loglikelihood_fn)\n\n    def step_fn(rng_key: PRNGKey, state):\n        return kernel(\n            rng_key,\n            state,\n            loglikelihood_fn,\n        )\n\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_elliptical_slice_nd/#bamojax.modified_blackjax.modified_elliptical_slice_nd.ellipsis","title":"<code>ellipsis(position, momentum, theta, mean)</code>","text":"<p>Generate proposal from the ellipsis.</p> <p>Given a scalar theta indicating a point on the circumference of the ellipsis and the shared mean vector for both position and momentum variables, generate proposed position and momentum to later accept or reject depending on the slice variable.</p> Source code in <code>bamojax/modified_blackjax/modified_elliptical_slice_nd.py</code> <pre><code>def ellipsis(position, momentum, theta, mean):\n    \"\"\"Generate proposal from the ellipsis.\n\n    Given a scalar theta indicating a point on the circumference of the ellipsis\n    and the shared mean vector for both position and momentum variables,\n    generate proposed position and momentum to later accept or reject\n    depending on the slice variable.\n\n    \"\"\"\n    position, unravel_fn = jax.flatten_util.ravel_pytree(position)\n    momentum, _ = jax.flatten_util.ravel_pytree(momentum)\n    position_centered = position - mean\n    momentum_centered = momentum - mean\n    return (\n        unravel_fn(\n            position_centered * jnp.cos(theta)\n            + momentum_centered * jnp.sin(theta)\n            + mean\n        ),\n        unravel_fn(\n            momentum_centered * jnp.cos(theta)\n            - position_centered * jnp.sin(theta)\n            + mean\n        ),\n    )\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/","title":"<code>bamojax.modified_blackjax.modified_tempered</code>","text":""},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.TemperedSMCState","title":"<code>TemperedSMCState</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Current state for the tempered SMC algorithm.</p> PyTree <p>The particles' positions.</p> <p>lmbda: float     Current value of the tempering parameter.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>class TemperedSMCState(NamedTuple):\n    \"\"\"Current state for the tempered SMC algorithm.\n\n    particles: PyTree\n        The particles' positions.\n    lmbda: float\n        Current value of the tempering parameter.\n\n    \"\"\"\n\n    particles: ArrayTree\n    weights: Array\n    lmbda: float\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.update_and_take_last","title":"<code>update_and_take_last(mcmc_init_fn, tempered_logposterior_fn, shared_mcmc_step_fn, num_mcmc_steps, n_particles)</code>","text":"<p>Given N particles, runs num_mcmc_steps of a kernel starting at each particle, and returns the last values, waisting the previous num_mcmc_steps-1 samples per chain.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>def update_and_take_last(\n    mcmc_init_fn,\n    tempered_logposterior_fn,\n    shared_mcmc_step_fn,\n    num_mcmc_steps,\n    n_particles,\n):\n    \"\"\"\n    Given N particles, runs num_mcmc_steps of a kernel starting at each particle, and\n    returns the last values, waisting the previous num_mcmc_steps-1\n    samples per chain.\n    \"\"\"\n\n    def mcmc_kernel(rng_key, position, step_parameters):\n        state = mcmc_init_fn(position, tempered_logposterior_fn)\n\n        def body_fn(state, rng_key):\n            new_state, info = shared_mcmc_step_fn(\n                rng_key, state, tempered_logposterior_fn, **step_parameters\n            )\n            return new_state, info\n\n        keys = jax.random.split(rng_key, num_mcmc_steps)\n        last_state, info = jax.lax.scan(body_fn, state, keys)\n        return last_state.position, info\n\n    return jax.vmap(mcmc_kernel), n_particles\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.build_kernel","title":"<code>build_kernel(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, resampling_fn, update_strategy=update_and_take_last)</code>","text":"<p>Build the base Tempered SMC kernel.</p> <p>Tempered SMC uses tempering to sample from a distribution given by</p> <p>.. math::     p(x) \\propto p_0(x) \\exp(-V(x)) \\mathrm{d}x</p> <p>where :math:<code>p_0</code> is the prior distribution, typically easy to sample from and for which the density is easy to compute, and :math:<code>\\exp(-V(x))</code> is an unnormalized likelihood term for which :math:<code>V(x)</code> is easy to compute pointwise.</p>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.build_kernel--parameters","title":"Parameters","text":"<p>logprior_fn     A function that computes the log density of the prior distribution loglikelihood_fn     A function that returns the probability at a given     position. mcmc_step_fn     A function that creates a mcmc kernel from a log-probability density function. mcmc_init_fn: Callable     A function that creates a new mcmc state from a position and a     log-probability density function. resampling_fn     A random function that resamples generated particles based of weights num_mcmc_iterations     Number of iterations in the MCMC chain.</p>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.build_kernel--returns","title":"Returns","text":"<p>A callable that takes a rng_key and a TemperedSMCState that contains the current state of the chain and that returns a new state of the chain along with information about the transition.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>def build_kernel(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    resampling_fn: Callable,\n    update_strategy: Callable = update_and_take_last,\n) -&gt; Callable:\n    \"\"\"Build the base Tempered SMC kernel.\n\n    Tempered SMC uses tempering to sample from a distribution given by\n\n    .. math::\n        p(x) \\\\propto p_0(x) \\\\exp(-V(x)) \\\\mathrm{d}x\n\n    where :math:`p_0` is the prior distribution, typically easy to sample from\n    and for which the density is easy to compute, and :math:`\\\\exp(-V(x))` is an\n    unnormalized likelihood term for which :math:`V(x)` is easy to compute\n    pointwise.\n\n    Parameters\n    ----------\n    logprior_fn\n        A function that computes the log density of the prior distribution\n    loglikelihood_fn\n        A function that returns the probability at a given\n        position.\n    mcmc_step_fn\n        A function that creates a mcmc kernel from a log-probability density function.\n    mcmc_init_fn: Callable\n        A function that creates a new mcmc state from a position and a\n        log-probability density function.\n    resampling_fn\n        A random function that resamples generated particles based of weights\n    num_mcmc_iterations\n        Number of iterations in the MCMC chain.\n\n    Returns\n    -------\n    A callable that takes a rng_key and a TemperedSMCState that contains the current state\n    of the chain and that returns a new state of the chain along with\n    information about the transition.\n\n    \"\"\"\n\n    def kernel(\n        rng_key: PRNGKey,\n        state: TemperedSMCState,\n        num_mcmc_steps: int,\n        lmbda: float,\n        mcmc_parameters: dict,\n    ) -&gt; tuple[TemperedSMCState, smc.base.SMCInfo]:\n        \"\"\"Move the particles one step using the Tempered SMC algorithm.\n\n        Parameters\n        ----------\n        rng_key\n            JAX PRNGKey for randomness\n        state\n            Current state of the tempered SMC algorithm\n        lmbda\n            Current value of the tempering parameter\n        mcmc_parameters\n            The parameters of the MCMC step function.  Parameters with leading dimension\n            length of 1 are shared amongst the particles.\n\n        Returns\n        -------\n        state\n            The new state of the tempered SMC algorithm\n        info\n            Additional information on the SMC step\n\n        \"\"\"\n        delta = lmbda - state.lmbda\n\n        # [MODIFICATION]\n        mcmc_parameters['temperature'] = state.lmbda*jnp.eye(1)\n        # [MODIFICATION]\n\n        shared_mcmc_parameters = {}\n        unshared_mcmc_parameters = {}\n        for k, v in mcmc_parameters.items():\n            if v.shape[0] == 1:\n                shared_mcmc_parameters[k] = v[0, ...]\n            else:\n                unshared_mcmc_parameters[k] = v\n\n        def log_weights_fn(position: ArrayLikeTree) -&gt; float:\n            return delta * loglikelihood_fn(position)\n\n        def tempered_logposterior_fn(position: ArrayLikeTree) -&gt; float:\n            logprior = logprior_fn(position)\n            tempered_loglikelihood = state.lmbda * loglikelihood_fn(position)\n            return logprior + tempered_loglikelihood\n\n        shared_mcmc_step_fn = partial(mcmc_step_fn, **shared_mcmc_parameters)\n\n        update_fn, num_resampled = update_strategy(\n            mcmc_init_fn,\n            tempered_logposterior_fn,\n            shared_mcmc_step_fn,\n            n_particles=state.weights.shape[0],\n            num_mcmc_steps=num_mcmc_steps,\n        )\n\n        smc_state, info = smc.base.step(\n            rng_key,\n            SMCState(state.particles, state.weights, unshared_mcmc_parameters),\n            update_fn,\n            jax.vmap(log_weights_fn),\n            resampling_fn,\n            num_resampled,\n        )\n\n        tempered_state = TemperedSMCState(\n            smc_state.particles, smc_state.weights, state.lmbda + delta\n        )\n\n        return tempered_state, info\n\n    return kernel\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.as_top_level_api","title":"<code>as_top_level_api(logprior_fn, loglikelihood_fn, mcmc_step_fn, mcmc_init_fn, mcmc_parameters, resampling_fn, num_mcmc_steps=10, update_strategy=update_and_take_last)</code>","text":"<p>Implements the (basic) user interface for the Adaptive Tempered SMC kernel.</p>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.as_top_level_api--parameters","title":"Parameters","text":"<p>logprior_fn     The log-prior function of the model we wish to draw samples from. loglikelihood_fn     The log-likelihood function of the model we wish to draw samples from. mcmc_step_fn     The MCMC step function used to update the particles. mcmc_init_fn     The MCMC init function used to build a MCMC state from a particle position. mcmc_parameters     The parameters of the MCMC step function.  Parameters with leading dimension     length of 1 are shared amongst the particles. resampling_fn     The function used to resample the particles. num_mcmc_steps     The number of times the MCMC kernel is applied to the particles per step.</p>"},{"location":"api/bamojax.modified_blackjax.modified_tempered/#bamojax.modified_blackjax.modified_tempered.as_top_level_api--returns","title":"Returns","text":"<p>A <code>SamplingAlgorithm</code>.</p> Source code in <code>bamojax/modified_blackjax/modified_tempered.py</code> <pre><code>def as_top_level_api(\n    logprior_fn: Callable,\n    loglikelihood_fn: Callable,\n    mcmc_step_fn: Callable,\n    mcmc_init_fn: Callable,\n    mcmc_parameters: dict,\n    resampling_fn: Callable,\n    num_mcmc_steps: Optional[int] = 10,\n    update_strategy=update_and_take_last,\n) -&gt; SamplingAlgorithm:\n    \"\"\"Implements the (basic) user interface for the Adaptive Tempered SMC kernel.\n\n    Parameters\n    ----------\n    logprior_fn\n        The log-prior function of the model we wish to draw samples from.\n    loglikelihood_fn\n        The log-likelihood function of the model we wish to draw samples from.\n    mcmc_step_fn\n        The MCMC step function used to update the particles.\n    mcmc_init_fn\n        The MCMC init function used to build a MCMC state from a particle position.\n    mcmc_parameters\n        The parameters of the MCMC step function.  Parameters with leading dimension\n        length of 1 are shared amongst the particles.\n    resampling_fn\n        The function used to resample the particles.\n    num_mcmc_steps\n        The number of times the MCMC kernel is applied to the particles per step.\n\n    Returns\n    -------\n    A ``SamplingAlgorithm``.\n\n    \"\"\"\n\n    kernel = build_kernel(\n        logprior_fn,\n        loglikelihood_fn,\n        mcmc_step_fn,\n        mcmc_init_fn,\n        resampling_fn,\n        update_strategy,\n    )\n\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return init(position)\n\n    def step_fn(rng_key: PRNGKey, state, lmbda):\n        return kernel(\n            rng_key,\n            state,\n            num_mcmc_steps,\n            lmbda,\n            mcmc_parameters,\n        )\n\n    return SamplingAlgorithm(init_fn, step_fn)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.more_proposals/","title":"<code>bamojax.modified_blackjax.more_proposals</code>","text":""},{"location":"api/bamojax.modified_blackjax.more_proposals/#bamojax.modified_blackjax.more_proposals.generate_bernoulli_noise","title":"<code>generate_bernoulli_noise(rng_key, position, theta)</code>","text":"<p>Given a position (pytree) and a probability theta, generate a new position by flipping each bit with probability theta.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def generate_bernoulli_noise(rng_key: PRNGKey, position, theta):\n    r\"\"\" Given a position (pytree) and a probability theta, generate a new position by flipping each bit with probability theta.\n    \"\"\"\n    p, unravel_fn = jax.flatten_util.ravel_pytree(position)\n    sample = jrnd.bernoulli(rng_key, shape=p.shape, p=theta)\n    return unravel_fn(sample)\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.more_proposals/#bamojax.modified_blackjax.more_proposals.bernoulli","title":"<code>bernoulli(theta)</code>","text":"<p>Create a proposal function that flips each Bernoulli random variable of the input position with probability theta.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def bernoulli(theta: Array) -&gt; Callable:\n    r\"\"\" Create a proposal function that flips each Bernoulli random variable of the input position with probability theta.\n    \"\"\"\n    def propose(rng_key: PRNGKey, position) -&gt; ArrayTree:\n        return generate_bernoulli_noise(rng_key, position, theta=theta)\n\n    #\n    return propose\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.more_proposals/#bamojax.modified_blackjax.more_proposals.build_xor_step","title":"<code>build_xor_step()</code>","text":"<p>Build a kernel that uses the xor operation to flip bits in a binary vector.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def build_xor_step():\n    r\"\"\" Build a kernel that uses the xor operation to flip bits in a binary vector.\n    \"\"\"\n    def kernel(\n        rng_key: PRNGKey, state, logdensity_fn: Callable, random_step: Callable\n    ):\n        def proposal_generator(key_proposal, position):\n            move_proposal = jax.tree_util.tree_map(lambda x: x.astype(int), random_step(key_proposal, position)) \n            new_position = jax.tree_util.tree_map(jnp.bitwise_xor, position, move_proposal)\n            return new_position\n\n        inner_kernel = blackjax.mcmc.random_walk.build_rmh()\n        return inner_kernel(rng_key, state, logdensity_fn, proposal_generator)\n\n    return kernel\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.more_proposals/#bamojax.modified_blackjax.more_proposals.xor_step_random_walk","title":"<code>xor_step_random_walk(logdensity_fn, random_step)</code>","text":"<p>Create a random walk MCMC algorithm that uses the xor operation to flip bits in a binary vector.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def xor_step_random_walk(logdensity_fn: Callable, random_step: Callable) -&gt; SamplingAlgorithm:\n    r\"\"\" Create a random walk MCMC algorithm that uses the xor operation to flip bits in a binary vector.\n    \"\"\"\n\n    kernel = build_xor_step()\n    def init_fn(position: ArrayLikeTree, rng_key=None):\n        del rng_key\n        return blackjax.mcmc.random_walk.init(position, logdensity_fn)\n\n    def step_fn(rng_key: PRNGKey, state):\n        return kernel(rng_key, state, logdensity_fn, random_step)\n\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/bamojax.modified_blackjax.more_proposals/#bamojax.modified_blackjax.more_proposals.bernoulli_random_walk","title":"<code>bernoulli_random_walk(logdensity_fn, theta)</code>","text":"<p>Create a random walk MCMC algorithm that moves across the space of binary vectors.</p> Source code in <code>bamojax/modified_blackjax/more_proposals.py</code> <pre><code>def bernoulli_random_walk(logdensity_fn: Callable, theta):\n    r\"\"\" Create a random walk MCMC algorithm that moves across the space of binary vectors.\n    \"\"\"\n    return xor_step_random_walk(logdensity_fn, bernoulli(theta))\n</code></pre>"},{"location":"api/bamojax.more_distributions/","title":"<code>bamojax.more_distributions</code>","text":""},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.Wishart","title":"<code>Wishart</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>Wishart distribution with parameters <code>dof</code> and <code>scale</code>.</p> <p>TODO: make batchable</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>class Wishart(Distribution):\n    \"\"\" Wishart distribution with parameters `dof` and `scale`.\n\n    TODO: make batchable\n\n    \"\"\"\n\n\n    def __init__(self, dof: int, scale: Optional[Array]):\n        \"\"\" Initializes a Wishart distribution.\n\n        Args:\n          dof: degrees of freedom\n          scale: scale matrix        \n        \"\"\"\n        super().__init__()\n        p = scale.shape[0]\n        assert dof &gt; p - 1, f'DoF must be &gt; p - 1, found DoF = {dof}, and p = {p}.'\n        self._dof = dof\n        self._scale = scale\n        self._p = p\n\n    #\n    @property\n    def event_shape(self) -&gt; Tuple[int, ...]:\n        \"\"\" Shape of event of distribution samples.\n\n        \"\"\"\n        return (self._p, self._p)\n\n    #\n    @property\n    def batch_shape(self) -&gt; Tuple[int, ...]:\n        \"\"\" Shape of batch of distribution samples.\n\n        \"\"\"\n        return jax.lax.broadcast_shapes(self._dof.shape, self._scale.shape)\n\n    #\n\n    def _sample_n(self, key: Array, n: int) -&gt; Array:\n        \"\"\" See `Distribution._sample_n`.\n\n        \"\"\"\n        X = jrnd.multivariate_normal(key, mean=jnp.zeros((self._p, )), cov=self._scale, shape=(n, self._dof))\n        wishart_matrices = jnp.einsum('ndp,ndq-&gt;npq', X, X)\n        return wishart_matrices\n\n    #\n    def log_prob(self, value: Array) -&gt; Array:\n        \"\"\" Computes the log probability of the Wishart distribution.\n\n        \"\"\"\n        _, logdetV = jnp.linalg.slogdet(self._scale)\n        _, logdetK = jnp.linalg.slogdet(value)\n\n        logZ = 0.5*self._dof*self._p*jnp.log(2) + 0.5*self._dof*logdetV + multigammaln(0.5*self._dof, self._p)\n        return 0.5*(self._dof - self._p - 1)*logdetK - 0.5*jnp.sum(jnp.diag(jnp.linalg.solve(self._scale, value))) - logZ\n\n    #\n    def mean(self) -&gt; Array:\n        \"\"\"Calculates the mean.\"\"\"\n\n        return self._dof*self._scale\n\n    #\n    def mode(self) -&gt; Array:\n        \"\"\"Calculates the mode.\"\"\"\n\n        assert self._dof &gt; self._p + 2, f'The mode is only defined for DoF &gt; p + 2, found DoF = {self._dof} and p = {self._p}.'\n        return (self._dof - self._p - 1)*self._scale\n\n    #\n    def variance(self) -&gt; Array:\n        \"\"\"Calculates the variance.\"\"\"\n\n        V_ij_squared = jnp.square(self._scale)  \n        V_ii = jnp.diag(self._scale)  \n        V_ii_V_jj = jnp.outer(V_ii, V_ii) \n        return self._dof * (V_ij_squared + V_ii_V_jj)\n</code></pre>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.Wishart.event_shape","title":"<code>event_shape</code>  <code>property</code>","text":"<p>Shape of event of distribution samples.</p>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.Wishart.batch_shape","title":"<code>batch_shape</code>  <code>property</code>","text":"<p>Shape of batch of distribution samples.</p>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.Wishart.log_prob","title":"<code>log_prob(value)</code>","text":"<p>Computes the log probability of the Wishart distribution.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def log_prob(self, value: Array) -&gt; Array:\n    \"\"\" Computes the log probability of the Wishart distribution.\n\n    \"\"\"\n    _, logdetV = jnp.linalg.slogdet(self._scale)\n    _, logdetK = jnp.linalg.slogdet(value)\n\n    logZ = 0.5*self._dof*self._p*jnp.log(2) + 0.5*self._dof*logdetV + multigammaln(0.5*self._dof, self._p)\n    return 0.5*(self._dof - self._p - 1)*logdetK - 0.5*jnp.sum(jnp.diag(jnp.linalg.solve(self._scale, value))) - logZ\n</code></pre>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.Wishart.mean","title":"<code>mean()</code>","text":"<p>Calculates the mean.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def mean(self) -&gt; Array:\n    \"\"\"Calculates the mean.\"\"\"\n\n    return self._dof*self._scale\n</code></pre>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.Wishart.mode","title":"<code>mode()</code>","text":"<p>Calculates the mode.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def mode(self) -&gt; Array:\n    \"\"\"Calculates the mode.\"\"\"\n\n    assert self._dof &gt; self._p + 2, f'The mode is only defined for DoF &gt; p + 2, found DoF = {self._dof} and p = {self._p}.'\n    return (self._dof - self._p - 1)*self._scale\n</code></pre>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.Wishart.variance","title":"<code>variance()</code>","text":"<p>Calculates the variance.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def variance(self) -&gt; Array:\n    \"\"\"Calculates the variance.\"\"\"\n\n    V_ij_squared = jnp.square(self._scale)  \n    V_ii = jnp.diag(self._scale)  \n    V_ii_V_jj = jnp.outer(V_ii, V_ii) \n    return self._dof * (V_ij_squared + V_ii_V_jj)\n</code></pre>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.GaussianProcessFactory","title":"<code>GaussianProcessFactory(cov_fn, mean_fn=Zero(), nd=None, jitter=1e-06)</code>","text":"<p>Returns an instantiated Gaussian process distribution object. </p> <p>This is essentially a dx.MultivariateNormalFullCovariance object, with its mean and covariance determined by the mean and covariance functions of the GP.</p> <p>Parameters:</p> Name Type Description Default <code>cov_fn</code> <code>Callable</code> <p>The GP covariance function. It assumes a signature of cov_fn(parameters: dict, x: Array, y: Array).      This is provided by the <code>jaxkern</code> library, but others can be used as well.</p> required <code>mean_fn</code> <code>Callable</code> <p>The GP mean function.</p> <code>Zero()</code> <code>nd</code> <code>Tuple[int, ...]</code> <p>A tuple of integers indicating optional additional output dimensions (for multi-task GPs).</p> <code>None</code> <code>jitter</code> <code>float</code> <p>A small value for numerical stability.</p> <code>1e-06</code> <p>Returns:     A GaussianProcessInstance distrax Distribution object.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def GaussianProcessFactory(cov_fn: Callable, mean_fn: Callable = Zero(),  nd: Tuple[int, ...] = None, jitter: float = 1e-6):\n    r\"\"\" Returns an instantiated Gaussian process distribution object. \n\n    This is essentially a dx.MultivariateNormalFullCovariance object, with its mean and covariance determined by the mean and covariance functions of the GP.\n\n    Args: \n        cov_fn: The GP covariance function. It assumes a signature of cov_fn(parameters: dict, x: Array, y: Array). \n                This is provided by the `jaxkern` library, but others can be used as well.\n        mean_fn: The GP mean function.\n        nd: A tuple of integers indicating optional additional output dimensions (for multi-task GPs).\n        jitter: A small value for numerical stability.\n    Returns:\n        A GaussianProcessInstance distrax Distribution object.\n\n    \"\"\"\n\n    class GaussianProcessInstance(Distribution):\n        \"\"\" An instantiated Gaussian process distribution object, i.e. a multivariate Gaussian.\n\n        \"\"\"\n\n        def __init__(self, input: Node, **params):\n            self.input = input\n\n            # In case of composite covariance functions:\n            if 'params' in params:\n                self.params = params['params']\n            else:\n                self.params = params\n\n        #\n        def _sample_n(self, key, n):\n            r\"\"\" Sample from the instantiated Gaussian process (i.e. multivariate Gaussian)\n\n            \"\"\"\n            x = self.input\n            m = x.shape[0]\n            output_shape = (m, )\n            if nd is not None:\n                output_shape = nd + output_shape\n\n            if n &gt; 1:\n                output_shape = (n, ) + output_shape\n\n            mu = self._get_mean()\n            cov = self._get_cov()\n            L = jnp.linalg.cholesky(cov)\n            z = jrnd.normal(key, shape=output_shape).T\n            V = jnp.tensordot(L, z, axes=(1, 0))\n            f = jnp.add(mu, jnp.moveaxis(V, 0, -1))\n            if jnp.ndim(f) == 1:\n                f = f[jnp.newaxis, :]\n            return f\n\n        #\n        def log_prob(self, value):\n            mu = self._get_mean()\n            cov = self._get_cov()\n            return dx.MultivariateNormalFullCovariance(loc=mu, covariance_matrix=cov).log_prob(value=value)\n\n        #\n        def sample_predictive_batched(self, key: Array, x_pred: Array, f: Array, num_batches:int = 20):\n            r\"\"\" Samples from the posterior predictve of the latent f, but in batches to converve memory.\n\n            Args:\n                key: PRNGkey\n                x_pred: Array\n                    The test locations\n                f: Array\n                    The trained GP to condition on\n                num_batches: int\n                    The number of batches to predict over.\n\n            Returns:\n                Returns samples from the posterior predictive distribution:\n\n                $$\n                    \\mathbf{f}* \\sim p(\\mathbf{f}* \\mid \\mathbf{f}, X, y x^*) = \\int p(\\mathbf{f}* \\mid x^*, \\mathbf{f}) p(\\mathbf{f} \\mid X, y) \\,\\text{d} \\mathbf{f}\n                ##\n\n\n            \"\"\"\n            if jnp.ndim(x_pred) == 1:\n                x_pred = x_pred[:, jnp.newaxis]\n\n            n_pred = x_pred.shape[0]\n            data_per_batch = int(n_pred / num_batches)\n            fpreds = list()\n            for batch in range(num_batches):\n                key, subkey = jrnd.split(key)\n                lb = data_per_batch*batch\n                ub = data_per_batch*(batch + 1)\n                fpred_batch = self.sample_predictive(subkey, x_pred[lb:ub, :], f)\n                fpreds.append(fpred_batch)\n\n            fpred = jnp.hstack(fpreds)\n            return fpred\n\n        #\n        def sample_predictive(self, key: Array, x_pred: Array, f: Array):\n            r\"\"\"Sample latent f for new points x_pred given one posterior sample.\n\n            See Rasmussen &amp; Williams. We are sampling from the posterior predictive for\n            the latent GP f, at this point not concerned with an observation model yet.\n\n            We have $[\\mathbf{f}, \\mathbf{f}^*]^T ~ \\mathcal{N}(0, KK)$, where $KK$ can be partitioned as:\n\n            $$\n                KK = \\begin{bmatrix} K(x,x) &amp; K(x,x^*) \\\\ K(x,x^*)\\top &amp; K(x^*,x^*)\\end{bmatrix}\n            $$\n\n            This results in the conditional\n            $$\n            \\mathbf{f}^* | x, x^*, \\mathbf{f} ~ \\mathcal{N}(\\mu, \\Sigma) \\enspace,\n             $$ where\n\n            $$\n            \\begin{align*}\n                \\mu &amp;= K(x^*, x)K(x,x)^-1 f \\enspace,\n                \\Sigma &amp;= K(x^*, x^*) - K(x^*, x) K(x, x)^-1 K(x, x^*) \\enspace.\n            \\end{align*}                \n            $$\n\n            Args:\n                key: The jrnd.PRNGKey object\n                x_pred: The prediction locations $x^*$\n                state_variables: A sample from the posterior\n\n            Returns:\n                A single posterior predictive sample $\\mathbf{f}^*$\n\n            \"\"\"\n            x = self.input\n            n = x.shape[0]\n            z = x_pred\n            if 'obs_noise' in self.params:\n                obs_noise = self.params['obs_noise']\n                if jnp.isscalar(obs_noise) or jnp.ndim(obs_noise) == 0:\n                    diagonal_noise = obs_noise**2 * jnp.eye(n, )\n                else:\n                    diagonal_noise = jnp.diagflat(obs_noise)**2\n            else:\n                diagonal_noise = 0\n\n            mean = mean_fn.mean(params=self.params, x=z)\n            Kxx = self.get_cov()\n            Kzx = cov_fn(params=self.params, x=z, y=x)\n            Kzz = cov_fn(params=self.params, x=z, y=z)\n\n            Kxx += jitter * jnp.eye(*Kxx.shape)\n            Kzx += jitter * jnp.eye(*Kzx.shape)\n            Kzz += jitter * jnp.eye(*Kzz.shape)\n\n            L = jnp.linalg.cholesky(Kxx + diagonal_noise)\n            v = jnp.linalg.solve(L, Kzx.T)\n\n            predictive_var = Kzz - jnp.dot(v.T, v)\n            predictive_var += jitter * jnp.eye(*Kzz.shape)\n            C = jnp.linalg.cholesky(predictive_var)\n\n            def get_sample(u_, target_):\n                alpha = jnp.linalg.solve(L.T, jnp.linalg.solve(L, target_))\n                predictive_mean = mean + jnp.dot(Kzx, alpha)\n                return predictive_mean + jnp.dot(C, u_)\n\n            #\n            if jnp.ndim(f) == 3:            \n                _, nu, d = f.shape\n                u = jrnd.normal(key, shape=(len(z), nu, d))\n                samples = jax.vmap(jax.vmap(get_sample, in_axes=1), in_axes=1)(u, f)\n                return samples.transpose([2, 0, 1])\n            elif jnp.ndim(f) == 1:\n                u = jrnd.normal(key, shape=(len(z),))\n                return get_sample(u, f)\n            else:\n                raise NotImplementedError(f'Shape of target must be (n,) or (n, nu, d)',\n                f'but {f.shape} was provided.')\n\n        #\n        def _get_mean(self):\n            \"\"\" Returns the mean of the GP at the input locations.\n\n            \"\"\"\n            return mean_fn.mean(params=self.params, x=self.input)\n\n        #\n        def get_mean(self):\n            return self._get_mean()\n\n        #\n        def _get_cov(self):\n            \"\"\" Returns the covariance of the GP at the input locations.\n\n            \"\"\"\n            x = self.input\n            m = x.shape[0]\n            return cov_fn(params=self.params, x=x, y=x) + jitter * jnp.eye(m)\n\n        #\n        def get_cov(self):\n            return self._get_cov()\n\n        #\n        @property\n        def event_shape(self):\n            r\"\"\" Event shape in this case is the shape of a single draw of $F = (f(x_1), ..., f(x_n))$\n\n            \"\"\"\n\n            output_shape = (self.input.shape[0], )\n            if nd is not None:\n                output_shape = nd + output_shape\n            return output_shape\n\n        #\n        @property\n        def batch_shape(self):\n            return ()\n\n        #\n\n    #\n    return GaussianProcessInstance\n</code></pre>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.AutoRegressionFactory","title":"<code>AutoRegressionFactory(ar_fn)</code>","text":"<p>Generates an autoregressive distribution with Gaussian emissions.</p> <p>This is a generator function that constructs a distrax Distribution object, which can then be queried for its log probability for inference.</p> <p>Parameters:</p> Name Type Description Default <code>ar_fn</code> <code>Callable</code> <p>A Callable function that takes innovations \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), and the previous instances \\(x(t-1), ..., x(t-p)\\), and performs whatever computation the user requires.</p> required Source code in <code>bamojax/more_distributions.py</code> <pre><code>def AutoRegressionFactory(ar_fn: Callable):\n    r\"\"\" Generates an autoregressive distribution with Gaussian emissions.\n\n    This is a generator function that constructs a distrax Distribution object, which can then be queried for its log probability for inference.\n\n    Args:\n        ar_fn: A Callable function that takes innovations $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, and the previous instances $x(t-1), ..., x(t-p)$, and performs whatever computation the user requires.\n\n    \"\"\"\n\n    class ARInstance(Distribution):\n        \"\"\" An instantiated autoregressive distribution object.\n\n        \"\"\"\n\n        def __init__(self, **kwargs):\n            self.parameters = kwargs\n\n        #\n        def _construct_lag_matrix(self, y, y_init):\n            r\"\"\" Construct $y$, and up to order shifts of it.\n\n            \"\"\"\n            order = 1 if jnp.isscalar(y_init) else y_init.shape[0]\n\n            @jax.jit\n            def update_fn(carry, i):\n                y_shifted = jnp.roll(carry, shift=1)  \n                y_shifted = y_shifted.at[0].set(y_init[i])  \n                return y_shifted, y_shifted  \n\n            #\n            _, columns = jax.lax.scan(update_fn, y, jnp.arange(order))\n            return columns\n\n        #\n        def log_prob(self, value):\n            r\"\"\" Returns the log-density of the complete AR distribution\n\n            \"\"\"\n            y_lagged = self._construct_lag_matrix(y=value, y_init=self.parameters['y0'])   \n            mu = ar_fn(y_prev=y_lagged, **self.parameters) \n            return dx.Normal(loc=mu, scale=self.parameters['scale']).log_prob(value)\n\n        #\n        def _sample_n(self, key, n):\n            r\"\"\" Sample from the AR distribution\n\n            \"\"\"\n            keys = jrnd.split(key, n)\n            samples = jax.vmap(self._sample_predictive)(keys)  \n            return samples\n\n        #        \n        def _sample_predictive(self, key):\n            r\"\"\" Sample from the AR(p) model.\n\n            Let:\n\n            $$\n            \\begin{align*}\n                \\epsilon_t &amp;\\sim \\mathcal{N}(0, \\sigma_y)\n                y_t &amp;= f(y_t-1, \\theta) + \\epsilon_t\n            \\end{align*}\n            $$ for $t = M+1, \\ldots, T$.\n\n            \"\"\"\n            @jax.jit\n            def ar_step(carry, epsilon_t):\n                y_t = ar_fn(y_prev=carry, **self.parameters) + epsilon_t\n                new_carry = jnp.concatenate([carry[1:], jnp.array([y_t])])\n                return new_carry, y_t\n\n            # \n            y_init = self.parameters['y0']\n            order = 1 if jnp.isscalar(y_init) else y_init.shape[0]\n            innovations = self.parameters['scale'] * jrnd.normal(key, shape=(self.parameters['T'] - order, ))\n            _, ys = jax.lax.scan(ar_step, y_init, innovations)\n            y = jnp.concatenate([y_init, ys])\n            return y\n\n        #                        \n        @property\n        def batch_shape(self):\n            return ( )\n\n        #\n        @property\n        def event_shape(self):\n            return (self.T, )\n\n        #\n\n    #\n    return ARInstance\n</code></pre>"},{"location":"api/bamojax.more_distributions/#bamojax.more_distributions.AscendingDistribution","title":"<code>AscendingDistribution(min_u, max_u, num_el)</code>","text":"<p>Creates a distribution of a sorted array of continuous values in [min_u, max_u].</p> <p>To ensure gradient-based methods can work on the model, all transformations must be bijectors. A generic sort() does not meet this condition, as it is not invertible. By using the tfb  bijector Ascending() in combination with scaling and deriving the expected maximum value of  dx.Transformed(Uniform, Ascending()), we can construct, in expectation, the desired random  variable. Note that individual draws main contain values that exceed max_u. </p> <p>Parameters:</p> Name Type Description Default <code>min_u, max_u</code> <p>The desired range.</p> required <code>num_el</code> <p>The length of the desired variate.</p> required <p>Returns:     A distribution over arrays of length <code>num_el</code>, with values in ascending order.</p> Source code in <code>bamojax/more_distributions.py</code> <pre><code>def AscendingDistribution(min_u, max_u, num_el):\n    r\"\"\" Creates a distribution of a sorted array of continuous values in [min_u, max_u].\n\n    To ensure gradient-based methods can work on the model, all transformations must be bijectors.\n    A generic sort() does not meet this condition, as it is not invertible. By using the tfb \n    bijector Ascending() in combination with scaling and deriving the expected maximum value of \n    dx.Transformed(Uniform, Ascending()), we can construct, in expectation, the desired random \n    variable. Note that individual draws main contain values that exceed max_u. \n\n    Args:\n        min_u, max_u: The desired range.\n        num_el: The length of the desired variate.\n    Returns:\n        A distribution over arrays of length `num_el`, with values in ascending order.\n\n\n    \"\"\"\n\n    R = 0.5 + (num_el-1)*(jnp.exp(1) - 1)\n    base_distribution = dx.Independent(dx.Uniform(low=jnp.zeros(num_el), high=jnp.ones(num_el)), reinterpreted_batch_ndims=1)\n\n    bijector = tfb.Chain([\n        tfb.Scale(scale=(max_u - min_u) / R),  \n        tfb.Shift(shift=jnp.array(min_u, dtype=jnp.float64)),  \n        tfb.Ascending()               \n    ])\n\n    return dx.Transformed(base_distribution, bijector)\n</code></pre>"},{"location":"api/bamojax.samplers/","title":"<code>bamojax.samplers</code>","text":""},{"location":"api/bamojax.samplers/#bamojax.samplers.gibbs_sampler","title":"<code>gibbs_sampler(model, step_fns=None, step_fn_params=None)</code>","text":"<p>Constructs a Gibbs sampler as a Blackjax SamplingAlgorithm.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The bamojax definition of a Bayesian model.</p> required <code>step_fns</code> <code>dict</code> <p>(optional) a set of step functions to use for updating each variable in turn.</p> <code>None</code> <code>step_fn_params</code> <code>dict</code> <p>(optional) parameters of the step functions</p> <code>None</code> <p>Returns:</p> Type Description <code>SamplingAlgorithm</code> <p>A SamplingAlgorithm object. This can be used to call the respective .init and .step functions in the inference routines.</p> Source code in <code>bamojax/samplers.py</code> <pre><code>def gibbs_sampler(model: Model, \n                  step_fns: dict = None, \n                  step_fn_params: dict = None) -&gt; SamplingAlgorithm:\n    r\"\"\" Constructs a Gibbs sampler as a Blackjax SamplingAlgorithm.\n\n    Args:\n        model: The bamojax definition of a Bayesian model.\n        step_fns: (optional) a set of step functions to use for updating each variable in turn.\n        step_fn_params: (optional) parameters of the step functions\n\n    Returns:\n        A SamplingAlgorithm object. This can be used to call the respective .init and .step functions in the inference routines.\n\n    \"\"\"\n\n    def set_step_fns_defaults(step_fns: dict = None, step_fn_params: dict = None):\n        r\"\"\" Set the step function of each node if not specified. Defaults to a Gaussian random walk with a stepsize of 0.01.\n\n        \"\"\"\n        if step_fns is None:\n            step_fns = {}\n            print('No step functions found; setting defaults.')\n        if step_fn_params is None:\n            step_fn_params = {}\n\n        sorted_free_variables = [node for node in model.get_node_order() if node.is_stochastic() and not node.is_observed() and not node in step_fns]\n        for node in sorted_free_variables:\n            step_fns[node] = normal_random_walk\n            num_elem = 1 if node.shape == () else jnp.prod(jnp.asarray(node.shape))\n            step_fn_params[node] = dict(sigma=0.01*jnp.eye(num_elem))\n\n        return step_fns, step_fn_params\n\n    #\n    step_fns, step_fn_params = set_step_fns_defaults(step_fns=step_fns, step_fn_params=step_fn_params)\n\n    def get_nonstandard_gibbs_step(node, \n                                   position, \n                                   loglikelihood_fn, \n                                   step_fns, \n                                   step_fn_params):\n        r\"\"\" The Blackjax SamplingAlgorithm is not parametrized in entirely the same way for different algorithms. To not clutter the gibbs_fn, exception cases are handled here.\n\n        \"\"\"\n        mean = node.get_distribution(position).get_mean()\n        cov = node.get_distribution(position).get_cov()\n        if step_fn_params[node]['name'] == 'elliptical_slice':                    \n            step_kernel = step_fns[node](loglikelihood_fn, mean=mean, cov=cov)\n            step_substate = step_kernel.init({node.name: position[node]})  \n        elif step_fn_params[node]['name'] == 'elliptical_slice_nd':                     \n            nd = step_fn_params[node]['nd']\n            step_kernel = step_fns[node](loglikelihood_fn, mean=mean, cov=cov, nd=nd)\n            step_substate = step_kernel.init({node.name: position[node]})  \n        elif step_fn_params[node]['name'] == 'mgrad_gaussian':\n            # see issue https://github.com/blackjax-devs/blackjax/issues/237,mgrad does not seem robust yet\n            loglikelihood_fn_mgrad = lambda state: loglikelihood_fn(state[node])\n            step_kernel = step_fns[node](logdensity_fn=loglikelihood_fn_mgrad, mean=mean, covariance=cov, **step_fn_params[node]['params'])\n            step_substate = step_kernel.init({node.name: position[node.name]}) \n        else:\n            raise NotImplementedError\n        return step_kernel, step_substate\n\n    #\n    def gibbs_fn(model: Model, \n                 key, \n                 state: dict, \n                 *args, \n                 **kwargs) -&gt; dict:\n        r\"\"\" Updates each latent variable given the current assignment of all other latent variables, according to the assigned step functions.\n\n        The signature of the Gibbs function is (key, state, temperature) -&gt; (state, info)\n\n        The Gibbs densities are defined as follows. Let $\\text{Pa}(x)$ give the parents of the set of variables $x$, and let $\\text{Ch}(x)$ give the set of children. Then the density is given by:\n\n        $$\n            p\\left(x \\mid \\text{Pa}(x)\\right) \\propto p\\left(\\text{Ch}(x) \\mid \\text{Pa}(\\text{Ch}(x))\\right) p\\left(x \\mid \\text{Pa}(x)\\right)\n        $$\n\n        Args:\n            key: PRNGKey\n            state: The current assignment of all latent variables.\n\n        Returns:\n            state: The updated assignment of all latent variables.\n            info: Additional information regarding the updates for every latent variable, such as acceptance rates.\n\n        \"\"\"\n\n        # In case we apply likelihood tempering\n        temperature = kwargs.get('temperature', 1.0)\n        position = state.position.copy()\n\n        info = {}\n        sorted_free_variables = [node for node in model.get_node_order() if node.is_stochastic() and not node.is_observed()]\n\n        for node in sorted_free_variables:\n            # Get conditional densities\n            conditionals = []\n            children = [c for c in model.get_children(node)]        \n            for child in children:\n                # Co-parents are all parents of the child, except node\n                co_parents = {parent for parent in model.get_parents(child) if parent != node}\n\n                # Values for co-parents are either taken from the position (if latent), or from their respective observations (if observed)\n                co_parent_arguments = {k: (position[k] if k in position else k.observations) for k in co_parents}\n\n                def loglikelihood_fn_(substate):\n                    dynamic_state = {**co_parent_arguments, node.name: substate[node]}\n                    child_value = child.observations if child.is_leaf() else position[child]\n                    return child.get_distribution(dynamic_state).log_prob(value=child_value)\n\n                #            \n                co_parents.add(node)\n                conditionals.append(loglikelihood_fn_)\n\n            loglikelihood_fn = lambda val: jnp.sum(jnp.asarray([temperature*ll_fn(val).sum() for ll_fn in conditionals]))\n\n            if 'implied_mvn_prior' in step_fn_params[node]:\n                # Some Blackjax step functions are tailored to multivariate Gaussian priors.\n                step_kernel, step_substate = get_nonstandard_gibbs_step(node, position, loglikelihood_fn, step_fns, step_fn_params)\n            else:\n                logprior_fn = lambda substate: node.get_distribution(position).log_prob(value=substate[node]).sum() \n                logdensity_fn = lambda substate_: loglikelihood_fn(substate_) + logprior_fn(substate_)\n                step_kernel = step_fns[node](logdensity_fn, **step_fn_params[node])   \n                step_substate = step_kernel.init({node.name: position[node]})   \n\n            key, subkey = jrnd.split(key)     \n            # [TODO]: add functionality to sample specific variables for different numbers of steps\n            step_substate, step_info = step_kernel.step(subkey, step_substate)\n            info[node.name] = step_info\n\n            position = {**position, **step_substate.position}\n\n            del step_kernel\n            del step_substate\n            del conditionals\n            del children\n\n        del state\n        return GibbsState(position=position), info\n\n    #\n    def init_fn(position, rng_key=None):\n        del rng_key\n        return GibbsState(position=position)\n\n    #\n    def step_fn(key: PRNGKey, state, *args, **kwargs):\n        state, info = gibbs_fn(model, key, state, *args, **kwargs)\n        return state, info\n\n    #\n    step_fn.__name__ = 'gibbs_step_fn'\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/bamojax.samplers/#bamojax.samplers.mcmc_sampler","title":"<code>mcmc_sampler(model, mcmc_kernel, mcmc_parameters=None)</code>","text":"<p>Constructs an MCMC sampler from a given Blackjax algorithm.</p> <p>This lightweight wrapper ensures the (optional) tempering parameter 'temperature', as part of the keyword-arguments of step_fn(..., **kwargs), is passed correctly.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>A bamojax model definition.</p> required <code>mcmc_kernel</code> <p>A Blackjax MCMC algorithm.</p> required <code>mcmc_parameters</code> <code>dict</code> <p>Optional Blackjax MCMC parameters, such as step sizes.</p> <code>None</code> <p>Returns:     A Blackjax SamplingAlgorithm object with methods <code>init_fn</code> and <code>step_fn</code>.</p> Source code in <code>bamojax/samplers.py</code> <pre><code>def mcmc_sampler(model: Model, \n                 mcmc_kernel, \n                 mcmc_parameters: dict = None):\n    \"\"\" Constructs an MCMC sampler from a given Blackjax algorithm.\n\n    This lightweight wrapper ensures the (optional) tempering parameter 'temperature',\n    as part of the keyword-arguments of step_fn(..., **kwargs), is passed correctly.\n\n    Args:\n        model: A bamojax model definition.\n        mcmc_kernel: A Blackjax MCMC algorithm.\n        mcmc_parameters: Optional Blackjax MCMC parameters, such as step sizes.\n    Returns:\n        A Blackjax SamplingAlgorithm object with methods `init_fn` and `step_fn`.\n\n    \"\"\"\n\n    def mcmc_fn(model: Model, \n                key, \n                state: dict, \n                *args, \n                **kwargs) -&gt; dict:\n\n        def apply_mcmc_kernel(key_, logdensity_fn, pos):\n            kernel_instance = mcmc_kernel(logdensity_fn=logdensity_fn, **mcmc_parameters)\n            state_ = kernel_instance.init(pos)\n            state_, info_ = kernel_instance.step(key_, state_)\n            return state_.position, info_\n\n        #\n        temperature = kwargs.get('temperature', 1.0)\n        position = state.position.copy()\n\n        loglikelihood_fn_ = model.loglikelihood_fn()\n        logprior_fn_ = model.logprior_fn()\n        tempered_logdensity_fn = lambda state: jnp.squeeze(temperature * loglikelihood_fn_(state) + logprior_fn_(state))\n        new_position, mcmc_info = apply_mcmc_kernel(key, tempered_logdensity_fn, position)\n        return MCMCState(position=new_position), mcmc_info\n\n    #\n    def init_fn(position, rng_key=None):\n        del rng_key\n        return MCMCState(position=position)\n\n    #\n    def step_fn(key: PRNGKey, state, *args, **kwargs):\n        state, info = mcmc_fn(model, key, state, *args, **kwargs)\n        return state, info\n\n    #\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"},{"location":"api/bamojax.samplers/#bamojax.samplers.reversible_jump_mcmc","title":"<code>reversible_jump_mcmc(models, auxiliary_proposal_dist, jump_functions, jacobians, projections, within_model_kernels, model_move_prob=0.5)</code>","text":"<p>Constructs a reversible jump MCMC algorithm for the given models.</p> <p>Implementation follows the reversible jump MCMC algorithm described by Hastie &amp; Green (2012).</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[Model]</code> <p>List of models to sample from.</p> required <code>auxiliary_proposal_dist</code> <code>Distribution</code> <p>Distribution to sample the auxiliary variable from.</p> required <code>jump_functions</code> <code>list[Callable]</code> <p>List of functions to transform the position when jumping between models.</p> required <code>jacobians</code> <code>list[Callable]</code> <p>List of Jacobian determinant functions for the jump functions</p> required <code>within_model_kernel</code> <p>List of within-model sampling algorithms, one for each model.</p> required <p>Returns:</p> Name Type Description <code>SamplingAlgorithm</code> <code>SamplingAlgorithm</code> <p>A sampling algorithm that implements the reversible jump MCMC.</p> References <p>Hastie, T., &amp; Green, P. J. (2012). Model choice using reversible jump Markov chain Monte Carlo</p> Source code in <code>bamojax/samplers.py</code> <pre><code>def reversible_jump_mcmc(models: list[Model], \n                         auxiliary_proposal_dist: Distribution,\n                         jump_functions: list[Callable],\n                         jacobians: list[Callable],\n                         projections: list[Callable],\n                         within_model_kernels: list[dict],\n                         model_move_prob: float = 0.5) -&gt; SamplingAlgorithm:\n    \"\"\"Constructs a reversible jump MCMC algorithm for the given models.\n\n    Implementation follows the reversible jump MCMC algorithm described by Hastie &amp; Green (2012).\n\n    Args:\n        models: List of models to sample from.\n        auxiliary_proposal_dist: Distribution to sample the auxiliary variable from.\n        jump_functions: List of functions to transform the position when jumping between models.\n        jacobians: List of Jacobian determinant functions for the jump functions\n        within_model_kernel: List of within-model sampling algorithms, one for each model.\n\n    Returns:\n        SamplingAlgorithm: A sampling algorithm that implements the reversible jump MCMC.\n\n    References:\n        Hastie, T., &amp; Green, P. J. (2012). Model choice using reversible jump Markov chain Monte Carlo\n\n    \"\"\"\n\n    assert len(models) == 2, 'Reversible jump MCMC currently only supports two models.'\n    assert len(jump_functions) == 2, 'Reversible jump MCMC requires two jump functions for the two models.'\n    assert len(jacobians) == 2, 'Reversible jump MCMC requires two Jacobian determinant functions for the two models.'\n    assert len(within_model_kernels) == 2, 'Reversible jump MCMC requires within-model sampling algorithms for each models.'\n\n    def make_within_model_kernel(model_index) -&gt; SamplingAlgorithm:\n        \"\"\"Creates a within-model kernel for the specified model index.\n\n        Args:\n            model_index: Index of the model for which to create the within-model kernel.\n\n        Returns:\n            A SamplingAlgorithm that performs within-model sampling for the specified model.\n\n        \"\"\"\n        return mcmc_sampler(models[model_index], \n                            mcmc_kernel=within_model_kernels[model_index]['mcmc_kernel'], \n                            mcmc_parameters=within_model_kernels[model_index]['mcmc_parameters'])\n\n    #    \n    def make_within_model_fn(model_index) -&gt; tuple[RJState, dict]:\n        \"\"\"Creates a function that performs a within-model move for the specified model index.\n\n        Args:\n            model_index: Index of the model for which to create the within-model move function.\n\n        Returns:\n            A function that takes a key, position, and optional arguments, and returns a new RJState and info dictionary.\n            The info dictionary imputes nan values for log probabilities and Jacobian determinants, as they are not used in within-model moves.\n\n        \"\"\"\n        def fn( key, position, *args):\n            temperature = args[0] if len(args) &gt; 0 else 1.0\n            kernel = mcmc_samplers[model_index]\n            initial_position = {k: position[k] for k in models[model_index].get_latent_nodes()}            \n            within_move_initial_state = kernel.init(position=initial_position)\n            new_state, info = kernel.step(key, within_move_initial_state, temperature=temperature)\n            new_position = {**position, **new_state.position, 'model_index': model_index}  \n            return RJState(position=new_position), {'within_model_move': 1, \n                                                    'is_accepted': info.is_accepted,\n                                                    'log_accept_ratio': jnp.log(info.acceptance_rate),\n                                                    'step_info': {'log_p_current': jnp.nan,\n                                                                  'log_p_proposed': jnp.nan,\n                                                                  'logq': jnp.nan,\n                                                                  'jacobian_det': jnp.nan}}\n        return fn\n\n    #\n    def make_model_logprob(model):\n        \"\"\"Creates a function that computes the log probability of the model given a position.\"\"\"\n\n        latent_keys = model.get_latent_nodes()\n\n        def fn(position):\n            \"\"\"Position might contain auxiliary variables and variables from other models, so we extract only the correct latent variables.\"\"\"\n            model_variables = {k: position[k] for k in latent_keys}\n            return model.loglikelihood_fn()(model_variables) + model.logprior_fn()(model_variables)\n\n        return fn\n\n    #\n    def reversible_jump_fn(key: PRNGKey, state: RJState, *args, **kwargs):\n        \"\"\"Performs a reversible jump MCMC step.\n\n        Args:\n            key: Random key for sampling.\n            state: Current state of the reversible jump MCMC, containing the model index and position.\n            *args: Additional arguments to pass to the within-model sampling functions. NOT USED\n            **kwargs: Additional keyword arguments to pass to the within-model sampling functions. NOT USED\n\n        Returns:\n            A tuple containing the next state and an info dictionary with details about the move.\n\n        \"\"\"\n\n        position = state.position\n        model_index = position['model_index']\n        key, subkey = jrnd.split(key)\n        move_type = jrnd.bernoulli(subkey, p=model_move_prob)\n        jacobian_det_up, jacobian_det_down = jacobians\n\n        def do_within_model(_):\n            \"\"\"Perform a standard bamojax within-model move.\"\"\"\n\n            temperature = kwargs.get('temperature', 1.0)\n            return jax.lax.switch(model_index, within_model_fns, key, position, temperature)\n\n        #\n        def do_between_model(_):\n            \"\"\"Perform a reversible jump between models.\n\n            Currently, there is only support for RJMCMC between two models.\n\n            \"\"\"\n            new_model_index = 1 - model_index\n            key_aux, key_accept = jrnd.split(key)            \n\n            def up_branch(_):\n                u = auxiliary_proposal_dist.sample(seed=key_aux)\n                new_position = jump_functions[0](position, u)  # make kappa from the auxiliary variable u\n                jac_det = jacobian_det_up(u)\n                logq = -1.0*auxiliary_proposal_dist.log_prob(u)  # Note the negative sign! To check: is this robust for other proposal distributions?\n                return new_position, jac_det, logq\n\n            #\n            def down_branch(_):\n                new_position = jump_functions[1](position) # discard auxiliary variable and kappa\n                jac_det = jacobian_det_down(new_position['kappa'])\n                logq = auxiliary_proposal_dist.log_prob(projections[1](new_position)) # log(kappa / mu) where mu is the mean of the auxiliary proposal\n                return new_position, jac_det, logq\n\n            #\n            new_position, jac_det, logq = jax.lax.cond(model_index == 0, up_branch, down_branch, operand=None)\n            new_position['model_index'] = new_model_index  # update model index in the new position\n            log_p_current = jax.lax.switch(model_index, model_logprobs, position)\n            log_p_proposed = jax.lax.switch(new_model_index, model_logprobs, new_position)\n            log_accept_ratio = log_p_proposed - log_p_current + logq + jnp.log(jac_det) \n\n            accept = jnp.log(jrnd.uniform(key_accept)) &lt; log_accept_ratio\n            next_state = jax.lax.cond(accept, lambda _: RJState(new_position), lambda _: state, operand=None)       \n\n            return next_state, {'within_model_move': 0, \n                                'is_accepted': accept, \n                                'log_accept_ratio': log_accept_ratio,\n                                'step_info': {'log_p_current': log_p_current,\n                                              'log_p_proposed': log_p_proposed,\n                                              'logq': logq,\n                                              'jacobian_det': jac_det}}\n\n        #\n        return jax.lax.cond(move_type, do_within_model, do_between_model, operand=None)\n\n    #\n    def init_fn(position: ArrayTree, rng_key=None):\n        del rng_key\n        return RJState(position=position)\n\n    #\n    def step_fn(key: PRNGKey, state, *args, **kwargs):\n        state, info = reversible_jump_fn(key, state, *args, **kwargs)\n        return state, info\n\n    #\n\n    within_model_fns = [make_within_model_fn(i) for i in range(len(models))]\n    model_logprobs = [make_model_logprob(model) for model in models]\n    mcmc_samplers = [make_within_model_kernel(i) for i in range(len(models))]\n\n    step_fn.__name__ = 'reversible_jump_fn'\n    return SamplingAlgorithm(init_fn, step_fn)\n</code></pre>"}]}